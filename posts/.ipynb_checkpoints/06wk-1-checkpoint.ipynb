{
 "cells": [
  {
   "cell_type": "raw",
   "id": "53bd0245-60b0-40bc-9fd4-d47470cbbe57",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d"
   },
   "source": [
    "---\n",
    "title: \"06wk-1: 합성곱신경망 (2) -- MNIST, Fashion-MNIST, ImageNet, CIFAR10 \"\n",
    "author: \"최규빈\"\n",
    "date: \"04/03/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeb201-7db9-4500-967c-df308017c564",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/06wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06b622-d159-4970-b217-ac4787733545",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "12ba7872-1155-4c2f-8f94-e201f3a3ea25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#{{<video https://youtu.be/playlist?list=PLQqh36zP38-wjNGgd4gmQJbQ66NLjUC2y&si=dusDZAwGOJS9TOKJ >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe14a88-1313-4a92-991e-9479afc21636",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "66839c05-69cf-4da5-b8a8-861e89481e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6d9f0-74a1-4c5d-b3cb-6fb61c420139",
   "metadata": {},
   "source": [
    "# 3. torch.eigensum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474cded-48c5-4cef-9941-0e4c8efcf49e",
   "metadata": {},
   "source": [
    "## A. transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "fa3d65ef-9547-4b80-96c1-442457d0e23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.arange(12).reshape(4,3)\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "01bb899f-b74e-4754-93d3-df0be29348d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3,  6,  9],\n",
       "        [ 1,  4,  7, 10],\n",
       "        [ 2,  5,  8, 11]])"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "18265793-22d8-4daa-9164-58d3f656e9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3,  6,  9],\n",
       "        [ 1,  4,  7, 10],\n",
       "        [ 2,  5,  8, 11]])"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->ji',tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f9298-4ed2-4b04-9eac-a096723a37f0",
   "metadata": {},
   "source": [
    "## B. 행렬곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "87678d33-7443-430a-bb1b-8969b398c792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr1 = torch.arange(12).reshape(4,3).float()\n",
    "tsr2 = torch.arange(15).reshape(3,5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "701aaf62-9245-44e0-8c5b-a82726e2ddaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "3efc65bc-8115-4304-a74a-add116f4ca1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "07a4f7c8-d189-4f1e-afdd-f04c3e15a3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25.,  28.,  31.,  34.,  37.],\n",
       "        [ 70.,  82.,  94., 106., 118.],\n",
       "        [115., 136., 157., 178., 199.],\n",
       "        [160., 190., 220., 250., 280.]])"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr1 @ tsr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "0b01990c-424f-49ec-8c53-22d9fde57c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25.,  28.,  31.,  34.,  37.],\n",
       "        [ 70.,  82.,  94., 106., 118.],\n",
       "        [115., 136., 157., 178., 199.],\n",
       "        [160., 190., 220., 250., 280.]])"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij,jk -> ik',tsr1,tsr2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82940c-f262-490e-9b3c-556640e34728",
   "metadata": {},
   "source": [
    "# 4. MNIST -- 직접설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "84efbf3a-ab82-4e99-9bd1-f908a2da2919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2])/255\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\n",
    "XX = torch.concat([X0,X1,X2])/255\n",
    "yy = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "d8912eed-0678-4f17-8ceb-e15fb1e50f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32]) \t torch.float32\n",
      "torch.Size([50000]) \t\t\t torch.int64\n",
      "torch.Size([10000, 3, 32, 32]) \t torch.float32\n",
      "torch.Size([10000]) \t\t\t torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t\\t',y.dtype)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4404394e-174e-4134-8e9b-c42abb485e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f2ed0fd50>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaBElEQVR4nO3dfWxT5/n/8Y8LwaXU8RRBYqeEfKMJ1rUgtgIDovJUrRHZikqhEg/dFDYNwXjQorTqCmwj2x+kQypCGitT0ZSCBlsmDSgaqJAJEpgYFY3SFWUVoiKM0JBFRMwOgZoB9+8PhH91Ex6OsXPFyfsl3RI+51w9F6dH+XDn2Ld9zjknAAAMPGLdAABg4CKEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYGawdQNfduvWLbW2tioQCMjn81m3AwDwyDmnzs5O5efn65FH7j3X6XMh1NraqoKCAus2AAAPqaWlRSNHjrznMX3u13GBQMC6BQBACjzIz/O0hdDbb7+toqIiPfroo5owYYKOHTv2QHX8Cg4A+ocH+XmelhCqqalReXm51q1bp8bGRk2bNk2lpaU6f/58Ok4HAMhQvnSsoj158mQ988wz2rp1a3zb17/+dc2dO1dVVVX3rI1GowoGg6luCQDQyyKRiLKzs+95TMpnQtevX1dDQ4NKSkoStpeUlOj48ePdjo/FYopGowkDADAwpDyELl26pJs3byovLy9he15entra2rodX1VVpWAwGB+8Mw4ABo60vTHhyw+knHM9PqRas2aNIpFIfLS0tKSrJQBAH5PyzwkNHz5cgwYN6jbraW9v7zY7kiS/3y+/35/qNgAAGSDlM6EhQ4ZowoQJqq2tTdheW1ur4uLiVJ8OAJDB0rJiQkVFhb7//e9r4sSJmjp1qt555x2dP39ey5cvT8fpAAAZKi0htGDBAnV0dOhXv/qVLl68qLFjx+rAgQMqLCxMx+kAABkqLZ8Tehh8TggA+geTzwkBAPCgCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJgZbN0AMBBNnz7dc01BQYHnmrVr13qukaSnnnrKc82FCxc817zxxhuea9577z3PNVeuXPFcg97BTAgAYIYQAgCYSXkIVVZWyufzJYxQKJTq0wAA+oG0PBN6+umn9be//S3+etCgQek4DQAgw6UlhAYPHszsBwBwX2l5JnTmzBnl5+erqKhICxcu1NmzZ+96bCwWUzQaTRgAgIEh5SE0efJk7dixQwcPHtS2bdvU1tam4uJidXR09Hh8VVWVgsFgfCTzNlQAQGZKeQiVlpZq/vz5GjdunL797W9r//79kqTt27f3ePyaNWsUiUTio6WlJdUtAQD6qLR/WHXYsGEaN26czpw50+N+v98vv9+f7jYAAH1Q2j8nFIvF9MknnygcDqf7VACADJPyEHrttddUX1+v5uZmffDBB3r55ZcVjUZVVlaW6lMBADJcyn8dd+HCBS1atEiXLl3SiBEjNGXKFJ04cUKFhYWpPhUAIMP5nHPOuokvikajCgaD1m0AD6ykpMRzzbvvvuu5Ji8vz3NNf7Rt2zbPNT/5yU+SOlcsFkuqDrdFIhFlZ2ff8xjWjgMAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGBUyBh3T16lXPNXyRY+9atWpVUnVbt25NcScDCwuYAgD6NEIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAmcHWDQB9yYgRIzzX+Hy+NHSCuzl16pTnmj179qShE6QCMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmWMAU+IKqqirPNUOGDElDJ7ibzs5OzzVtbW1p6ASpwEwIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGRYwRb+0cOHCpOrmzZuX4k5snTt3Lqm6//u//0tpH8DdMBMCAJghhAAAZjyH0NGjRzVnzhzl5+fL5/Np7969Cfudc6qsrFR+fr6GDh2qmTNnqqmpKVX9AgD6Ec8h1NXVpfHjx2vLli097t+4caM2bdqkLVu26OTJkwqFQnr++eeT+iIqAED/5vmNCaWlpSotLe1xn3NOmzdv1rp16+IPeLdv3668vDzt2rVLy5Yte7huAQD9SkqfCTU3N6utrU0lJSXxbX6/XzNmzNDx48d7rInFYopGowkDADAwpDSE7nyPe15eXsL2vLy8u37He1VVlYLBYHwUFBSksiUAQB+WlnfH+Xy+hNfOuW7b7lizZo0ikUh8tLS0pKMlAEAflNIPq4ZCIUm3Z0ThcDi+vb29vdvs6A6/3y+/35/KNgAAGSKlM6GioiKFQiHV1tbGt12/fl319fUqLi5O5akAAP2A55nQlStX9Omnn8ZfNzc366OPPlJOTo5GjRql8vJybdiwQaNHj9bo0aO1YcMGPfbYY1q8eHFKGwcAZD7PIfThhx9q1qxZ8dcVFRWSpLKyMr377rt6/fXXde3aNa1YsUKXL1/W5MmTdejQIQUCgdR1DQDoF3zOOWfdxBdFo1EFg0HrNpDhWltbk6q727PLvuCDDz7wXPPDH/4wqXP9+te/9lzzwgsvJHUur+72cY97mTZtWho6wf1EIhFlZ2ff8xjWjgMAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmEnpN6sC9zNlyhTPNT/60Y8814wYMcJzTbLq6uo818RiMc81ixYt8lwTiUQ810jSzp07PdfMmDHDc00yX/GSzCr7I0eO9FwjSRcuXEiqDg+OmRAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzPuecs27ii6LRaFILFCIzNDU1ea558skn09BJz6qrqz3XLFu2zHPNzZs3Pdf0dQ0NDZ5rvvGNb6S+kR6sWrUqqbqtW7emuJOBJRKJKDs7+57HMBMCAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgZrB1A8hca9as8Vzzta99LQ2ddLdx48ak6n72s595rumPi5Em4wc/+IHnmsbGxjR0gkzCTAgAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZFjCFRo4cmVTd4sWLPdf4fD7PNf/73/8812zevNlzjcRipA/jxo0b1i0gAzETAgCYIYQAAGY8h9DRo0c1Z84c5efny+fzae/evQn7lyxZIp/PlzCmTJmSqn4BAP2I5xDq6urS+PHjtWXLlrseM3v2bF28eDE+Dhw48FBNAgD6J89vTCgtLVVpaek9j/H7/QqFQkk3BQAYGNLyTKiurk65ubkaM2aMli5dqvb29rseG4vFFI1GEwYAYGBIeQiVlpZq586dOnz4sN566y2dPHlSzz33nGKxWI/HV1VVKRgMxkdBQUGqWwIA9FEp/5zQggUL4n8eO3asJk6cqMLCQu3fv1/z5s3rdvyaNWtUUVERfx2NRgkiABgg0v5h1XA4rMLCQp05c6bH/X6/X36/P91tAAD6oLR/Tqijo0MtLS0Kh8PpPhUAIMN4nglduXJFn376afx1c3OzPvroI+Xk5CgnJ0eVlZWaP3++wuGwzp07p7Vr12r48OF66aWXUto4ACDzeQ6hDz/8ULNmzYq/vvM8p6ysTFu3btWpU6e0Y8cO/fe//1U4HNasWbNUU1OjQCCQuq4BAP2C5xCaOXOmnHN33X/w4MGHagi9r6amJqm6p556ynNNR0eH55rvfe97nmv+85//eK4B0PtYOw4AYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYCbt36yK3jVs2DDPNVlZWWnopGf//Oc/PdccOnQoDZ0g1b7yla/0ynmampo81+zbty8NnSAVmAkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwwwKm/cySJUs810yYMCH1jdzF6tWre+1cSN7EiRM919TU1KShk+4ikYjnms8++ywNnSAVmAkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwwwKmQD82fvz4pOr27NnjuSY/P99zTXt7u+ean/70p55r0HcxEwIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGBUwBA1lZWZ5rfvGLX3iueeWVVzzXSL23GOnLL7/sueb48eOea9B3MRMCAJghhAAAZjyFUFVVlSZNmqRAIKDc3FzNnTtXp0+fTjjGOafKykrl5+dr6NChmjlzppqamlLaNACgf/AUQvX19Vq5cqVOnDih2tpa3bhxQyUlJerq6oofs3HjRm3atElbtmzRyZMnFQqF9Pzzz6uzszPlzQMAMpunNya8//77Ca+rq6uVm5urhoYGTZ8+Xc45bd68WevWrdO8efMkSdu3b1deXp527dqlZcuWpa5zAEDGe6hnQpFIRJKUk5MjSWpublZbW5tKSkrix/j9fs2YMeOu72iJxWKKRqMJAwAwMCQdQs45VVRU6Nlnn9XYsWMlSW1tbZKkvLy8hGPz8vLi+76sqqpKwWAwPgoKCpJtCQCQYZIOoVWrVunjjz/WH//4x277fD5fwmvnXLdtd6xZs0aRSCQ+Wlpakm0JAJBhkvqw6urVq7Vv3z4dPXpUI0eOjG8PhUKSbs+IwuFwfHt7e3u32dEdfr9ffr8/mTYAABnO00zIOadVq1Zp9+7dOnz4sIqKihL2FxUVKRQKqba2Nr7t+vXrqq+vV3FxcWo6BgD0G55mQitXrtSuXbv03nvvKRAIxJ/zBINBDR06VD6fT+Xl5dqwYYNGjx6t0aNHa8OGDXrssce0ePHitPwFAACZy1MIbd26VZI0c+bMhO3V1dVasmSJJOn111/XtWvXtGLFCl2+fFmTJ0/WoUOHFAgEUtIwAKD/8DnnnHUTXxSNRhUMBq3byFjZ2dmeaw4fPpzUub75zW96rvnss88819y4ccNzzcqVKz3XSP//YwfpVl5e7rlm/vz5qW/kLlpbWz3XLFiwwHMNi5H2b5FI5L4/k1g7DgBghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgJqlvVkXfFY1GPdfEYrE0dNKzJ554olfO89e//rVXztPXJbsqOCtio7cwEwIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGBUyhV155Jam63/zmN55rpk+f7rnm8ccf91zT1928edNzzdq1az3XvPPOO55rpOQWwgWSwUwIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGZ9zzlk38UXRaFTBYNC6DaTJd7/7Xc81y5cv91zzne98x3ONlNyCn0ePHvVck8wCpn/+85891wCWIpGIsrOz73kMMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmWMAUAJAWLGAKAOjTCCEAgBlPIVRVVaVJkyYpEAgoNzdXc+fO1enTpxOOWbJkiXw+X8KYMmVKSpsGAPQPnkKovr5eK1eu1IkTJ1RbW6sbN26opKREXV1dCcfNnj1bFy9ejI8DBw6ktGkAQP8w2MvB77//fsLr6upq5ebmqqGhQdOnT49v9/v9CoVCqekQANBvPdQzoUgkIknKyclJ2F5XV6fc3FyNGTNGS5cuVXt7+13/G7FYTNFoNGEAAAaGpN+i7ZzTiy++qMuXL+vYsWPx7TU1NXr88cdVWFio5uZm/fznP9eNGzfU0NAgv9/f7b9TWVmpX/7yl8n/DQAAfdKDvEVbLkkrVqxwhYWFrqWl5Z7Htba2uqysLPeXv/ylx/2ff/65i0Qi8dHS0uIkMRgMBiPDRyQSuW+WeHomdMfq1au1b98+HT16VCNHjrznseFwWIWFhTpz5kyP+/1+f48zJABA/+cphJxzWr16tfbs2aO6ujoVFRXdt6ajo0MtLS0Kh8NJNwkA6J88vTFh5cqV+sMf/qBdu3YpEAiora1NbW1tunbtmiTpypUreu211/SPf/xD586dU11dnebMmaPhw4frpZdeSstfAACQwbw8B9Jdfu9XXV3tnHPu6tWrrqSkxI0YMcJlZWW5UaNGubKyMnf+/PkHPkckEjH/PSaDwWAwHn48yDMhFjAFAKQFC5gCAPo0QggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAICZPhdCzjnrFgAAKfAgP8/7XAh1dnZatwAASIEH+Xnuc31s6nHr1i21trYqEAjI5/Ml7ItGoyooKFBLS4uys7ONOrTHdbiN63Ab1+E2rsNtfeE6OOfU2dmp/Px8PfLIvec6g3uppwf2yCOPaOTIkfc8Jjs7e0DfZHdwHW7jOtzGdbiN63Cb9XUIBoMPdFyf+3UcAGDgIIQAAGYyKoT8fr/Wr18vv99v3YoprsNtXIfbuA63cR1uy7Tr0OfemAAAGDgyaiYEAOhfCCEAgBlCCABghhACAJjJqBB6++23VVRUpEcffVQTJkzQsWPHrFvqVZWVlfL5fAkjFApZt5V2R48e1Zw5c5Sfny+fz6e9e/cm7HfOqbKyUvn5+Ro6dKhmzpyppqYmm2bT6H7XYcmSJd3ujylTptg0myZVVVWaNGmSAoGAcnNzNXfuXJ0+fTrhmIFwPzzIdciU+yFjQqimpkbl5eVat26dGhsbNW3aNJWWlur8+fPWrfWqp59+WhcvXoyPU6dOWbeUdl1dXRo/fry2bNnS4/6NGzdq06ZN2rJli06ePKlQKKTnn3++361DeL/rIEmzZ89OuD8OHDjQix2mX319vVauXKkTJ06otrZWN27cUElJibq6uuLHDIT74UGug5Qh94PLEN/61rfc8uXLE7Y9+eST7o033jDqqPetX7/ejR8/3roNU5Lcnj174q9v3brlQqGQe/PNN+PbPv/8cxcMBt3vfvc7gw57x5evg3POlZWVuRdffNGkHyvt7e1Okquvr3fODdz74cvXwbnMuR8yYiZ0/fp1NTQ0qKSkJGF7SUmJjh8/btSVjTNnzig/P19FRUVauHChzp49a92SqebmZrW1tSXcG36/XzNmzBhw94Yk1dXVKTc3V2PGjNHSpUvV3t5u3VJaRSIRSVJOTo6kgXs/fPk63JEJ90NGhNClS5d08+ZN5eXlJWzPy8tTW1ubUVe9b/LkydqxY4cOHjyobdu2qa2tTcXFxero6LBuzcyd//8D/d6QpNLSUu3cuVOHDx/WW2+9pZMnT+q5555TLBazbi0tnHOqqKjQs88+q7Fjx0oamPdDT9dBypz7oc+ton0vX/5qB+dct239WWlpafzP48aN09SpU/XVr35V27dvV0VFhWFn9gb6vSFJCxYsiP957NixmjhxogoLC7V//37NmzfPsLP0WLVqlT7++GP9/e9/77ZvIN0Pd7sOmXI/ZMRMaPjw4Ro0aFC3f8m0t7d3+xfPQDJs2DCNGzdOZ86csW7FzJ13B3JvdBcOh1VYWNgv74/Vq1dr3759OnLkSMJXvwy0++Fu16EnffV+yIgQGjJkiCZMmKDa2tqE7bW1tSouLjbqyl4sFtMnn3yicDhs3YqZoqIihUKhhHvj+vXrqq+vH9D3hiR1dHSopaWlX90fzjmtWrVKu3fv1uHDh1VUVJSwf6DcD/e7Dj3ps/eD4ZsiPPnTn/7ksrKy3O9//3v3r3/9y5WXl7thw4a5c+fOWbfWa1599VVXV1fnzp49606cOOFeeOEFFwgE+v016OzsdI2Nja6xsdFJcps2bXKNjY3u3//+t3POuTfffNMFg0G3e/dud+rUKbdo0SIXDoddNBo17jy17nUdOjs73auvvuqOHz/umpub3ZEjR9zUqVPdE0880a+uw49//GMXDAZdXV2du3jxYnxcvXo1fsxAuB/udx0y6X7ImBByzrnf/va3rrCw0A0ZMsQ988wzCW9HHAgWLFjgwuGwy8rKcvn5+W7evHmuqanJuq20O3LkiJPUbZSVlTnnbr8td/369S4UCjm/3++mT5/uTp06Zdt0GtzrOly9etWVlJS4ESNGuKysLDdq1ChXVlbmzp8/b912SvX095fkqqur48cMhPvhftchk+4HvsoBAGAmI54JAQD6J0IIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGb+H0S4+LkK0t74AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.einsum('cij -> ijc',X[0]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81058d3d-855b-47cc-bfdf-b6e70a7def2a",
   "metadata": {},
   "source": [
    "## A. y: (n,3)-float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ce223c36-98b7-442c-9aaf-9894bba83a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9858\n",
      "val: 0.9917\n"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,3),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y.argmax(axis=1)).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy.argmax(axis=1)).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21a270-bfe0-4659-8d61-2483478add8d",
   "metadata": {},
   "source": [
    "## B. y: (n,)-int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9e8ec864-ea1c-40bc-8734-915303281364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0,  ..., 2, 2, 2]), tensor([0, 0, 0,  ..., 2, 2, 2]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.argmax(axis=-1)\n",
    "yy = yy.argmax(axis=-1)\n",
    "y,yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0261ae34-4574-43ef-a63e-2b9cbdf12121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18623, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([18623]) \t\t torch.int64\n",
      "torch.Size([3147, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([3147]) \t\t torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t',y.dtype)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2e32ab1a-44cd-40d3-8344-03b17150d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9780\n",
      "val: 0.9822\n"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,3),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}') # <-- 여기수정\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}') # <-- 여기수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08e099-598c-4971-bd36-76973f366c3b",
   "metadata": {},
   "source": [
    "# 5. Fashion-MNIST -- fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f02c98-b785-41ec-8ed1-5b77688dd1e9",
   "metadata": {},
   "source": [
    "`-` Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "155edf23-35db-4ac3-af9e-3a177e358944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('https://media.githubusercontent.com/media/guebin/PP2023/main/posts/fashion-mnist_train.csv')\n",
    "df_test=pd.read_csv('https://media.githubusercontent.com/media/guebin/PP2023/main/posts/fashion-mnist_test.csv')\n",
    "def rshp(row):\n",
    "    return row.reshape(1,28,28)\n",
    "X = torch.tensor(np.apply_along_axis(rshp,axis=1,arr=np.array(df_train.iloc[:,1:]))).float()\n",
    "XX  = torch.tensor(np.apply_along_axis(rshp,axis=1,arr=np.array(df_test.iloc[:,1:]))).float()\n",
    "y = torch.tensor(np.array(df_train.label))\n",
    "yy  = torch.tensor(np.array(df_test.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6abf37c9-6418-44da-9575-bfd755b7838d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([60000]) \t\t torch.int64\n",
      "torch.Size([10000, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([10000]) \t\t torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t',y.dtype)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c719fe6c-fc81-4259-8089-e9e49adae0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f2ecda2d0>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.einsum('cij -> ijc',X[0]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402659db-7c3f-4fbc-834b-0ce8c5b06006",
   "metadata": {},
   "source": [
    "## A. torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "18a1516f-34c6-4802-b79b-8c8bc2f38688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9095\n",
      "val: 0.8692\n"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e757b3c-1e4f-4acb-8ec8-47175f4cd776",
   "metadata": {},
   "source": [
    "## B. fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9508e991-9faf-4c0f-9f44-f28a0b4486c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.543901</td>\n",
       "      <td>0.478390</td>\n",
       "      <td>0.854700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403238</td>\n",
       "      <td>0.417672</td>\n",
       "      <td>0.861200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353587</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319807</td>\n",
       "      <td>0.401830</td>\n",
       "      <td>0.872600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297642</td>\n",
       "      <td>0.407712</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.282916</td>\n",
       "      <td>0.416574</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.434183</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.256538</td>\n",
       "      <td>0.447821</td>\n",
       "      <td>0.875200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.250892</td>\n",
       "      <td>0.478167</td>\n",
       "      <td>0.871800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>0.490654</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9089\n",
      "val: 0.8690\n"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=128) \n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=5000) \n",
    "dls = DataLoaders(dl1,dl2)\n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#optimizr = torch.optim.Adam(net.parameters())\n",
    "lrnr = Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3: 적합 \n",
    "lrnr.fit(10)\n",
    "# Step4: 예측 및 평가 \n",
    "\n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcb9fa-8a7b-47da-85dd-be161264df9e",
   "metadata": {},
   "source": [
    "# 6. ImageNet -- 직접설계/transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ba73-626f-4852-abee-09c063cdd0e3",
   "metadata": {},
   "source": [
    "## A. 알렉스넷[@krizhevsky2012imagenet]의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9522c3-ca0b-4986-a86a-9fbc32cf5278",
   "metadata": {},
   "source": [
    "`-` 야사로 배우는 인공지능: <https://brunch.co.kr/@hvnpoet/109>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb65429-a88b-4d95-bce5-44028e04438c",
   "metadata": {},
   "source": [
    "## B. 알렉스넷의 아키텍처 써보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16687d6-7d2b-40f1-948c-37ce7d8facc7",
   "metadata": {},
   "source": [
    "`-` 알렉스넷의 아키텍처: \n",
    "\n",
    "-ref: <https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063c737-2620-4a96-a5fd-86e830b196d6",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3817fd8-1e80-4947-9ebf-706280d309a0",
   "metadata": {},
   "source": [
    "`-` 재미삼아 써보면.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "cd8bd6aa-9004-4cd6-9642-d2f121fd06b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 227, 227])"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.zeros(1,3*227*227).reshape(1,3,227,227)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "bdd5f796-6695-480b-8ed6-df39a04ea845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,96,kernel_size=(11,11),stride=4),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.MaxPool2d((3,3),stride=2), # default stride는 3\n",
    "    torch.nn.Conv2d(96,256,kernel_size=(5,5),padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((3,3),stride=2), # default stride는 3\n",
    "    torch.nn.Conv2d(256,384,kernel_size=(3,3),padding=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384,384,kernel_size=(3,3),padding=1),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.Conv2d(384,256,kernel_size=(3,3),padding=1),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.MaxPool2d((3,3),stride=2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(9216,4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(4096,4096),        \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),    \n",
    "    torch.nn.Linear(4096,1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daf05d-9924-43e3-98e8-d987e3a55d75",
   "metadata": {},
   "source": [
    "## C. 알렉스넷으로 ImageNet 적합 -- HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "f6a4287e-c7fc-40e2-8291-28dafb5bd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34923f60-be6a-4c4a-a836-eebc25cbbfa7",
   "metadata": {},
   "source": [
    "# 7. CIFAR10 -- transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef977c7e-e839-4c64-9e09-5191e324482a",
   "metadata": {},
   "source": [
    "## A. `dls` 만들자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24d546-daf3-4678-a44a-0fe01fde57e5",
   "metadata": {},
   "source": [
    "`-` X,y를 얻자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "951a8d56-993b-4f21-8623-c9ee0c184dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/cgb3/.fastai/data/cifar10/labels.txt'),Path('/home/cgb3/.fastai/data/cifar10/train'),Path('/home/cgb3/.fastai/data/cifar10/test')]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.CIFAR)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "54e33b0f-aa27-4a90-be18-9c431f1ac0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'frog',\n",
       " 'automobile',\n",
       " 'airplane',\n",
       " 'ship',\n",
       " 'deer',\n",
       " 'truck',\n",
       " 'bird']"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [str(l).split('/')[-1] for l in (path/'train').ls()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "6160ec79-b1d4-439c-90d0-e383e4c0f10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'train/{l}').ls()],axis=0).float()/255\n",
    "XX = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'test/{l}').ls()],axis=0).float()/255\n",
    "y = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'train/{l}').ls()])\n",
    "yy = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'test/{l}').ls()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "3e0a32f5-de50-4b65-8d2d-d04926b59aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32]) \t torch.float32\n",
      "torch.Size([50000]) \t\t torch.int64\n",
      "torch.Size([10000, 3, 32, 32]) \t torch.float32\n",
      "torch.Size([10000]) \t\t torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t',y.dtype)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56a233-b8c9-4be5-abca-b2e957e5e3fb",
   "metadata": {},
   "source": [
    "`-` 데이터를 시각화해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "cf15b80e-d3a9-40c5-bfbd-f6cc91ece64f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ship,6')"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylabel = [l for l in labels for fname in (path/f'train/{l}').ls()]\n",
    "i = 30002\n",
    "plt.imshow(torch.einsum('cij->ijc',X[i]))\n",
    "plt.title(f'{ylabel[i]},{y[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d0d69-02b6-4a31-a73f-2c6589ac8cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 넘.. 어려운뎅? \n",
    "- 스트레스받아.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406bfae-9074-4a20-8b3c-2e696ad6728a",
   "metadata": {},
   "source": [
    "`-` 아무튼 dls를 만들자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "32ced4a4-424f-4cbb-90b4-dd3f76fdbf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=10000)\n",
    "dls = DataLoaders(dl1,dl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ec224-a990-4b8c-afcc-40fa1df8f3c9",
   "metadata": {},
   "source": [
    "`-` 아래와 같이 쉽게 만들수도있음... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "7033e450-dec3-4e40-81c5-97b47410635e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dls = ImageDataLoaders.from_folder(path,train='train',valid='test')\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006400e-e76b-4e00-a911-405a2226d75e",
   "metadata": {},
   "source": [
    "## B. 수제네트워크로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0050fb-fe3a-47da-8854-624b7d496654",
   "metadata": {},
   "source": [
    "`-` 시도1: 이게 좀 힘들어요 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "ba19fd0d-7c2d-476a-be5f-7cc2b7e6841f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.491399</td>\n",
       "      <td>2.304589</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.404345</td>\n",
       "      <td>2.300255</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.794670</td>\n",
       "      <td>2.297755</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.466218</td>\n",
       "      <td>23.315241</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.008362</td>\n",
       "      <td>55.064445</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.679833</td>\n",
       "      <td>67.542747</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.687613</td>\n",
       "      <td>54.044514</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.861962</td>\n",
       "      <td>62.691330</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.410336</td>\n",
       "      <td>73.803596</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.461291</td>\n",
       "      <td>58.387405</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.1000\n",
      "val: 0.0999\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=10000)\n",
    "dls = DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3136,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83539a3-c107-4274-a9c1-d849c528bc9b",
   "metadata": {},
   "source": [
    "- ????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130e68a-a329-4b3e-bf50-d8231995f5f8",
   "metadata": {},
   "source": [
    "`-` 시도2: 셔플!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "eb6a6fc1-c932-4a8b-8923-23a7d215d30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.386804</td>\n",
       "      <td>1.365188</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.271729</td>\n",
       "      <td>1.294105</td>\n",
       "      <td>0.535300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.218193</td>\n",
       "      <td>1.232671</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.143906</td>\n",
       "      <td>1.180212</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>1.159967</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.081856</td>\n",
       "      <td>1.125518</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.041728</td>\n",
       "      <td>1.094741</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.035460</td>\n",
       "      <td>1.112487</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.040533</td>\n",
       "      <td>1.094738</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>1.092688</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6746\n",
      "val: 0.6173\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=10000)\n",
    "dls = DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3136,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04b1bd-6d64-47bc-b575-e1f3dbc3ab55",
   "metadata": {},
   "source": [
    "`-` 시도3: 복잡하게.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "80d4267a-1897-42ac-8103-b332bb35db1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.394630</td>\n",
       "      <td>1.359299</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.276867</td>\n",
       "      <td>1.272218</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.182427</td>\n",
       "      <td>1.185291</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.109149</td>\n",
       "      <td>1.138360</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.077146</td>\n",
       "      <td>1.140882</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.020239</td>\n",
       "      <td>1.050353</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.985863</td>\n",
       "      <td>1.020022</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.953660</td>\n",
       "      <td>1.027684</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.948453</td>\n",
       "      <td>1.033547</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.901153</td>\n",
       "      <td>0.977052</td>\n",
       "      <td>0.659900</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.7019\n",
      "val: 0.6599\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=10000)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,256,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(256,64,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(64,16,(5,5)),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1600,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1482c7c-a342-48c0-923b-73b80ea4627c",
   "metadata": {},
   "source": [
    "## C. TransferLearning으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3877433-1165-48a7-b1f1-a26206ff5c28",
   "metadata": {},
   "source": [
    "`-` ResNet18을 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "fec1f582-ee0b-4222-868f-7ac81bd3bc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/cgb3/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 111MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c87087-4b8c-4c26-9641-c24f13d03538",
   "metadata": {},
   "source": [
    "`-` 마지막의 레이어만 수정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "f0907f77-037e-44d1-8db1-3c2087c67870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.fc = torch.nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8f01d-e2aa-4999-b7a0-6fa4cff2072b",
   "metadata": {},
   "source": [
    "`-` 학습해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "ebaf1a49-349d-457a-b3f2-99137b22c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.807740</td>\n",
       "      <td>0.818521</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.661819</td>\n",
       "      <td>0.744722</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540584</td>\n",
       "      <td>0.663652</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.452428</td>\n",
       "      <td>0.668054</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.368493</td>\n",
       "      <td>0.670008</td>\n",
       "      <td>0.790700</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.296857</td>\n",
       "      <td>0.676296</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.261985</td>\n",
       "      <td>0.749436</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.217399</td>\n",
       "      <td>0.937166</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.179561</td>\n",
       "      <td>0.755643</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>0.827992</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9603\n",
      "val: 0.7943\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=10000)\n",
    "# Step2:\n",
    "net = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\n",
    "net.fc = torch.nn.Linear(512,10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff2a4a-ffa6-49cf-9de0-a03575e7cc10",
   "metadata": {},
   "source": [
    "`-` 좀 더 fastai에 가깝게.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "4f5a471e-358d-4d32-abd5-44356da16d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.244365</td>\n",
       "      <td>1.114961</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.004763</td>\n",
       "      <td>0.914378</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.872134</td>\n",
       "      <td>0.819131</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.872844</td>\n",
       "      <td>0.781726</td>\n",
       "      <td>0.722900</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.838873</td>\n",
       "      <td>0.761367</td>\n",
       "      <td>0.732600</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.768586</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>0.739900</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.755221</td>\n",
       "      <td>0.726606</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.746790</td>\n",
       "      <td>0.715648</td>\n",
       "      <td>0.749500</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.724227</td>\n",
       "      <td>0.709525</td>\n",
       "      <td>0.755400</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.729048</td>\n",
       "      <td>0.700866</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step1:\n",
    "dls = ImageDataLoaders.from_folder(path, train='train', valid='test')\n",
    "# Step2:\n",
    "lrnr = vision_learner(\n",
    "    dls=dls,\n",
    "    arch=resnet18,\n",
    "    #--#\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "# lrnr.model.to(\"cpu\")\n",
    "# print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "# print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de1df8-27e5-492a-ba1e-ed4a47fa4102",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A1. 자잘한 용어 정리 ($\\star$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392db2f-f51d-49db-9ec9-b961033fb1fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. 지도학습 \n",
    "\n",
    "`-` 우리가 수업에서 다루는 데이터는 주로 아래와 같은 느낌이다. \n",
    "\n",
    "1.  데이터는 $(X,y)$의 형태로 정리되어 있다. \n",
    "\n",
    "2.  $y$는 우리가 관심이 있는 변수이다. 즉 우리는 $y$를 적절하게 추정하는 것에 관심이 있다.\n",
    "\n",
    "3.  $X$는 $y$를 추정하기 위해 필요한 정보이다.\n",
    "\n",
    "|  $X$ = 설명변수 = 독립변수   | $y$ = 반응변수 = 종속변수  |     비고     |     순서     |           예시           |\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "|            이미지            |          카테고리          | 합성곱신경망 |   상관없음   |  개/고양이 이미지 구분   |\n",
    "|         유저,아이템          |            평점            |  추천시스템  |   상관없음   |    넷플릭스 영화추천     |\n",
    "|     과거~오늘까지의주가      |          내일주가          |  순환신경망  | 순서상관있음 |         주가예측         |\n",
    "| 처음 $m$개의 단어(혹은 문장) | 이후 1개의 단어(혹은 문장) |  순환신경망  | 순서상관있음 |     챗봇, 텍스트생성     |\n",
    "| 처음 $m$개의 단어(혹은 문장) |          카테고리          |  순환신경망  | 순서상관있음 | 영화리뷰 텍스트 감정분류 |\n",
    "\n",
    "`-` 이러한 문제상황, 즉 $(X,y)$가 주어졌을때 $X \\to y$를 추정하는 문제를 supervised learning 이라한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dfc4b-99b7-48fd-a4bb-abce8cf06675",
   "metadata": {},
   "source": [
    "## B. 모델이란? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8c88f-6e44-4daf-ae6d-31d0dff99222",
   "metadata": {},
   "source": [
    "> 모델이란 단어는 제 발작버튼이었어요.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcf657-af62-405c-a2ee-b309a0cffa2d",
   "metadata": {},
   "source": [
    "`-` 통계학에서 모델은 y와 x의 관계를 의미하며 오차항의 설계를 포함하는 개념이다. 이는 통계학이 \"데이터 = 정보 + 오차\"의 관점을 유지하기 때문이다. 따라서 통계학에서 모델링이란 \n",
    "\n",
    "$$y_i = net(x_i) + \\epsilon_i$$\n",
    "\n",
    "에서 (1) 적절한 함수 $net$를 선택하는 일 (2) 적절한 오차항 $\\epsilon_i$ 을 설계하는일 모두를 포함한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92f936-321a-4750-9f72-6ff52c0740df",
   "metadata": {},
   "source": [
    "`-` 딥러닝 혹은 머신러닝에서 모델은 단순히\n",
    "\n",
    "$$y_i \\approx net(x_i)$$\n",
    "\n",
    "를 의미하는 경우가 많다. 즉 \"model=net\"라고 생각해도 무방하다. 이 경우 \"모델링\"이란 단순히 적절한 $net$을 설계하는 것만을 의미할 경우가 많다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e9f2f-dd2b-4d79-9d9f-0ab16bbf0cf7",
   "metadata": {},
   "source": [
    "`-` 그래서 생긴일\n",
    "\n",
    "- 통계학교재 특: 분류문제와 회귀문제를 엄밀하게 구분하지 않는다. 사실 오차항만 다를뿐이지 크게보면 같은 회귀모형이라는 관점이다. 그래서 일반화선형모형(GLM)이라는 용어를 쓴다. \n",
    "- 머신러닝/딥러닝교재 특: 회귀문제와 분류문제를 구분해서 설명한다. (표도 만듦) 이는 오차항에 대한 기술을 모호하게 하여 생기는 현상이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c3aea-6667-4f41-8b99-34f9e75258c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. 학습이란? \n",
    "\n",
    "`-` 학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한 “규칙” 혹은 “원리”를 찾는 것이다.\n",
    "\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한 “맵핑”을 찾는 것이다.\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한 \"함수”을 찾는 것이다. 즉 $y\\approx f(X)$가 되도록 만드는 $f$를 잘 찾는 것이다. (이 경우 \"함수를 추정한다\"라고 표현)\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한 “모델” 혹은 \"모형\"을 찾는 것이다. 즉 $y\\approx model(X)$가 되도록 만드는 $model$을 잘 찾는 것이다. (이 경우 \"모형을 학습시킨다\"라고 표현)\n",
    "-   **학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한 “네트워크”을 찾는 것이다. 즉 $y\\approx net(X)$가 되도록 만드는 $net$을 잘 찾는 것이다. (이 경우 \"네트워크를 학습시킨다\"라고 표현)**\n",
    "\n",
    "`-` prediction이란 학습과정에서 찾은 “규칙” 혹은 “원리”를 $X$에 적용하여 $\\hat{y}$을 구하는 과정이다. 학습과정에서 찾은 규칙 혹은 원리는 $f$,$model$,$net$ 으로 생각가능한데 이에 따르면 아래가 성립한다.\n",
    "\n",
    "-   $\\hat{y} = f(X)$\n",
    "-   $\\hat{y} = model(X)$\n",
    "-   $\\hat{y} = net(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314cdd3-7293-4248-88a8-3cae06695f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## D. $\\hat{y}$를 부르는 다양한 이름\n",
    "\n",
    "`-` $\\hat{y}$는 $X$가 주어진 자료에 있는 값인지 아니면 새로운 값 인지에 따라 지칭하는 이름이 미묘하게 다르다.\n",
    "\n",
    "1.  $X \\in data$: $\\hat{y}=net(X)$ 는 predicted value, fitted value 라고\n",
    "    부른다.\n",
    "\n",
    "2.  $X \\notin data$: $\\hat{y}=net(X)$ 는 predicted value, predicted\n",
    "    value with new data 라고 부른다.\n",
    "\n",
    "`-` 경우1은 “$loss$ = $y$ 와 $\\hat{y}$ 의 차이” 를 정의할 수 있으나 경우2는 그렇지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af032a5-a5b3-4659-94d9-75498faf2051",
   "metadata": {
    "tags": []
   },
   "source": [
    "## E. 다양한 코드들 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6818b-7d16-47fb-b7e8-77fd9cfb8926",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "`-` 파이썬 코드..\n",
    "\n",
    "```Python\n",
    "#Python\n",
    "predictor.fit(X,y) # autogluon 에서 \"학습\"을 의미하는 과정\n",
    "model.fit(X,y) # sklearn 에서 \"학습\"을 의미하는 과정\n",
    "learner.learn() # fastai 에서 \"학습\"을 의미하는 과정\n",
    "learner.fine_tune(1) # fastai 에서 \"부분학습\"을 의미하는 과정\n",
    "learner.predict(cat1) # fastai 에서 \"예측\"을 의미하는 과정 \n",
    "model.fit(x, y, batch_size=32, epochs=10) # keras에서 \"학습\"을 의미하는 과정\n",
    "model.predict(test_img) # keras에서 \"예측\"을 의미하는 과정 \n",
    "```\n",
    "\n",
    "`-` R 코드..\n",
    "\n",
    "```r\n",
    "# R\n",
    "ols <- lm(y~x) # 선형회귀분석에서 학습을 의미하는 함수\n",
    "ols$fitted.values # 선형회귀분석에서 yhat을 출력 \n",
    "predict(ols, newdata=test) # 선형회귀분석에서 test에 대한 예측값을 출력하는 함수\n",
    "ols$coef # 선형회귀분석에서 weight를 확인하는 방법\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96318470-d917-4d3a-b429-fe6bf7dc15ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A2. 참고자료들 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f636e-b765-463e-b0c4-2cd5ef2ca017",
   "metadata": {},
   "source": [
    "`-` 케라스/텐서플로우: <https://guebin.github.io/STBDA2022/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8a404-651d-41c5-8fad-76588718bb5a",
   "metadata": {},
   "source": [
    "`-` 상속: <https://guebin.github.io/PP2023/posts/03_Class/2023-06-12-15wk-1.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83f4bd-8e7c-4c85-b99d-762d01d605f1",
   "metadata": {},
   "source": [
    "`-` sklearn/autogluon: <https://guebin.github.io/MP2023/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a70bc3-8fdb-49a2-8ba6-8cbf755c5f50",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 리눅스관련: <https://guebin.github.io/DSTBX2024/> -- 자료 부실함.. 강의영상 없는것 많음.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d19bd-802a-438e-b697-27dc0703f4f4",
   "metadata": {},
   "source": [
    "# A3. DNN, ANN, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307c2d0-ace4-4482-b07b-5559c4a7d8e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` DNN 은 깊은신경망, ANN 은 인공신경망, MLP 는 다층퍼셉트론이라 번역된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956b602-509f-4f55-9779-3697dfc5ee57",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 2개 있으므로 MLP라고 볼 수 있다. DNN 이라 보기는 애매하다. (그래서 이걸 얕은신경망(shallow network)이라고 표현하기도 합니다)\n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9103f-d301-4389-9b8e-6c0eda2057a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 7개 있으므로 MLP라고 볼 수 있다. 이 정도면 깊어보이니까 DNN 이라 주장할 수 있어보인다. \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),\n",
    "    torch.nn.Sigmoid(),    \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d7765-eebc-4cbe-b072-27ea257004e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 3개 있으므로 MLP라고 볼 수 있다. 이건 DNN이라고 봐야하나? 깊다기 보다는 넓은 신경망인데... \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1),\n",
    "    torch.nn.Sigmoid(),    \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcb826-d22d-40bb-8855-8c962a3c504c",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크도 ANN이라 볼 수 있다.^[그렇지만 이걸 ANN이라고 부르는 사람은 없는듯] 레이어는 2장이지만 MLP라고 부르진 않는다. \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)), # <-- 학습할 파라메터\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(2304,1), # <-- 학습할 파라메터\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53923753-9dcb-4ac4-b2a0-0d5d0ad5f6ce",
   "metadata": {},
   "source": [
    "`-` 야매개념: 요즘은 거의 ANN $\\approx$ MLP $\\approx$ DNN 의 느낌으로 이해해도 무방함\n",
    "\n",
    "- 어지간한 모형은 다 ANN이라 우길 수 있다. 회귀분석도, 로지스틱분석도 마음먹으면 ANN으로 우길 수 있다. 그래서 \"ANN을 썼다\"라는건 엄청 모호한 말이다. 이런 이유로 사람들은 거의 MLP를 쓴 경우에 ANN을 썼다고 하고, 회귀모형을 쓴 경우에는 굳이 ANN을 썼다고 표현하지 않는다. \n",
    "- MLP과 DNN은 구분이 모호하다. 하나이상의 은닉층만 포함하고 있으면 MLP라고 부를 수 있다. 적은 노드수를 유지하면서 은닉층을 여러개 쓰면 깊은 신경망이라고 하고, 많은 노드를 사용하면서 은닉층을 얇게, 그리고 노드를 많이 쓰면 넓은신경망이라고 한다. 노드수와 관계없이 층이 얇은 경우는 얕은신경망이라고 한다.^[저는 이 표현 너무 싫어해요] 즉 MLP의 모양에 따라서 \"깊은신경망\", \"얕은신경망\", \"넓은신경망\" 등의 용어를 사용한다. \n",
    "- 일반적으로 은닉층이 1개있으면 얕은신경망, 2개 이상이면 깊은신경망이라고 부른다고 합의되어있다. (은닉층이 2층까지 얕은신경망이라고 부르는 사람도 존재함) 얼마나 많은 노드부터 넓은신경망이라고 부르는지는 (제가 아는 한) 합의된바가 없다. \n",
    "얼마나 깊을때 DNN으로 부를지 명확한 합의가 되어있지 않다. (3층-MLP부터 DNN으로 부르는 방식이 지지를 얻는듯. 그렇지만 4층-MLP 부터 DNN으로 부르는 사람도 존재함.)\n",
    "- MLP의 정의가 가장 깔끔하다고 생각하지만 요즘 잘 쓰는 용어는 아니다. (MLP의 논문은 너무 예전임. 층을 세는것도 다름)\n",
    "- **제 결론**: 따지고 보자면 DNN $\\subset$ MLP $\\subset$ ANN 이다. 그렇지만 MLP이지만 DNN은 아닌 네트워크를 지칭한다든가, ANN 이지만 MLP는 아닌 네트워크를 지칭하는 일은 흔하지 않으며, 지칭하더라도 부연설명을 친절하게 해준다. 따라서 부연설명 없이 ANN, MLP, DNN 을 지칭한다면 거의 DNN을 의미한다고 봐도 무방하다. 즉 ANN $\\approx$ MLP $\\approx$ DNN 라고 보면 된다. (엄밀하게는 틀린개념이죠) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
