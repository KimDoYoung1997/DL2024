{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GYxIXZPYAxyS",
        "outputId": "f0e93018-7ad8-4314-efd5-6602e9dee261"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import time\n",
        "\n",
        "# # 4x4 그리드 환경 정의 (16개의 상태)\n",
        "# n_rows, n_cols = 4, 4\n",
        "# n_states = n_rows * n_cols\n",
        "\n",
        "# # 행동 정의 (상, 하, 좌, 우)\n",
        "# actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# # 정책 정의 (랜덤 정책으로 설정) -> 모든 상태에서 네 방향으로 같은 확률로 이동\n",
        "# policy = np.ones((n_states, len(actions))) / len(actions)\n",
        "\n",
        "# # 상태 전이 확률 및 보상\n",
        "# def transition_reward(state, action):\n",
        "#     row, col = divmod(state, n_cols)  # 상태를 그리드 좌표로 변환\n",
        "#     if action == 'up':\n",
        "#         next_row = max(row - 1, 0)\n",
        "#         next_col = col\n",
        "#     elif action == 'down':\n",
        "#         next_row = min(row + 1, n_rows - 1)\n",
        "#         next_col = col\n",
        "#     elif action == 'left':\n",
        "#         next_row = row\n",
        "#         next_col = max(col - 1, 0)\n",
        "#     elif action == 'right':\n",
        "#         next_row = row\n",
        "#         next_col = min(col + 1, n_cols - 1)\n",
        "\n",
        "#     next_state = next_row * n_cols + next_col\n",
        "#     reward = -1  # 모든 이동의 비용은 -1로 설정\n",
        "#     return next_state, reward\n",
        "\n",
        "# # 할인율\n",
        "# gamma = 0.9\n",
        "\n",
        "# # 상태 가치 함수 초기화 (모든 상태의 가치를 0으로 시작)\n",
        "# v = np.zeros(n_states)\n",
        "\n",
        "# # 미로 그리드 시각화 함수\n",
        "# def plot_grid(v, k, state=None, action=None):\n",
        "#     plt.figure(figsize=(6, 6))\n",
        "#     plt.imshow(v.reshape(n_rows, n_cols), cmap='coolwarm', interpolation='none', vmin=-20, vmax=0)\n",
        "#     plt.colorbar(label='Value')\n",
        "#     plt.title(f'Iteration: {k}, State: {state}, Action: {action}')\n",
        "\n",
        "#     for i in range(n_rows):\n",
        "#         for j in range(n_cols):\n",
        "#             state_value = v[i * n_cols + j]\n",
        "#             plt.text(j, i, f'{state_value:.1f}', ha='center', va='center', color='black')\n",
        "\n",
        "#     plt.grid(False)\n",
        "#     plt.xticks([])\n",
        "#     plt.yticks([])\n",
        "#     plt.pause(1)  # 1초 대기 (각 상태 변화를 쉽게 확인)\n",
        "#     plt.show()\n",
        "\n",
        "# # 정책 평가 함수\n",
        "# def policy_evaluation(policy, gamma=0.9, theta=1e-6):\n",
        "#     k = 0  # iteration 카운트\n",
        "#     while True:\n",
        "#         delta = 0\n",
        "#         # 모든 상태에 대해 가치 업데이트\n",
        "#         for state in range(n_states):\n",
        "#             v_new = 0\n",
        "#             # 현재 상태에서 가능한 모든 행동에 대해 기대 가치 계산\n",
        "#             for action_idx, action_prob in enumerate(policy[state]):\n",
        "#                 # 행동에 따라 다음 상태와 보상 계산\n",
        "#                 next_state, reward = transition_reward(state, actions[action_idx])\n",
        "#                 # 가치 함수 업데이트: 보상 + 할인된 다음 상태 가치\n",
        "#                 v_new += action_prob * (reward + gamma * v[next_state])\n",
        "\n",
        "#             # 최대 변화량을 기록 (수렴 여부 확인을 위해)\n",
        "#             delta = max(delta, abs(v_new - v[state]))\n",
        "#             v[state] = v_new  # 상태 가치 함수 업데이트\n",
        "\n",
        "#             # 미로 그리드를 업데이트하고 상태, 행동을 시각적으로 확인\n",
        "#             plot_grid(v, k, state, actions[action_idx])\n",
        "\n",
        "#         # 변화량이 충분히 작으면 종료\n",
        "#         if delta < theta:\n",
        "#             break\n",
        "#         k += 1  # iteration 증가\n",
        "\n",
        "#     return v\n",
        "\n",
        "# # 정책 평가 실행\n",
        "# v_pi = policy_evaluation(policy, gamma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sjlIV6nKb3cD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# 4x4 그리드 환경 정의 (16개의 상태)\n",
        "n_rows, n_cols = 4, 4\n",
        "n_states = n_rows * n_cols\n",
        "\n",
        "# 행동 정의 (상, 하, 좌, 우)\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "# 할인율b\n",
        "gamma = 0.9\n",
        "# 상태 가치 함수 초기화 (모든 상태의 가치를 0으로 시작)\n",
        "v = np.zeros(n_states)\n",
        "# 정책 정의 (랜덤 정책으로 설정) -> 모든 상태에서 네 방향으로 같은 확률로 이동\n",
        "policy = np.ones((n_states, len(actions))) / len(actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q-LWHfcb6z-",
        "outputId": "c83a1b30-a38d-4c02-8a54-c90a63248000"
      },
      "outputs": [],
      "source": [
        "policy , policy.shape , n_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u12CT_0NcK8Y",
        "outputId": "695e0fe8-ec32-473e-bdad-8d5d5d311a64"
      },
      "outputs": [],
      "source": [
        "# 상태 전이 확률 및 보상\n",
        "def transition_reward(state, action):\n",
        "    row, col = divmod(state, n_cols)  # 상태를 그리드 좌표로 변환\n",
        "    print(\"---------------------- transition_reward 함수 실행 ----------------------\")\n",
        "    print(f\"row : {row}, col : {col}\")\n",
        "    if action == 'up':\n",
        "        print(f\"case 1 의 row-1={row-1}\")\n",
        "        next_row = max(row - 1, 0)\n",
        "        next_col = col\n",
        "        print(f\"next_row : {next_row} , next_col : {next_col}\")\n",
        "\n",
        "    elif action == 'down':\n",
        "        print(f\"case 2 의 row+1={row+1} , n_rows-1={n_rows - 1}\")\n",
        "        next_row = min(row + 1, n_rows - 1) # n_rows = 4, row =\n",
        "        next_col = col\n",
        "        print(f\"next_row : {next_row} , next_col : {next_col}\")\n",
        "    elif action == 'left':\n",
        "        print(f\"case 3 의 col-1={col - 1}\")\n",
        "        next_row = row\n",
        "        next_col = max(col - 1, 0)\n",
        "        print(f\"next_row : {next_row} , next_col : {next_col}\")\n",
        "\n",
        "    elif action == 'right':\n",
        "        print(f\"case 4 의 col+1={col+1} , n_cols-1={n_cols - 1}\")\n",
        "        next_row = row\n",
        "        next_col = min(col + 1, n_cols - 1)\n",
        "        print(f\"next_row : {next_row} , next_col : {next_col}\")\n",
        "\n",
        "    next_state = next_row * n_cols + next_col\n",
        "    reward = -1  # 모든 이동의 비용은 -1로 설정\n",
        "    print(f\"next_state: {next_state} \\t reward: {reward}\")\n",
        "    print(\"---------------------- transition_reward 함수 종료 ----------------------\")\n",
        "\n",
        "    return next_state, reward\n",
        "# transition_reward(0,\"down\")\n",
        "row, col = divmod(7, n_cols)\n",
        "print(row,col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD3B69WCdb0l",
        "outputId": "b7ad1b74-ba81-410e-ec70-c6cfa88f6390"
      },
      "outputs": [],
      "source": [
        "transition_reward(0,\"down\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhWfjvXugEja",
        "outputId": "b68b9624-ff19-445e-9317-900174733f39"
      },
      "outputs": [],
      "source": [
        "transition_reward(0,\"right\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "eKfoDrpZir6Q"
      },
      "outputs": [],
      "source": [
        "# 미로 그리드 시각화 함수\n",
        "def plot_grid(v, k, state=None, action=None):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(v.reshape(n_rows, n_cols), cmap='coolwarm', interpolation='none', vmin=-20, vmax=0)\n",
        "    plt.colorbar(label='Value')\n",
        "    plt.title(f'Iteration: {k}, State: {state}, Action: {action}')\n",
        "\n",
        "    for i in range(n_rows):\n",
        "        for j in range(n_cols):\n",
        "            state_value = v[i * n_cols + j]\n",
        "            plt.text(j, i, f'{state_value:.1f}', ha='center', va='center', color='black')\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.pause(1)  # 1초 대기 (각 상태 변화를 쉽게 확인)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "YMEwRRRcr7Zk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 정책 평가 함수\n",
        "def policy_evaluation(policy, gamma=0.9, theta=1e-6):\n",
        "    k = 0  # iteration 카운트\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # 모든 상태에 대해 가치 업데이트\n",
        "        for state in range(n_states):\n",
        "            v_new = 0\n",
        "            # 현재 상태에서 가능한 모든 행동에 대해 기대 가치 계산\n",
        "            for action_idx, action_prob in enumerate(policy[state]):\n",
        "                # 행동에 따라 다음 상태와 보상 계산\n",
        "                next_state, reward = transition_reward(state, actions[action_idx])\n",
        "                # 가치 함수 업데이트: 보상 + 할인된 다음 상태 가치\n",
        "                v_new += action_prob * (reward + gamma * v[next_state])\n",
        "\n",
        "            # 최대 변화량을 기록 (수렴 여부 확인을 위해)\n",
        "            delta = max(delta, abs(v_new - v[state]))\n",
        "            v[state] = v_new  # 상태 가치 함수 업데이트\n",
        "\n",
        "            # 미로 그리드를 업데이트하고 상태, 행동을 시각적으로 확인\n",
        "            plot_grid(v, k, state, actions[action_idx])\n",
        "\n",
        "        # 변화량이 충분히 작으면 종료\n",
        "        if delta < theta:\n",
        "            break\n",
        "        k += 1  # iteration 증가\n",
        "\n",
        "    return v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "GAoH6iGCr-V1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 정책 평가 실행\n",
        "# v_pi = policy_evaluation(policy, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Izv8QZo9LS",
        "outputId": "947f21df-3368-4dfb-ce00-1ef409299117"
      },
      "outputs": [],
      "source": [
        "actions ,v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CITJwlXKmg52",
        "outputId": "33e4ca93-d0c1-4a56-f2cc-534a594ae9a3"
      },
      "outputs": [],
      "source": [
        "print(policy.shape)\n",
        "\n",
        "for state in range(n_states):\n",
        "  # print(policy[state])\n",
        "  v_new = 0\n",
        "  print(f\"state {state} start\")\n",
        "  for action_idx, action_prob in enumerate(policy[state]):\n",
        "    # print(action_idx,action_prob)\n",
        "    # print(f\"state {state} actions :{actions[action_idx]} \\t action_prob : {action_prob}\")  # action_idx 는 0,1,2,3 이고 이게 리스트 actions index로 들어간다.\n",
        "    # 행동에 따라 다음 상태와 보상 계산\n",
        "    next_state, reward = transition_reward(state, actions[action_idx])\n",
        "    print(f\"state{state} 에서 action:{actions[action_idx]}을  policy를 따라  action_prob {action_prob} 확률로 선택하여 next_state {next_state}로 진행한다.\")  # action_idx 는 0,1,2,3 이고 이게 리스트 actions index로 들어간다.\n",
        "    v_new += action_prob * (reward + gamma * v[next_state])\n",
        "    print(f\"v_new {v_new} += action_prob {action_prob} * (reward {reward} + gamma {gamma} * v[next_state] {v[next_state]})\")\n",
        "  print(f\"state {state} end\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
