{
 "cells": [
  {
   "cell_type": "raw",
   "id": "53bd0245-60b0-40bc-9fd4-d47470cbbe57",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d"
   },
   "source": [
    "---\n",
    "title: \"06wk-1: 합성곱 신경망 (2) -- MNIST, Fashion MNIST, ImageNet, CIFAR10\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/08/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeb201-7db9-4500-967c-df308017c564",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/06wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06b622-d159-4970-b217-ac4787733545",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc048cf-9338-46ea-97fa-51b2025dc2ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-wTzMRmZzvr2TL12DNo6jXF&si=RnG6_-uhn9Yb57nU >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe14a88-1313-4a92-991e-9479afc21636",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66839c05-69cf-4da5-b8a8-861e89481e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai.vision.all "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6d9f0-74a1-4c5d-b3cb-6fb61c420139",
   "metadata": {},
   "source": [
    "# 3. torch.eigensum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474cded-48c5-4cef-9941-0e4c8efcf49e",
   "metadata": {},
   "source": [
    "## A. transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d65ef-9547-4b80-96c1-442457d0e23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr = torch.tensor([1,2,3,4]).reshape(2,2)\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb899f-b74e-4754-93d3-df0be29348d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18265793-22d8-4daa-9164-58d3f656e9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.einsum('ij->ji',tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f9298-4ed2-4b04-9eac-a096723a37f0",
   "metadata": {},
   "source": [
    "## B. 행렬곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87678d33-7443-430a-bb1b-8969b398c792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr1 = torch.arange(12).reshape(4,3).float()\n",
    "tsr2 = torch.arange(15).reshape(3,5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701aaf62-9245-44e0-8c5b-a82726e2ddaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc65bc-8115-4304-a74a-add116f4ca1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4f7c8-d189-4f1e-afdd-f04c3e15a3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsr1 @ tsr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr1.matmul(tsr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01990c-424f-49ec-8c53-22d9fde57c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.einsum('ij,jk -> ik',tsr1,tsr2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb991c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(0,4)\n",
    "B = torch.arange(2,6)\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "C = torch.einsum('i,i->',A,B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(0,4).reshape(4,1)\n",
    "B = torch.arange(2,6).reshape(4,1)\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "C = torch.einsum('ij,ij->ij',A,B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afe361",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(0,4)\n",
    "_A=A.reshape(2,2)\n",
    "torch.einsum('i->',A),torch.einsum('ij->',_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3cd78-c89f-4300-9cfb-2464c1e06856",
   "metadata": {},
   "source": [
    "## C. 이미지변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.zeros(16).reshape(4,4) + 1.0\n",
    "g = torch.zeros(16).reshape(4,4) + 0.5\n",
    "b = torch.zeros(16).reshape(4,4) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([r,g,b],axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt 에서 보려면 channel이 맨 마지막에 와야한다.\n",
    "plt.imshow(torch.stack([r,g,b],axis=-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([r,g,b],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb252af0-eed5-493c-81e1-1cc245b4003b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = torch.zeros(16).reshape(4,4) + 1.0\n",
    "g = torch.zeros(16).reshape(4,4)\n",
    "b = torch.zeros(16).reshape(4,4)\n",
    "img_plt = torch.stack([r,g,b],axis=-1) # matplotlib 를 쓰기 위해서는 이미지가 이렇게 저장되어있어야한다.  \n",
    "img_torch = torch.stack([r,g,b],axis=0).reshape(1,3,4,4) # torch를 쓰기 위해서는 이미지가 이렇게 저장되어있어야한다.  1: obs 2: ch 3: width 4: height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573c8b5-5e5f-4eaf-8e32-68aa109c63b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 두 데이터의 차원이 다르다.\n",
    "img_plt.shape, img_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b76f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError: Invalid shape (1, 3, 4, 4) for image data\n",
    "plt.imshow(img_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8248398-ed82-4409-9cd4-ed9e28514d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead43ddd-d7be-4e2c-986a-858e91b90f8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "만약에 `img_torch`를 matplotlib 으로 보고싶다면? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a8100-0d28-452d-a649-5933d86aa57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 잘못된코드\n",
    "plt.imshow(img_torch.reshape(4,4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e2996-aa45-4b1d-a415-6626f0bb3ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 올바른코드\n",
    "plt.imshow(torch.einsum('ocij -> ijc',img_torch))   # shape (1, 3, 4, 4) -> (4, 4, 3) 으로 einsum을 이용하여 reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른코드\n",
    "plt.imshow(torch.einsum('ocwh->whc',img_torch)) # shape (1, 3, 4, 4) -> (4, 4, 3) 으로 einsum을 이용하여 reshape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82940c-f262-490e-9b3c-556640e34728",
   "metadata": {},
   "source": [
    "# 4. MNIST -- 직접설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e77dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x71f3ff0cc820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "plt.imshow(torchvision.io.read_image(path/'training/0/41447.png').reshape(28,28,1),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84efbf3a-ab82-4e99-9bd1-f908a2da2919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2])/255        # X를 0~1 사이의 float로 변환\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()  # y는 float\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\n",
    "XX = torch.concat([X0,X1,X2])/255\n",
    "yy = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52a425f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([980, 1, 28, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a61612a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18623, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([18623, 3]) \t torch.float32\n",
      "torch.Size([3147, 1, 28, 28]) \t torch.float32\n",
      "torch.Size([3147, 3]) \t torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,'\\t',X.dtype)     # X 가 int 이면 torch에서 학습이 불가능하다. 강의 11분30초에서 언급\n",
    "print(y.shape,'\\t',y.dtype)     # onehot encoding 할거기에 y는 float타입\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 꼭 X 를 255로 나눠줄 필요는 없긴하다. X가 float 타입이고 y또한 onehot방식이니 float 타입이기만 하면 된다.\n",
    "# path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "# X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "# X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "# X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "# X = torch.concat([X0,X1,X2]).float()        # X를 0~1 사이의 float로 변환\n",
    "# y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()  # y는 float\n",
    "# X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\n",
    "# X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\n",
    "# X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\n",
    "# XX = torch.concat([X0,X1,X2]).float()\n",
    "# yy = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0.shape , X.shape , X.dtype , y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.read_image((path/'training/0').ls()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.read_image((path/'training/0').ls()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('cwh->whc',X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.read_image((path/'training/0').ls()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.io.read_image((path/'training/0').ls()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.io.read_image((path/'training/0').ls()[0]).reshape(28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32476c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(torchvision.io.read_image((path/'training/0').ls()[0]).reshape(28,28,1))\n",
    "plt.imshow(torch.einsum('cwh->whc',torchvision.io.read_image((path/'training/0').ls()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9042f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST) \n",
    "[torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404394e-174e-4134-8e9b-c42abb485e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('cij -> ijc',X[0]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81058d3d-855b-47cc-bfdf-b6e70a7def2a",
   "metadata": {},
   "source": [
    "## A. y: (n,3)-float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6d4ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2])/255        # X를 0~1 사이의 float로 변환\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()  # y는 float\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\n",
    "XX = torch.concat([X0,X1,X2])/255\n",
    "yy = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce223c36-98b7-442c-9aaf-9894bba83a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,3),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y.argmax(axis=1)).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy.argmax(axis=1)).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0713c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=128)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=128)\n",
    "\n",
    "net=torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,kernel_size=(2,2)),    # torch.Size([18623, 16, 27, 27])\n",
    "    torch.nn.ReLU(),                            # torch.Size([18623, 16, 27, 27])\n",
    "    torch.nn.MaxPool2d(kernel_size=(2,2)),      # torch.Size([18623, 16, 13, 13])\n",
    "    torch.nn.Flatten(),                         # torch.Size([18623, 2704])\n",
    "    torch.nn.Linear(2704,3),                    # torch.Size([18623, 3])\n",
    "    # softmax\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl1:\n",
    "        yhat=net(xi.to(\"cuda:0\"))\n",
    "        loss=loss_fn(yhat,yi.to(\"cuda:0\"))\n",
    "        loss.backward()\n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "print(f'train: {(net(X).data.argmax(axis=-1) == y.argmax(axis=-1)).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=-1) == yy.argmax(axis=-1)).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79953c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f'x_batch={len(xi.tolist())} \\t y_batch={len(yi.tolist())}')\n",
    "    \n",
    "# for xi,yi in dl:\n",
    "#     print(f'x_batch={xi.tolist()} \\t y_batch={yi.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06923f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape , X[[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f72627",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b875f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21a270-bfe0-4659-8d61-2483478add8d",
   "metadata": {},
   "source": [
    "## B. y: (n,)-int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape , y , y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ec864-ea1c-40bc-8734-915303281364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y를 int로 형변환.\n",
    "y = y.argmax(axis=-1)\n",
    "yy = yy.argmax(axis=-1)\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t',y.dtype)   # 원핫 인코딩이 아니라 y가 int 타입\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261ae34-4574-43ef-a63e-2b9cbdf12121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t',y.dtype)   # 원핫 인코딩이 아니라 y가 int 타입\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e32ab1a-44cd-40d3-8344-03b17150d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step1: 데이터정리 (dls생성)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(ds,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m) \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step2: 적합에 필요한 오브젝트 생성\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/utils/data/dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 206\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,3),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "# print(f'train: {(net(X).data.argmax(axis=-1) == y.argmax(axis=-1)).float().mean():.4f}')\n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}') # <-- 여기수정\n",
    "\n",
    "# print(f'val: {(net(XX).data.argmax(axis=-1) == yy.argmax(axis=-1)).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}') # <-- 여기수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001aad33",
   "metadata": {},
   "source": [
    "## C. y: (n,)-int , 클래스가 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62113507",
   "metadata": {},
   "source": [
    "### torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01d2a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[str(labels).split('/')[-1] for labels in (path/'training').ls()]\n",
    "labels = sorted(labels)\n",
    "\n",
    "X = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in ((path/f'training/{l}').ls())]).float()/255\n",
    "y = torch.tensor([i for i,l in enumerate(labels) for fname in ((path/f'training/{l}').ls())])\n",
    "XX = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in ((path/f'testing/{l}').ls())]).float()/255\n",
    "yy = torch.tensor([i for i,l in enumerate(labels) for fname in ((path/f'testing/{l}').ls())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fcfc58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 1, 28, 28]),\n",
       " torch.float32,\n",
       " torch.Size([60000]),\n",
       " torch.int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , X.dtype ,y.shape , y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c0429c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 정확도 : 0.9864166975021362\n"
     ]
    }
   ],
   "source": [
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=128,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=128)\n",
    "\n",
    "\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,kernel_size=(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=(2,2)),\n",
    "    torch.nn.Flatten()\n",
    ") \n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,10)\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1,\n",
    "    net2\n",
    ").to(\"cuda:0\")\n",
    "loss_fn  = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "for epoc in range(5):\n",
    "    for x_i,y_i in dl1:\n",
    "        yhat = net(x_i.to(\"cuda:0\"))\n",
    "        loss = loss_fn(yhat,y_i.to(\"cuda:0\"))\n",
    "        loss.backward()\n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "\n",
    "print(f' 정확도 : {(net(X.to(\"cuda:0\")).argmax(-1) == y.to(\"cuda:0\")).float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06f0838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X[sample]의 inference : 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x71f3daf388e0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 57000\n",
    "net.to(\"cpu\")\n",
    "print(f' X[sample]의 inference : {net(torch.stack([X[sample]])).argmax()}')\n",
    "plt.imshow(torch.einsum('ijkl -> kl', torch.stack([X[sample]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08cfbd",
   "metadata": {},
   "source": [
    "### fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "617b6536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.192893</td>\n",
       "      <td>0.145338</td>\n",
       "      <td>0.959700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.099630</td>\n",
       "      <td>0.087165</td>\n",
       "      <td>0.973600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078297</td>\n",
       "      <td>0.067179</td>\n",
       "      <td>0.979500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063822</td>\n",
       "      <td>0.057957</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.054744</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049633</td>\n",
       "      <td>0.049708</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.048298</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.041591</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034571</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9920\n",
      "val: 0.9860\n"
     ]
    }
   ],
   "source": [
    "import fastai.metrics\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(5) \n",
    "# 데이터셋 정의\n",
    "ds1 = torch.utils.data.TensorDataset(X, y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX, yy)\n",
    "\n",
    "# 데이터로더 정의\n",
    "dl1 = torch.utils.data.DataLoader(ds1, batch_size=200, shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2, batch_size=128)\n",
    "\n",
    "# fastai DataLoader\n",
    "dls = fastai.data.core.DataLoaders(dl1, dl2)\n",
    "\n",
    "# 네트워크 정의 (Batch Normalization 추가)\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 16, kernel_size=(5, 5)),\n",
    "    # torch.nn.BatchNorm2d(16),  # BatchNorm2d 추가\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304, 10)\n",
    ")\n",
    "\n",
    "# 두 개의 네트워크를 이어 붙인 전체 모델\n",
    "net_fast = torch.nn.Sequential(\n",
    "    net1,\n",
    "    net2\n",
    ")\n",
    "\n",
    "# 손실 함수 정의\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Learner 정의\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net_fast,\n",
    "    loss_func=loss_fn,\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "lrnr.fit(10)\n",
    "\n",
    "# 모델을 CPU로 이동하여 예측 정확도 출력\n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08e099-598c-4971-bd36-76973f366c3b",
   "metadata": {},
   "source": [
    "# 5. Fashion-MNIST -- fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f02c98-b785-41ec-8ed1-5b77688dd1e9",
   "metadata": {},
   "source": [
    "`-` Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "155edf23-35db-4ac3-af9e-3a177e358944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('https://media.githubusercontent.com/media/guebin/PP2023/main/posts/fashion-mnist_train.csv')\n",
    "df_test=pd.read_csv('https://media.githubusercontent.com/media/guebin/PP2023/main/posts/fashion-mnist_test.csv')\n",
    "def rshp(row):\n",
    "    return row.reshape(1,28,28)\n",
    "X = torch.tensor(np.apply_along_axis(rshp,axis=1,arr=np.array(df_train.iloc[:,1:]))).float()\n",
    "XX  = torch.tensor(np.apply_along_axis(rshp,axis=1,arr=np.array(df_test.iloc[:,1:]))).float()\n",
    "y = torch.tensor(np.array(df_train.label))\n",
    "yy  = torch.tensor(np.array(df_test.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf37c9-6418-44da-9575-bfd755b7838d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t\\t',y.dtype)     # dtype 이 int 형 (one hot encoding형태가 아님)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t\\t',yy.dtype)   # dtype 이 int 형 (one hot encoding형태가 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719fe6c-fc81-4259-8089-e9e49adae0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('cij -> ijc',X[0]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('cij -> ijc',X[-1]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea38dd8",
   "metadata": {},
   "source": [
    "y의 class는 총 10개. 따라서 softmax도 최종 출력층은 10개가 되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b510450",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402659db-7c3f-4fbc-834b-0ce8c5b06006",
   "metadata": {},
   "source": [
    "## A. torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "18a1516f-34c6-4802-b79b-8c8bc2f38688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9150\n",
      "val: 0.8744\n"
     ]
    }
   ],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=128) \n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,10),   # 10으로 최종 클래스 바꿈\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "# Step3: 적합 \n",
    "net.to(\"cuda:0\")\n",
    "for epoc in range(10):\n",
    "    for xi,yi in dl:\n",
    "        ## 1\n",
    "        ## 2\n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        ## 3 \n",
    "        loss.backward()\n",
    "        ## 4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "net.to(\"cpu\")\n",
    "# Step4: 예측 및 평가 \n",
    "print(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e757b3c-1e4f-4acb-8ec8-47175f4cd776",
   "metadata": {},
   "source": [
    "## B. fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "315bf4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.75"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.learner.Learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508e991-9faf-4c0f-9f44-f28a0b4486c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step1: 데이터정리 (dls생성)\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=128) \n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)   # validation인데도 batch_size가 있는 이유? : 효율적인 gpu 메모리를 사용하기 위해. (학습속도를 빠르게 하려는건 아니다.)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2: 적합에 필요한 오브젝트 생성\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2304,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#optimizr = torch.optim.Adam(net.parameters())\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    # torch.Size([60000, 1, 28, 28])\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3: 적합 \n",
    "lrnr.fit(10)\n",
    "# Step4: 예측 및 평가 \n",
    "\n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lrnr.model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcb9fa-8a7b-47da-85dd-be161264df9e",
   "metadata": {},
   "source": [
    "# 6. ImageNet -- 직접설계/transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ba73-626f-4852-abee-09c063cdd0e3",
   "metadata": {},
   "source": [
    "## A. 알렉스넷[@krizhevsky2012imagenet]의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9522c3-ca0b-4986-a86a-9fbc32cf5278",
   "metadata": {},
   "source": [
    "`-` 야사로 배우는 인공지능: <https://brunch.co.kr/@hvnpoet/109>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb65429-a88b-4d95-bce5-44028e04438c",
   "metadata": {},
   "source": [
    "## B. 알렉스넷의 아키텍처 써보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16687d6-7d2b-40f1-948c-37ce7d8facc7",
   "metadata": {},
   "source": [
    "`-` 알렉스넷의 아키텍처: \n",
    "\n",
    "-ref: <https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20the%20name%20of,at%20the%20University%20of%20Toronto.>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063c737-2620-4a96-a5fd-86e830b196d6",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3817fd8-1e80-4947-9ebf-706280d309a0",
   "metadata": {},
   "source": [
    "`-` 재미삼아 써보면.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bd6aa-9004-4cd6-9642-d2f121fd06b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = torch.zeros(1,3*227*227).reshape(1,3,227,227)\n",
    "img.shape , img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('ocij ->ijc',img),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eea0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = torch.zeros(1,3*227*227).reshape(1,3,227,227)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,96,kernel_size=(11,11),stride=4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=(3,3),stride=2), # default stride는 3\n",
    "    torch.nn.Conv2d(96,256,kernel_size=(5,5),padding=2), # image의 width 와 height 를 그대로 27x27로 유지하기 위해 padding\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=(3,3),stride=2),\n",
    "    torch.nn.Conv2d(256,384,kernel_size=(3,3),padding=1,),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384,384,kernel_size=(3,3),padding=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384,256,kernel_size=(3,3),padding=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=(3,3),stride=2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(9216,4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(4096,4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(4096,1000),\n",
    ")\n",
    "#net(img).shape , plt.imshow(torch.einsum('ocij ->ijc',img),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b2c8a-2331-4d06-b339-a0608a0dab44",
   "metadata": {},
   "source": [
    "`-` 참고사항: `torchvision.models.alexnet()`을 이용하여 네크워크를 선언할 수도 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389e428-6367-4868-b7af-b77351af8dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchvision.models.alexnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58e221-4538-4faf-98d8-fc17d8d6d5da",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "직접구현한 알렉스넷과 `torchvision.models.alexnet()`를 이용한 알렉스넷은 약간다름. \n",
    "\n",
    "그 이유는 파이토치에서는 원래 논문에서 구현된 알렉스넷이 아니라 이후 수정된 알렉스넷을 사용하기 때문임. 이 내용은 파이토치 공식홈페이지에서 아래와 같이 명시되어있음. \n",
    "\n",
    "```\n",
    "AlexNet was originally introduced in the ImageNet Classification with Deep Convolutional Neural Networks paper. Our implementation is based instead on the “One weird trick” paper above.\n",
    "```\n",
    "\n",
    "ref: <https://pytorch.org/vision/main/models/generated/torchvision.models.alexnet.html>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daf05d-9924-43e3-98e8-d987e3a55d75",
   "metadata": {},
   "source": [
    "## C. 알렉스넷으로 ImageNet 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4287e-c7fc-40e2-8291-28dafb5bd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pass # 데이터가 너무 커서.. 코랩에서 못할것같아요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34923f60-be6a-4c4a-a836-eebc25cbbfa7",
   "metadata": {},
   "source": [
    "# 7. CIFAR10 -- transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef977c7e-e839-4c64-9e09-5191e324482a",
   "metadata": {},
   "source": [
    "## A. `dls` 만들자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24d546-daf3-4678-a44a-0fe01fde57e5",
   "metadata": {},
   "source": [
    "`-` X,y를 얻자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951a8d56-993b-4f21-8623-c9ee0c184dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/myuser/.fastai/data/cifar10/train'),Path('/home/myuser/.fastai/data/cifar10/test'),Path('/home/myuser/.fastai/data/cifar10/labels.txt')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.CIFAR)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/myuser/.fastai/data/cifar10/labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99349b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c05c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path('/home/myuser/.fastai/data/cifar10/train').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[str(l).split('/')[-1] for l in (path/'train').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[fname for fname in (path/'train'/f'{labels[0]}').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type([fname for fname in Path('/home/myuser/.fastai/data/cifar10/train').ls()]) , type(list(Path('/home/myuser/.fastai/data/cifar10/train').ls()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d247a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[fname for fname in Path('/home/myuser/.fastai/data/cifar10/train').ls()] == list(Path('/home/myuser/.fastai/data/cifar10/train').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.read_image(Path('/home/myuser/.fastai/data/cifar10/train/cat/10255_cat.png'))/255 # float로 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.einsum('cij->ijc',torchvision.io.read_image(Path('/home/myuser/.fastai/data/cifar10/train/cat/10255_cat.png'))))\n",
    "_cat1=torch.einsum('cij->ijc',torchvision.io.read_image(Path('/home/myuser/.fastai/data/cifar10/train/cat/10255_cat.png')))\n",
    "_cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([_cat1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torchvision.io.read_image(str(fname)) for fname in (Path('/home/myuser/.fastai/data/cifar10/train/cat').ls())])\n",
    "# X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.stack(torchvision.io.read_image(str(list(Path('/home/myuser/.fastai/data/cifar10/train/cat').ls()))))\n",
    "# 리스트의 각 파일 경로에 대해 torchvision.io.read_image를 호출하여 텐서를 반환하고, 이를 torch.stack으로 쌓음\n",
    "torch.stack(list(map(lambda fname: torchvision.io.read_image(str(fname)), Path('/home/myuser/.fastai/data/cifar10/train/cat').ls())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[label for label in Path('/home/myuser/.fastai/data/cifar10/train').ls()] == list(Path('/home/myuser/.fastai/data/cifar10/train').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e33b0f-aa27-4a90-be18-9c431f1ac0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frog',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'cat',\n",
       " 'airplane',\n",
       " 'automobile',\n",
       " 'ship',\n",
       " 'deer',\n",
       " 'bird',\n",
       " 'truck']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [str(l).split('/')[-1] for l in (path/'train').ls()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889ba2f",
   "metadata": {},
   "source": [
    "labels = [str(l).split('/')[-1] for l in (path/'train').ls()] 에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('/home/myuser/.fastai/data/cifar10/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11855994",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('/home/myuser/.fastai/data/cifar10/train').ls()    # list와 비슷함 따라서 comprehension도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Path('/home/myuser/.fastai/data/cifar10/train').ls()]    # list와 비슷함 따라서 comprehension도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba74bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[label for label in Path('/home/myuser/.fastai/data/cifar10/train').ls()]    # list와 비슷함 따라서 comprehension도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196944c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(label) for label in Path('/home/myuser/.fastai/data/cifar10/train').ls()]    # list와 비슷함 따라서 comprehension도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(label).split('/') for label in Path('/home/myuser/.fastai/data/cifar10/train').ls()]    # list와 비슷함 따라서 comprehension도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b20af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ship',\n",
       " 'deer',\n",
       " 'airplane',\n",
       " 'cat',\n",
       " 'truck',\n",
       " 'bird',\n",
       " 'frog',\n",
       " 'dog',\n",
       " 'automobile',\n",
       " 'horse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [str(l).split('/')[-1] for l in (path/'train').ls()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l='cat'\n",
    "fname='10255_cat.png'\n",
    "torchvision.io.read_image(f'/home/myuser/.fastai/data/cifar10/train/{l}/{fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "path , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6160ec79-b1d4-439c-90d0-e383e4c0f10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'train/{l}').ls()],axis=0).float()/255\n",
    "XX = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'test/{l}').ls()],axis=0).float()/255\n",
    "y = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'train/{l}').ls()])\n",
    "yy = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'test/{l}').ls()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e769786",
   "metadata": {},
   "outputs": [],
   "source": [
    "y , y.dtype , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a32f5-de50-4b65-8d2d-d04926b59aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.shape,'\\t',X.dtype)\n",
    "print(y.shape,'\\t\\t\\t',y.dtype)\n",
    "print(XX.shape,'\\t',XX.dtype)\n",
    "print(yy.shape,'\\t\\t\\t',yy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56a233-b8c9-4be5-abca-b2e957e5e3fb",
   "metadata": {},
   "source": [
    "`-` 데이터를 시각화해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15b80e-d3a9-40c5-bfbd-f6cc91ece64f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ylabel = [l for l in labels for fname in (path/f'train/{l}').ls()]\n",
    "i = 30002\n",
    "plt.imshow(torch.einsum('cij->ijc',X[i]))\n",
    "plt.title(f'{ylabel[i]},{y[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d0d69-02b6-4a31-a73f-2c6589ac8cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 그림이 너무 어려운데? \n",
    "- 맞추기 힘들겠는데.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406bfae-9074-4a20-8b3c-2e696ad6728a",
   "metadata": {},
   "source": [
    "`-` dls를 만들자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ced4a4-424f-4cbb-90b4-dd3f76fdbf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ec224-a990-4b8c-afcc-40fa1df8f3c9",
   "metadata": {},
   "source": [
    "`-` 아래와 같이 쉽게 만들수도있음... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033e450-dec3-4e40-81c5-97b47410635e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dls = fastai.vision.data.ImageDataLoaders.from_folder(path,train='train',valid='test')\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006400e-e76b-4e00-a911-405a2226d75e",
   "metadata": {},
   "source": [
    "## B. 수제네트워크로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0050fb-fe3a-47da-8854-624b7d496654",
   "metadata": {},
   "source": [
    "`-` 시도1: 이게 좀 힘들어요 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ba19fd0d-7c2d-476a-be5f-7cc2b7e6841f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.426586</td>\n",
       "      <td>2.300822</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.402122</td>\n",
       "      <td>2.298293</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.512521</td>\n",
       "      <td>2.295981</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.823177</td>\n",
       "      <td>2.297400</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.296788</td>\n",
       "      <td>2.272974</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.678270</td>\n",
       "      <td>2.283129</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.456698</td>\n",
       "      <td>2.278770</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.426718</td>\n",
       "      <td>2.258300</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.281897</td>\n",
       "      <td>3.010326</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.208364</td>\n",
       "      <td>4.467347</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.1612\n",
      "val: 0.1609\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3136,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83539a3-c107-4274-a9c1-d849c528bc9b",
   "metadata": {},
   "source": [
    "- train loss 는 줄어드는데 validation loss 는 늘어난다?  \n",
    "    - 전형적인 overfitting이 일어났다는 말\n",
    "    - 모델이 너무 복잡한가보다.. 모델을 더 간단하게 만들어볼까?\n",
    "    - 아니면 dropout을 해볼까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130e68a-a329-4b3e-bf50-d8231995f5f8",
   "metadata": {},
   "source": [
    "`-` 시도2: 셔플!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "eb6a6fc1-c932-4a8b-8923-23a7d215d30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.572572</td>\n",
       "      <td>1.494848</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.404309</td>\n",
       "      <td>1.395495</td>\n",
       "      <td>0.508800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.321636</td>\n",
       "      <td>1.328628</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.284444</td>\n",
       "      <td>1.292992</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.241136</td>\n",
       "      <td>1.270820</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.199869</td>\n",
       "      <td>1.230174</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.169258</td>\n",
       "      <td>1.217521</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.136352</td>\n",
       "      <td>1.168456</td>\n",
       "      <td>0.596200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.111384</td>\n",
       "      <td>1.159994</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.104794</td>\n",
       "      <td>1.164575</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6313\n",
      "val: 0.5997\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "torch.manual_seed(5)\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,16,(5,5)),\n",
    "    # torch.nn.BatchNorm2d(16),  # BatchNorm2d 추가\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3136,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22577c8c-f8fc-4a2e-ba23-2687f902a189",
   "metadata": {},
   "source": [
    "- 셔플의 차이가 이렇게 크다니??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04b1bd-6d64-47bc-b575-e1f3dbc3ab55",
   "metadata": {},
   "source": [
    "`-` 시도3: 복잡하게.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "80d4267a-1897-42ac-8103-b332bb35db1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.02% [2/196 00:00&lt;00:03 2.3055]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6752\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,256,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(256,64,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(64,16,(5,5)),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1600,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "# lrnr.model.to(\"cpu\")\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y), batch_size=64)\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "lrnr.model.to(\"cuda\")  # 모델을 GPU로 이동\n",
    "\n",
    "for xb, yb in train_loader:\n",
    "    xb, yb = xb.to(\"cuda\"), yb.to(\"cuda\")\n",
    "    preds = lrnr.model(xb).data.argmax(axis=1)\n",
    "    train_correct += (preds == yb).sum().item()\n",
    "    train_total += yb.size(0)\n",
    "\n",
    "train_accuracy = train_correct / train_total\n",
    "print(f'train: {train_accuracy:.4f}')\n",
    "\n",
    "\n",
    "# # 코랩사용시 아래는 주석처리할것 (이유: 코랩의 RAM이 충분하지 않음) valiation set의 accuracy는 fastai결과로 확인할것. \n",
    "# print(f'train: {(lrnr.model(X.to(\"cuda:0\")).data.argmax(axis=1) == y.to(\"cuda:0\")).float().mean():.4f}')\n",
    "# print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2429dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.464931</td>\n",
       "      <td>1.378294</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.291652</td>\n",
       "      <td>1.247403</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.193617</td>\n",
       "      <td>1.182798</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.121722</td>\n",
       "      <td>1.158104</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.073851</td>\n",
       "      <td>1.135512</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.028479</td>\n",
       "      <td>1.036952</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.996343</td>\n",
       "      <td>1.126535</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.982785</td>\n",
       "      <td>1.028916</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.949030</td>\n",
       "      <td>1.011770</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.926221</td>\n",
       "      <td>1.004633</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,256,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(256,64,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(64,16,(5,5)),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1600,10),\n",
    ")\n",
    "net = torch.nn.Sequential(\n",
    "    net1, # 2d-part\n",
    "    net2, # 1d-part \n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "# 코랩사용시 아래는 주석처리할것 (이유: 코랩의 RAM이 충분하지 않음) valiation set의 accuracy는 fastai결과로 확인할것. \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1482c7c-a342-48c0-923b-73b80ea4627c",
   "metadata": {},
   "source": [
    "## C. TransferLearning으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3877433-1165-48a7-b1f1-a26206ff5c28",
   "metadata": {},
   "source": [
    "`-` ResNet18을 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1f582-ee0b-4222-868f-7ac81bd3bc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet18()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c87087-4b8c-4c26-9641-c24f13d03538",
   "metadata": {},
   "source": [
    "`-` 마지막의 레이어만 수정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0907f77-037e-44d1-8db1-3c2087c67870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.fc = torch.nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8f01d-e2aa-4999-b7a0-6fa4cff2072b",
   "metadata": {},
   "source": [
    "`-` 학습해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebaf1a49-349d-457a-b3f2-99137b22c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.151441</td>\n",
       "      <td>1.799495</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.916356</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.791128</td>\n",
       "      <td>0.932131</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696242</td>\n",
       "      <td>0.882637</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.567153</td>\n",
       "      <td>0.986475</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.498813</td>\n",
       "      <td>0.924379</td>\n",
       "      <td>0.714600</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.409941</td>\n",
       "      <td>0.854304</td>\n",
       "      <td>0.732300</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>0.873051</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.291597</td>\n",
       "      <td>0.996725</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.227251</td>\n",
       "      <td>0.975937</td>\n",
       "      <td>0.749100</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9385\n",
      "val: 0.7491\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net = torchvision.models.resnet18()\n",
    "net.fc = torch.nn.Linear(512,10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "# 코랩사용시 아래는 주석처리할것 (이유: 코랩의 RAM이 충분하지 않음) valiation set의 accuracy는 fastai결과로 확인할것. \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}') # \n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1e04d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.160900</td>\n",
       "      <td>1.206902</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.939721</td>\n",
       "      <td>1.369956</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.757404</td>\n",
       "      <td>0.970610</td>\n",
       "      <td>0.670700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.693608</td>\n",
       "      <td>1.580133</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.597326</td>\n",
       "      <td>0.958859</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.518641</td>\n",
       "      <td>0.858398</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.427724</td>\n",
       "      <td>0.875215</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.339022</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.301873</td>\n",
       "      <td>0.846702</td>\n",
       "      <td>0.759900</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.230715</td>\n",
       "      <td>1.050875</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9029\n",
      "val: 0.7334\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net = torchvision.models.resnet18()\n",
    "net.fc = torch.nn.Linear(512,10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fit(10)\n",
    "# Step4: \n",
    "# 코랩사용시 아래는 주석처리할것 (이유: 코랩의 RAM이 충분하지 않음) valiation set의 accuracy는 fastai결과로 확인할것. \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}') # \n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df10a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.350407</td>\n",
       "      <td>1.307775</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.901432</td>\n",
       "      <td>1.186770</td>\n",
       "      <td>0.586200</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.894174</td>\n",
       "      <td>0.959755</td>\n",
       "      <td>0.658500</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.821100</td>\n",
       "      <td>1.023855</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.706292</td>\n",
       "      <td>0.921987</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.545681</td>\n",
       "      <td>0.870398</td>\n",
       "      <td>0.714700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.426122</td>\n",
       "      <td>0.865029</td>\n",
       "      <td>0.737200</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.303252</td>\n",
       "      <td>0.824686</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.125592</td>\n",
       "      <td>0.893004</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.979585</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027929</td>\n",
       "      <td>1.007130</td>\n",
       "      <td>0.785600</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9985\n",
      "val: 0.7857\n"
     ]
    }
   ],
   "source": [
    "# Step1:\n",
    "ds1 = torch.utils.data.TensorDataset(X,y)\n",
    "ds2 = torch.utils.data.TensorDataset(XX,yy)\n",
    "dl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\n",
    "dl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\n",
    "dls = fastai.data.core.DataLoaders(dl1,dl2)\n",
    "# Step2:\n",
    "net = torchvision.models.resnet18()\n",
    "# net.fc = torch.nn.Linear(512,10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrnr = fastai.learner.Learner(\n",
    "    dls=dls,\n",
    "    model=net,\n",
    "    loss_func=loss_fn,\n",
    "    #--#\n",
    "    metrics=[fastai.metrics.accuracy]\n",
    ")\n",
    "# Step3:\n",
    "lrnr.fine_tune(10)  # fine_tune\n",
    "# Step4: \n",
    "# 코랩사용시 아래는 주석처리할것 (이유: 코랩의 RAM이 충분하지 않음) valiation set의 accuracy는 fastai결과로 확인할것. \n",
    "lrnr.model.to(\"cpu\")\n",
    "print(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}') # \n",
    "print(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621ba12-a395-4fb5-9d2a-58862bd05855",
   "metadata": {},
   "source": [
    "::: {.callout-caution}\n",
    "통계학과서버를 이용하시는 분들은 다른 학생들을 위하여 실습이 끝난이후 커널을 죽여주시기 바랍니다. 그렇지 않으면 GPU메모리 부족으로 다른학생들이 실습하기 어렵습니다.(무슨말인지 모르겠으면 저에게 물어보세요)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a3b87-384b-4e21-8f97-5329aff3c623",
   "metadata": {},
   "source": [
    "# 8. HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3fe38-5681-496d-b57b-8db1cf12e799",
   "metadata": {},
   "source": [
    "없어요.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
