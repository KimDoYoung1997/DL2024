{
 "cells": [
  {
   "cell_type": "raw",
   "id": "51e1d740-b82a-4af2-a0d9-394ed0756728",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d"
   },
   "source": [
    "---\n",
    "title: \"05wk-1: 깊은 신경망 (4) -- GPU 사용법, SGD, Softmax와 CrossEntropy\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/01/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d0a99-db20-4891-b2e2-14080c1d8a43",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/05wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85595e0d-d9a6-4e79-a1e6-8ce6d7465d08",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a52785-46dd-46f5-a7b5-9879287cc580",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-wjNGgd4gmQJbQ66NLjUC2y&si=dusDZAwGOJS9TOKJ >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53144898-ba7e-4f40-b9c4-77020801a055",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84306e3-564a-472e-ae0a-5322379da476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import fastai.data.all "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59a45c-2115-47c9-bbc7-f07f66d9d6d7",
   "metadata": {},
   "source": [
    "# 3. CPU vs GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618df51-1ff6-4c58-94df-ced67d4c37df",
   "metadata": {},
   "source": [
    "`-` 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f9185-d898-4bbb-906a-bb752800a434",
   "metadata": {},
   "source": [
    "`-` 코랩에서 GPU설정하는 방법: (아래영상참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a83013-ee9b-47d0-a043-665c19337c62",
   "metadata": {},
   "source": [
    "{{<video https://youtu.be/GyBL2YBCzoc >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9403b4-9751-430e-8810-8ca8f2302dac",
   "metadata": {},
   "source": [
    "## A. GPU 사용방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb51ffe-27e0-4dde-a471-bc06f3926c1e",
   "metadata": {},
   "source": [
    "`-` cpu 연산이 가능한 메모리에 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d54e22e-e238-4206-a7b6-ad0a42ea3048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(43052)\n",
    "x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \n",
    "y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \n",
    "net_cpu = torch.nn.Linear(1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95efdc-7889-4892-b0ec-00189110af24",
   "metadata": {},
   "source": [
    "`-` gpu 연산이 가능한 메모리에 데이터 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fc404a-ec68-4306-8c44-10f453373da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 15:32:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   56C    P3              88W / 420W |   1052MiB / 24576MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22b2e9c-9e14-4ad2-84ef-8885ffcea830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(43052)\n",
    "x_gpu = x_cpu.to(\"cuda:0\")\n",
    "y_gpu = y_cpu.to(\"cuda:0\")\n",
    "net_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9601e845-9a4b-4bec-9053-879be0749db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 15:33:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   58C    P2             135W / 420W |   1321MiB / 24576MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fbd2e-84fc-47cb-ad1d-6f4b26f137e0",
   "metadata": {},
   "source": [
    "> GPU에 메모리를 올리면 GPU메모리가 점유된다! (26MiB -> 287MiB) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53fcb8-275d-41c4-b3dc-d56d5f5a96fc",
   "metadata": {},
   "source": [
    "`-` cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed37ca9-745f-4e78-bea6-c17993c9fcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.1000],\n",
       "         [0.2000]]),\n",
       " tensor([[0.0000],\n",
       "         [0.2000],\n",
       "         [0.4000]]),\n",
       " Parameter containing:\n",
       " tensor([[-0.3467]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.8470], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cpu, y_cpu, net_cpu.weight, net_cpu.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7cb375-73ca-4bdd-a290-ecedfc2bf7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.1000],\n",
       "         [0.2000]], device='cuda:0'),\n",
       " tensor([[0.0000],\n",
       "         [0.2000],\n",
       "         [0.4000]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.8470], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpu, y_gpu, net_gpu.weight, net_gpu.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5b0f1-7529-47ec-bab5-8ed91640a1ac",
   "metadata": {},
   "source": [
    "`-` gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c42fa-5f99-411b-9655-0711a2375220",
   "metadata": {},
   "source": [
    "(예시1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9843f929-e8e0-46b3-a799-c99ba1e46fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8470],\n",
       "        [-0.8817],\n",
       "        [-0.9164]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_cpu(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fbba8-de43-4442-83f0-ab19642039b0",
   "metadata": {},
   "source": [
    "(예시2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bdd165-cb2c-4c9b-9d16-e03c2346f273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8470],\n",
       "        [-0.8817],\n",
       "        [-0.9164]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_gpu(x_gpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ee03b-ddcd-479e-a1de-681c629bd241",
   "metadata": {},
   "source": [
    "(예시3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae322-4185-43e4-92c2-8b3229153934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_cpu(x_gpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu.to(\"cuda:0\")\n",
    "net_gpu(x_cpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_gpu(x_cpu.to(\"cuda:0\")\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d649b43-9d57-4ea7-a5be-41bf78292a98",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "#### 강의중 net을 재선언한 이유\n",
    "\n",
    "`-` 아래와 같이 `x_cpu` 혹은 `y_cpu`에 `.to(\"cuda:0\")`메소드를 쓸 경우 \n",
    "\n",
    "```Python\n",
    "x_cpu.to(\"cuda:0\")\n",
    "y_cpu.to(\"cuda:0\")\n",
    "```\n",
    "\n",
    "`x_cpu`와 `y_cpu`는 cpu에 그대로 있음. \n",
    "\n",
    "`-` 그런데 아래와 같이 `net_cpu`에서 `.to(\"cuda:0\")`메소드를 쓸 경우 \n",
    "\n",
    "```Python\n",
    "net_cpu.to(\"cuda:0\")\n",
    "```\n",
    "\n",
    "`net_cpu` 자체가 gpu에 올라가게 됨. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a42c6-4fea-4011-9940-5eb438d3d937",
   "metadata": {},
   "source": [
    "(예시4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd836ac-5da1-4d85-ad0b-1de6c16120cf",
   "metadata": {},
   "source": [
    "(예시5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538e301-9c48-40f2-b5bd-6e599a1521cc",
   "metadata": {},
   "source": [
    "(예시6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72472217-0944-4268-8a3f-6bee4b8c8b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2068, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y_gpu-net_gpu(x_gpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1fe1e-ade1-4fdf-b814-97b14b9a05e4",
   "metadata": {},
   "source": [
    "(예시7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc92139-3108-47c5-89b1-b01266c9d19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43my_gpu\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnet_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.mean((y_gpu-net_cpu(x_cpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12465e2-8b7a-4a28-8f0b-51c6aa716af8",
   "metadata": {},
   "source": [
    "(예시8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07158f86-6034-411e-8530-633758eaad34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43my_cpu\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnet_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gpu\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.mean((y_cpu-net_gpu(x_gpu))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25a3a7a-bc4e-4007-83cb-d2f8c3f9a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "net_gpu(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63858552-cc37-453f-8cf8-cac3d3d267c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.mean((y_cpu-net_cpu(x_cpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd12bfa-6e85-4265-962e-8619fe0b2105",
   "metadata": {},
   "source": [
    "## B. 시간측정 (예비학습) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19a108c-fa7a-411d-8a3e-67d40fe149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbdf1cd-a97a-44aa-93f1-fe3fa2b3c76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436f27b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d8cc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30453991889953613"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e185b-733a-452e-9221-d5cdfad0ddbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. CPU vs GPU (512 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb010616-edf0-418d-ab29-5ad06b7cb5d5",
   "metadata": {},
   "source": [
    "`-` CPU (512 nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f6b1282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3479437828063965"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85e79b-564b-4e22-b35b-3d50de40a64d",
   "metadata": {},
   "source": [
    "`-` GPU (512 nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5300c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972074bf-14bf-4c38-8f27-171a21e58699",
   "metadata": {
    "tags": []
   },
   "source": [
    "- CPU가 더 빠르다??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f64f6-bca2-42d6-87b3-d9609729d338",
   "metadata": {},
   "source": [
    "## D. CPU vs GPU (20,480 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994456c9-cfbd-4164-8d5f-23199460924b",
   "metadata": {},
   "source": [
    "`-` CPU (20,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ebfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,20480),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20480,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea7890-1ec1-4d6f-af39-1392882ebd24",
   "metadata": {},
   "source": [
    "`-` GPU (20,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632bc05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,20480),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20480,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94ff63-02a6-4de3-98c7-4905f1c8f44f",
   "metadata": {},
   "source": [
    "- 왜 이런 차이가 나는가? \n",
    "- 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e6aa5-069b-4069-9699-a16451da140f",
   "metadata": {},
   "source": [
    "## E. CPU vs GPU (204,800 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf5035-822b-4dcd-8023-1cb59d8a1cd7",
   "metadata": {},
   "source": [
    "`-` CPU (204,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567de51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,204800),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(204800,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0476c5-8e67-4258-b4b8-61b2a4419f04",
   "metadata": {},
   "source": [
    "`-` GPU (204,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76da7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,204800),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(204800,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b8bed-a7c0-408f-86a2-dcc55f4ee356",
   "metadata": {},
   "source": [
    "# 4. \"확률적\" 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd71642-4b9d-4e05-b617-9559249bb814",
   "metadata": {},
   "source": [
    "## A. 의문: 좀 이상하지 않아요? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f7132-5af5-4930-872f-078a4c5082c0",
   "metadata": {},
   "source": [
    "`-` 국민상식: GPU 비싸요.. <https://bbs.ruliweb.com/community/board/300143/read/61066881>\n",
    "\n",
    "- GPU 메모리 많아봐야 24GB, 그래도 비싸요.. <http://shop.danawa.com/virtualestimate/?controller=estimateMain&methods=index&marketPlaceSeq=16>\n",
    "- GPU 메모리가 80GB일 경우 가격: <https://prod.danawa.com/info/?pcode=21458333>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a05215-135e-463f-bbbf-771b5f65b0ea",
   "metadata": {},
   "source": [
    "`-` 우리가 분석하는 데이터: 빅데이터..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e8920-2867-49f5-bb5c-7d88f7ebf771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-10,10,100000).reshape(-1,1)\n",
    "eps = torch.randn(100000).reshape(-1,1)\n",
    "y = x*2 + eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70984f-d684-4ddb-b948-113fb16d4f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'o',alpha=0.05)\n",
    "plt.plot(x,2*x,'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418df34-7660-4e95-9080-85d4e976a813",
   "metadata": {},
   "source": [
    "`-` 데이터의 크기가 커지는 순간 `X.to(\"cuda:0\")`, `y.to(\"cuda:0\")` 쓰면 난리나겠는걸? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea0878-c743-4930-bf34-76abaf578783",
   "metadata": {},
   "source": [
    "`-` 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0a457-d71c-4959-a357-4d53d43f3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x[::100],y[::100],'o',alpha=0.05)\n",
    "plt.plot(x,2*x,'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[::1000].shape # 1000개씩 점프하면서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba287425-1536-40de-92ab-bd47c1744d75",
   "metadata": {},
   "source": [
    "- 대충 이거만 가지고 적합해도 충분히 정확할것 같은데?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbad39b-ca43-48d4-98dd-4abd5ba19890",
   "metadata": {},
   "source": [
    "## B. X,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d7735-3663-4ca2-99f2-2829b9dd9c66",
   "metadata": {},
   "source": [
    "`-` 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395028f-300c-4d92-8ee8-e18cd8c6c28a",
   "metadata": {},
   "source": [
    "`-` 아래의 알고리즘을 생각해보자. \n",
    "\n",
    "1. 데이터를 반으로 나눈다. \n",
    "2. 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. \n",
    "3. yhat, loss, grad, update 수행\n",
    "4. 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. \n",
    "5. yhat, loss, grad, update 수행 \n",
    "6. 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. \n",
    "7. 반복 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484eb1d",
   "metadata": {},
   "source": [
    "> 이러면 되는거아니야???? ---> 맞아요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685c5eb-e7c3-4533-8ca2-5e1d1507a3b5",
   "metadata": {},
   "source": [
    "## C. 경사하강법, 확률적경사하강법, 미니배치 경사하강법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f3b2e-3405-4992-86d3-ab928e8f2785",
   "metadata": {},
   "source": [
    "10개의 샘플이 있다고 가정. $\\{(x_i,y_i)\\}_{i=1}^{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6ccf-7523-43da-826a-1891d10ee663",
   "metadata": {},
   "source": [
    "`# ver1` --  모든 샘플을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f45b4-ee4a-4960-b537-4a8342eebe60",
   "metadata": {},
   "source": [
    "(epoch 1) $loss=\\sum_{i=1}^{10}(y_i-w_0-w_1x_i)^2 \\to slope  \\to update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9caaf97-f13a-4742-9966-f982058c0a00",
   "metadata": {},
   "source": [
    "(epoch 2) $loss=\\sum_{i=1}^{10}(y_i-w_0-w_1x_i)^2 \\to slope  \\to update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45cefa-9807-4437-83f3-db58cf80ebd5",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376f5ff-5786-45a8-a3a5-272a93d3901c",
   "metadata": {},
   "source": [
    "> 우리가 항상 이렇게 했죠!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700db88-6fbd-4e2f-a1d8-8861bb6674d1",
   "metadata": {},
   "source": [
    "`# ver2` -- 하나의 샘플만을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a184d8-bf1b-4abe-a723-27b1e94e848d",
   "metadata": {},
   "source": [
    "(epoch 1) \n",
    "\n",
    "- $loss=(y_1-w_0-w_1x_1)^2 \\to slope \\to update$\n",
    "- $loss=(y_2-w_0-w_1x_2)^2 \\to slope \\to update$\n",
    "- ...\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee15892-09b4-4f29-aa05-c720ca6a3777",
   "metadata": {},
   "source": [
    "(epoch 2) \n",
    "\n",
    "- $loss=(y_1-w_0-w_1x_1)^2  \\to slope  \\to  update$\n",
    "- $loss=(y_2-w_0-w_1x_2)^2  \\to slope  \\to  update$\n",
    "- ...\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab1cd8-be7b-4ee5-8c97-c196f9de88fd",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9dc8d0-b7ea-4778-966e-c2c8e8cba3e1",
   "metadata": {},
   "source": [
    "`# ver3` -- $m (\\leq n)$ 개의 샘플을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626bb7d-07e2-4078-a9e2-c9db8ee13f3d",
   "metadata": {},
   "source": [
    "$m=3$이라고 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3b3aa-cbc0-48c0-bbf5-40ba698a1e34",
   "metadata": {},
   "source": [
    "(epoch 1) \n",
    "\n",
    "- $loss=\\sum_{i=1}^{3}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=4}^{6}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=7}^{9}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1e55b-045c-4389-81fd-69ef9ab9b122",
   "metadata": {},
   "source": [
    "(epoch 2) \n",
    "\n",
    "- $loss=\\sum_{i=1}^{3}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=4}^{6}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=7}^{9}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ff5b2-8fa4-4756-ae22-21995eca38f7",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5071655-fd59-4193-a765-40011743de77",
   "metadata": {},
   "source": [
    "## D. 용어의 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eff37b-d7aa-4468-9098-908dc16538cb",
   "metadata": {},
   "source": [
    "**옛날**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cb47c-cba9-41b0-977d-93bbef211aca",
   "metadata": {},
   "source": [
    "`-` ver1: gradient descent, batch gradient descent\n",
    "\n",
    "`-` ver2: stochastic gradient descent (sample 1개)\n",
    "\n",
    "`-` ver3: mini-batch gradient descent, mini-batch stochastic gradient descent (sample n개 == batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd197cc-e449-421c-b38e-15270423aed6",
   "metadata": {},
   "source": [
    "**요즘**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf62fb9-69da-4fbd-a4fb-4790fb635c9c",
   "metadata": {},
   "source": [
    "`-` ver1: gradient descent\n",
    "\n",
    "`-` ver2: stochastic gradient descent with batch size = 1\n",
    "\n",
    "`-` **ver3: stochastic gradient descent**\n",
    "- https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648dc2f-b339-4500-9898-aac476469b53",
   "metadata": {},
   "source": [
    "## E. Dataset(`ds`), DataLoader(`dl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc8064",
   "metadata": {},
   "source": [
    "> 취지는 알겠으나, C의 과정을 실제 구현하려면 진짜 힘들것 같아요.. (입코딩과 손코딩의 차이) --> 이걸 해결하기 위해서 파이토치에서는 DataLoader라는 오브젝트를 준비했음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bfcf7-8401-407d-843d-64280a88dcb5",
   "metadata": {},
   "source": [
    "`-` ds: 섭스크립터블함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd133a9e-03df-4a28-80a1-eb07f327da25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 1.],\n",
       "        [2., 1.],\n",
       "        [3., 1.],\n",
       "        [4., 1.],\n",
       "        [5., 0.],\n",
       "        [6., 0.],\n",
       "        [7., 0.],\n",
       "        [8., 0.],\n",
       "        [9., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(range(10)).float().reshape(-1,1)\n",
    "y=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\n",
    "torch.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08062bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([x,y],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b195f8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Dataset wrapping tensors.\n",
      "\n",
      "Each sample will be retrieved by indexing tensors along the first dimension.\n",
      "\n",
      "Args:\n",
      "    *tensors (Tensor): tensors that have the same size of the first dimension.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/utils/data/dataset.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch.utils.data.TensorDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b339974a-d346-4aef-b991-eb737f4dbbba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7deabba79700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "ds=torch.utils.data.TensorDataset(x,y)\n",
    "ds  # 뭐 정보가 없네.. 어쩌란거지..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac0ac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_protocol',\n",
       " 'tensors']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7a0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08623d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'__iter__'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "130c639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'__getitem__'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a881d",
   "metadata": {},
   "source": [
    "__iter__ 이외에도 __getitem__ 이 있으면 for 문을 사용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "279cd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.]), tensor([1.]))\n",
      "(tensor([1.]), tensor([1.]))\n",
      "(tensor([2.]), tensor([1.]))\n",
      "(tensor([3.]), tensor([1.]))\n",
      "(tensor([4.]), tensor([1.]))\n",
      "(tensor([5.]), tensor([0.]))\n",
      "(tensor([6.]), tensor([0.]))\n",
      "(tensor([7.]), tensor([0.]))\n",
      "(tensor([8.]), tensor([0.]))\n",
      "(tensor([9.]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "for i in ds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f1739b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([4.])\n",
      "tensor([5.])\n",
      "tensor([6.])\n",
      "tensor([7.])\n",
      "tensor([8.])\n",
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "for i in ds:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20e5d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<iterator at 0x7496a300e670>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c31e7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.]), tensor([1.]))\n",
      "(tensor([1.]), tensor([1.]))\n",
      "(tensor([2.]), tensor([1.]))\n",
      "(tensor([3.]), tensor([1.]))\n",
      "(tensor([4.]), tensor([1.]))\n",
      "(tensor([5.]), tensor([0.]))\n",
      "(tensor([6.]), tensor([0.]))\n",
      "(tensor([7.]), tensor([0.]))\n",
      "(tensor([8.]), tensor([0.]))\n",
      "(tensor([9.]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # __getitem__ 메서드를 사용하여 인덱스를 통해 요소에 접근\n",
    "        # print(index)\n",
    "        item = ds.__getitem__(index)\n",
    "        print(item)\n",
    "        index += 1\n",
    "    except IndexError:\n",
    "        # 더 이상 요소가 없으면 IndexError가 발생하므로 반복을 종료\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6298cfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.]), tensor([1.])),\n",
       " (tensor([1.]), tensor([1.])),\n",
       " (tensor([7.]), tensor([0.])))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set(dir(ds)) & {'__getitem__'}\n",
    "# __getitem__ 이 있다는건 리스트처럼 사용하는게 가능하다는 것! , 추가로 for 문도 가능함.\n",
    "ds[0] ,ds[1] , ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ef067a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensors'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'tensors'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5084ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38c41614-ece3-4067-8092-4ff8720279bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e125dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a,_b=ds.tensors \n",
    "b_numpy = _b.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "745784fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(b_numpy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5f64a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds.tensors)) & {'__getitem__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be51d15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __getitem__ 이 있으면 List 처럼 사용가능하기도 하고\n",
    "_aa,_bb= ds.tensors \n",
    "_aa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "784356d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(_aa)) & {'__getitem__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e62992bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "0.0\n",
      "[1.0]\n",
      "1.0\n",
      "[2.0]\n",
      "2.0\n",
      "[3.0]\n",
      "3.0\n",
      "[4.0]\n",
      "4.0\n",
      "[5.0]\n",
      "5.0\n",
      "[6.0]\n",
      "6.0\n",
      "[7.0]\n",
      "7.0\n",
      "[8.0]\n",
      "8.0\n",
      "[9.0]\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# __getitem__ 이 있으면 for문에서  사용가능하기도 한다.\n",
    "for i in _aa:\n",
    "    print(i.tolist())\n",
    "    print(i[0].tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d93c4c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.]), tensor([1.])),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0],(x,y)[0] # (x,y) 튜플자체는 아님.. 인덱싱이 다르게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a06af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf03c4-e2c8-412b-a5b2-9a747a47733e",
   "metadata": {
    "tags": []
   },
   "source": [
    "::: {.callout-note}\n",
    "여기서 제가 `__iter__` 가 숨겨져 있는 오브젝트일 경우만 `for`문이 동작한다고 설명 했는데요, `__getitem__`이 있는 경우도 동작한다고 합니다. **제가 잘못 알고 있었어요. 혼란을 드려 죄송합니다**. \n",
    "\n",
    "- 그래도 `dl`은 for 를 돌리기위해서 만든 오브젝트라는 설명은 맞는 설명입니다. \n",
    "- `ds`역시 독특한 방식의 인덱싱을 지원하도록 한 오브젝트라는 설명도 맞는 설명입니다. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9418f1f-cdab-45c6-a4b1-d8203cedcb2c",
   "metadata": {},
   "source": [
    "`-` dl: 섭스크립터블하지 않지만 이터러블함 \n",
    "- __getitem__ 이 없어서 섭스크립터블하지 않지만\n",
    "- __iter__ 이 있어서 이터러블하긴 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5613856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05b05710-3ee4-4f65-b6cf-d0702c884166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## dataset\n",
    "# ds=torch.utils.data.TensorDataset(x,y)\n",
    "\n",
    "# dataloader\n",
    "dl=torch.utils.data.DataLoader(dataset=ds,batch_size=3)\n",
    "# dl = torch.utils.data.DataLoader(dataset=ds, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "#set(dir(dl)) & {'__iter__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eaabb58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(dl)) & {'__getitem__'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224a765",
   "metadata": {},
   "source": [
    "__iter__ 가 있으니 for 문을 돌릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "daf49862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__iter__'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(dl)) & {'__iter__'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02bb6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])]\n",
      "[tensor([[3.],\n",
      "        [4.],\n",
      "        [5.]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])]\n",
      "[tensor([[6.],\n",
      "        [7.],\n",
      "        [8.]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])]\n",
      "[tensor([[9.]]), tensor([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "for i in dl:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66b824b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    # print(f\"x_batch:{xi} \\t y_batch:{yi}\")\n",
    "    print(type(xi))\n",
    "    print(type(yi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cedd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]]) \t y_batch:tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "x_batch:tensor([[3.],\n",
      "        [4.],\n",
      "        [5.]]) \t y_batch:tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "x_batch:tensor([[6.],\n",
      "        [7.],\n",
      "        [8.]]) \t y_batch:tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "x_batch:tensor([[9.]]) \t y_batch:tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f\"x_batch:{xi} \\t y_batch:{yi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07cf4f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:[[0.0], [1.0], [2.0]] \t y_batch:[[1.0], [1.0], [1.0]]\n",
      "x_batch:[[3.0], [4.0], [5.0]] \t y_batch:[[1.0], [1.0], [0.0]]\n",
      "x_batch:[[6.0], [7.0], [8.0]] \t y_batch:[[0.0], [0.0], [0.0]]\n",
      "x_batch:[[9.0]] \t y_batch:[[0.0]]\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f\"x_batch:{xi.tolist()} \\t y_batch:{yi.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ad0071f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04bfbd",
   "metadata": {},
   "source": [
    "`-` 마지막관측치는 뭔데 단독으로 업데이트하냐?? 편향되는 위험이 있는거 아냐?? --> shuffle True 같이 자잘한 옵션도 있음.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35537fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch=[[9.0], [2.0], [6.0]] \t y_batch=[[0.0], [1.0], [0.0]]\n",
      "x_batch=[[0.0], [8.0], [4.0]] \t y_batch=[[1.0], [0.0], [1.0]]\n",
      "x_batch=[[1.0], [7.0], [5.0]] \t y_batch=[[1.0], [0.0], [0.0]]\n",
      "x_batch=[[3.0]] \t y_batch=[[1.0]]\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True)\n",
    "for xi,yi in dl:\n",
    "    print(f'x_batch={xi.tolist()} \\t y_batch={yi.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76043e9d",
   "metadata": {},
   "source": [
    "`-` DataLoader의 주요 파라미터\n",
    "- dataset: 로드할 데이터셋. Dataset 클래스의 인스턴스\n",
    "- batch_size: 배치 크기. 한 번에 로드할 데이터 포인트의 수\n",
    "- shuffle: 매 에폭(epoch)마다 데이터를 섞을지 여부.\n",
    "    - epoch: 전체 데이터셋이 학습 알고리즘을 통과하는 한 번의 완전한 사이클을 의미\n",
    "    - 일반적으로 학습 데이터셋에는 True를, 검증/테스트 데이터셋에는 False를 사용\n",
    "- num_workers: 데이터 로딩에 사용할 서브 프로세스의 수. 더 빠른 데이터 로딩을 위해 0보다 큰 값을 설정할 수 있습니다.\n",
    "    - 이 값이 0이면, 메인 프로세스가 모든 데이터 로딩 작업을 수행합니다.\n",
    "- collate_fn: 배치 데이터를 어떻게 처리할지 정의하는 함수입니다. 사용자 정의 데이터 처리를 위해 커스터마이즈할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32381ec5-1ac2-4621-8314-de10e2088c27",
   "metadata": {},
   "source": [
    "## F. ds, dl을 이용한 MNIST 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3ec7c",
   "metadata": {},
   "source": [
    "`-` 목표: 확률적경사하강법과 그냥 경사하강법의 성능을 \"동일 반복횟수\"로 비교해보자.\n",
    "\n",
    "- batch_size = 2048로 설정할것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6b0dd-1223-440b-8a6d-1a21b68f34bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 그냥 경사하강법 -- 미니배치 안쓰는 학습, 우리가 맨날하는 그거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f5a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09423ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/myuser/.fastai/data/mnist_png')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3866735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.io.read_image(path/'training/0/1775.png').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7f582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7deabba31fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAacUlEQVR4nO3df2hV9/3H8ddV46265JaQH/dmxhCcslJFqDp/0GjsZmpgQWsFbWFENlxbNV2WFjcnxWww01kUC1ltVzqrW12FTp1UV5uhSSzOYcWuYq0oxpmhIVXsvTHaiPXz/SN4v7saU8/13rxzc58P+EDvOefteef01Fc/Ofd+rs855wQAgIFB1g0AANIXIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzQ6wbuN3Nmzd1/vx5ZWZmyufzWbcDAPDIOaeOjg4VFBRo0KDe5zr9LoTOnz+vwsJC6zYAAPeptbVVI0eO7PWYfvfruMzMTOsWAAAJcC9/nycthF577TUVFxfrgQce0MSJE3XgwIF7quNXcAAwMNzL3+dJCaFt27apurpaq1at0tGjR1VSUqLy8nKdO3cuGacDAKQoXzJW0Z4yZYoeeeQRbdy4MbrtoYce0rx581RXV9drbSQSUSAQSHRLAIA+Fg6HlZWV1esxCZ8JXb9+XUeOHFFZWVnM9rKyMh08ePCO47u6uhSJRGIGACA9JDyELl68qK+//lr5+fkx2/Pz89XW1nbH8XV1dQoEAtHBO+MAIH0k7Y0Jtz+Qcs71+JBq5cqVCofD0dHa2pqslgAA/UzCPyeUk5OjwYMH3zHraW9vv2N2JEl+v19+vz/RbQAAUkDCZ0JDhw7VxIkT1dDQELO9oaFB06dPT/TpAAApLCkrJtTU1OhHP/qRJk2apGnTpukPf/iDzp07p2effTYZpwMApKikhNDChQt16dIl/eY3v9GFCxc0btw47dmzR0VFRck4HQAgRSXlc0L3g88JAcDAYPI5IQAA7hUhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwMsW4AQPI8//zzcdU55xLcSeL4fD7PNZs3b47rXOFwOK463DtmQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMywgClgoLy83HPNhg0bPNd85zvf8VwjDbwFTHfv3h3XuVjANPmYCQEAzBBCAAAzCQ+h2tpa+Xy+mBEMBhN9GgDAAJCUZ0IPP/yw/vGPf0RfDx48OBmnAQCkuKSE0JAhQ5j9AAC+UVKeCZ06dUoFBQUqLi7WokWLdObMmbse29XVpUgkEjMAAOkh4SE0ZcoUbdmyRXv37tWbb76ptrY2TZ8+XZcuXerx+Lq6OgUCgegoLCxMdEsAgH4q4SFUXl6uJ598UuPHj9cPfvCD6PvzN2/e3OPxK1euVDgcjo7W1tZEtwQA6KeS/mHVESNGaPz48Tp16lSP+/1+v/x+f7LbAAD0Q0n/nFBXV5dOnDihUCiU7FMBAFJMwkPoxRdfVFNTk1paWvSvf/1LCxYsUCQSUWVlZaJPBQBIcQn/ddx///tfPfXUU7p48aJyc3M1depUHTp0SEVFRYk+FQAgxSU8hN59991E/5FAn3nwwQc917z66queax5//HHPNTk5OZ5rgP6OteMAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYSfqX2gH3y+fz9dm5duzY4bmmpKQkCZ0gkRYsWBBX3dq1az3XOOfiOle6YiYEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADDDKtro93784x97rnnjjTfiOlc8K3b31arJTU1Nnmu+//3vJ6GTxNmwYYPnmqqqKs81v/3tbz3XSFJWVpbnmlWrVsV1rnTFTAgAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZFjBFn3ruuec81/zud79LQieJ88UXX3iuWbJkieeaAwcOeK7p7z7//HPPNZ999pnnmoceeshzjSTV1NR4ronnZ/rTn/7kuWagYCYEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADAjM8556yb+F+RSESBQMC6jbSSm5sbV93bb7/tuaakpMRzzfDhwz3XxOuPf/yj55o33njDc82RI0c816BbUVGR55qPPvoornOFQiHPNSdOnPBcM378eM81qSAcDisrK6vXY5gJAQDMEEIAADOeQ6i5uVkVFRUqKCiQz+fTzp07Y/Y751RbW6uCggINGzZMpaWlOn78eKL6BQAMIJ5DqLOzUxMmTFB9fX2P+9euXav169ervr5ehw8fVjAY1OzZs9XR0XHfzQIABhbP36xaXl6u8vLyHvc557RhwwatWrVK8+fPlyRt3rxZ+fn52rp1q5555pn76xYAMKAk9JlQS0uL2traVFZWFt3m9/s1c+ZMHTx4sMearq4uRSKRmAEASA8JDaG2tjZJUn5+fsz2/Pz86L7b1dXVKRAIREdhYWEiWwIA9GNJeXecz+eLee2cu2PbLStXrlQ4HI6O1tbWZLQEAOiHPD8T6k0wGJTUPSP63w95tbe33zE7usXv98vv9yeyDQBAikjoTKi4uFjBYFANDQ3RbdevX1dTU5OmT5+eyFMBAAYAzzOhK1eu6PTp09HXLS0t+uSTT5Sdna1Ro0apurpaa9as0ZgxYzRmzBitWbNGw4cP19NPP53QxgEAqc9zCH388ceaNWtW9HVNTY0kqbKyUm+//bZWrFiha9euaenSpbp8+bKmTJmiDz/8UJmZmYnrGgAwILCAKbRhw4a46pYvX57YRhLovffei6tu0aJFCe4E/cHJkyfjqhs9enSCO+nZkCEJfTzfb7CAKQCgXyOEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmBmYS7emsZ/85Ceea6qqqpLQSeK89dZbnmt++tOfJqETpCqfz9endbh3zIQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYYQHTfiw3N9dzTTwLdzrnPNfEa+PGjZ5rfvGLXyShE6STeO/xvvxvI10xEwIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGBUz7sTfffNNzzcSJE5PQSc++/PJLzzWvvPKK55qrV696rsHANXfuXM81oVAornNdv37dc826deviOle6YiYEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADAuY9pGZM2d6rikpKUlCJ4kzf/58zzXnzp1LQidIJ9XV1Z5rhg8fHte5Tp065bnmpZdeiutc6YqZEADADCEEADDjOYSam5tVUVGhgoIC+Xw+7dy5M2b/4sWL5fP5YsbUqVMT1S8AYADxHEKdnZ2aMGGC6uvr73rMnDlzdOHChejYs2fPfTUJABiYPL8xoby8XOXl5b0e4/f7FQwG424KAJAekvJMqLGxUXl5eRo7dqyWLFmi9vb2ux7b1dWlSCQSMwAA6SHhIVReXq533nlH+/bt07p163T48GE99thj6urq6vH4uro6BQKB6CgsLEx0SwCAfirhnxNauHBh9J/HjRunSZMmqaioSLt37+7xcyUrV65UTU1N9HUkEiGIACBNJP3DqqFQSEVFRXf90Jff75ff7092GwCAfijpnxO6dOmSWltbFQqFkn0qAECK8TwTunLlik6fPh193dLSok8++UTZ2dnKzs5WbW2tnnzySYVCIZ09e1a/+tWvlJOToyeeeCKhjQMAUp/nEPr44481a9as6Otbz3MqKyu1ceNGHTt2TFu2bNGXX36pUCikWbNmadu2bcrMzExc1wCAAcFzCJWWlso5d9f9e/fuva+GBqoJEyZ4rnnwwQcT30gCNTc3W7eAFPfqq696riktLfVcc/PmTc81kjRoECubJRtXGABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgJunfrIpu8XyfUm+rlSfSe++91yfnwcA2adIkzzWjRo3yXBPPitjt7e2eayTp+eefj6sO946ZEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADMsYNpHNm3a5Llm6tSpnmsyMjI81yxYsMBzjSTl5uZ6rvniiy/iOhf6VjyLkb7//vuea3JycjzXxGPJkiVx1e3duzfBneB2zIQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCY8TnnnHUT/ysSiSgQCFi30S+cPHnSc83o0aOT0EnP/v3vf3uuefzxxz3XXLx40XMNus2cOTOuuurqas81FRUVcZ3LqwMHDniumTdvXlznCofDcdWhWzgcVlZWVq/HMBMCAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghgVM+7Hy8nLPNe+//34SOrH1+uuve675/PPP4zqXz+fzXNNX/wnNnz/fc01paWlc57p582ZcdV797Gc/81xTX1+fhE6QDCxgCgDo1wghAIAZTyFUV1enyZMnKzMzU3l5eZo3b94d33njnFNtba0KCgo0bNgwlZaW6vjx4wltGgAwMHgKoaamJi1btkyHDh1SQ0ODbty4obKyMnV2dkaPWbt2rdavX6/6+nodPnxYwWBQs2fPVkdHR8KbBwCktiFeDv7ggw9iXm/atEl5eXk6cuSIZsyYIeecNmzYoFWrVkUfom7evFn5+fnaunWrnnnmmcR1DgBIeff1TOjWV99mZ2dLklpaWtTW1qaysrLoMX6/XzNnztTBgwd7/DO6uroUiURiBgAgPcQdQs451dTU6NFHH9W4ceMkSW1tbZKk/Pz8mGPz8/Oj+25XV1enQCAQHYWFhfG2BABIMXGH0PLly/Xpp5/qL3/5yx37bv+shXPurp+/WLlypcLhcHS0trbG2xIAIMV4eiZ0S1VVlXbt2qXm5maNHDkyuj0YDErqnhGFQqHo9vb29jtmR7f4/X75/f542gAApDhPMyHnnJYvX67t27dr3759Ki4ujtlfXFysYDCohoaG6Lbr16+rqalJ06dPT0zHAIABw9NMaNmyZdq6dav+9re/KTMzM/qcJxAIaNiwYfL5fKqurtaaNWs0ZswYjRkzRmvWrNHw4cP19NNPJ+UHAACkLk8htHHjRkl3rke1adMmLV68WJK0YsUKXbt2TUuXLtXly5c1ZcoUffjhh8rMzExIwwCAgYMFTPux4cOHe6555ZVXPNfw+a3/158XMI1HPD+P1P0c16u///3vnmt+/vOfe6659dEQ9H8sYAoA6NcIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGbi+mZV9I2rV696rlmxYoXnmoyMDM81klRRUeG5Jjc3N65zIT6nT5+Oq66xsdFzDauxIx7MhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJjxOeecdRP/KxKJKBAIWLeBezBhwgTPNSUlJUnoBHdTX19v3QLSWDgcVlZWVq/HMBMCAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghgVMAQBJwQKmAIB+jRACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZjyFUF1dnSZPnqzMzEzl5eVp3rx5OnnyZMwxixcvls/nixlTp05NaNMAgIHBUwg1NTVp2bJlOnTokBoaGnTjxg2VlZWps7Mz5rg5c+bowoUL0bFnz56ENg0AGBiGeDn4gw8+iHm9adMm5eXl6ciRI5oxY0Z0u9/vVzAYTEyHAIAB676eCYXDYUlSdnZ2zPbGxkbl5eVp7NixWrJkidrb2+/6Z3R1dSkSicQMAEB68DnnXDyFzjnNnTtXly9f1oEDB6Lbt23bpm9961sqKipSS0uLXnrpJd24cUNHjhyR3++/48+pra3Vr3/96/h/AgBAvxQOh5WVldX7QS5OS5cudUVFRa61tbXX486fP+8yMjLcX//61x73f/XVVy4cDkdHa2urk8RgMBiMFB/hcPgbs8TTM6FbqqqqtGvXLjU3N2vkyJG9HhsKhVRUVKRTp071uN/v9/c4QwIADHyeQsg5p6qqKu3YsUONjY0qLi7+xppLly6ptbVVoVAo7iYBAAOTpzcmLFu2TH/+85+1detWZWZmqq2tTW1tbbp27Zok6cqVK3rxxRf1z3/+U2fPnlVjY6MqKiqUk5OjJ554Iik/AAAghXl5DqS7/N5v06ZNzjnnrl696srKylxubq7LyMhwo0aNcpWVle7cuXP3fI5wOGz+e0wGg8Fg3P+4l2dCcb87LlkikYgCgYB1GwCA+3Qv745j7TgAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgJl+F0LOOesWAAAJcC9/n/e7EOro6LBuAQCQAPfy97nP9bOpx82bN3X+/HllZmbK5/PF7ItEIiosLFRra6uysrKMOrTHdejGdejGdejGdejWH66Dc04dHR0qKCjQoEG9z3WG9FFP92zQoEEaOXJkr8dkZWWl9U12C9ehG9ehG9ehG9ehm/V1CAQC93Rcv/t1HAAgfRBCAAAzKRVCfr9fq1evlt/vt27FFNehG9ehG9ehG9ehW6pdh373xgQAQPpIqZkQAGBgIYQAAGYIIQCAGUIIAGAmpULotddeU3FxsR544AFNnDhRBw4csG6pT9XW1srn88WMYDBo3VbSNTc3q6KiQgUFBfL5fNq5c2fMfuecamtrVVBQoGHDhqm0tFTHjx+3aTaJvuk6LF68+I77Y+rUqTbNJkldXZ0mT56szMxM5eXlad68eTp58mTMMelwP9zLdUiV+yFlQmjbtm2qrq7WqlWrdPToUZWUlKi8vFznzp2zbq1PPfzww7pw4UJ0HDt2zLqlpOvs7NSECRNUX1/f4/61a9dq/fr1qq+v1+HDhxUMBjV79uwBtw7hN10HSZozZ07M/bFnz54+7DD5mpqatGzZMh06dEgNDQ26ceOGysrK1NnZGT0mHe6He7kOUorcDy5FfO9733PPPvtszLbvfve77pe//KVRR31v9erVbsKECdZtmJLkduzYEX198+ZNFwwG3csvvxzd9tVXX7lAIOBef/11gw77xu3XwTnnKisr3dy5c036sdLe3u4kuaamJudc+t4Pt18H51LnfkiJmdD169d15MgRlZWVxWwvKyvTwYMHjbqycerUKRUUFKi4uFiLFi3SmTNnrFsy1dLSora2tph7w+/3a+bMmWl3b0hSY2Oj8vLyNHbsWC1ZskTt7e3WLSVVOByWJGVnZ0tK3/vh9utwSyrcDykRQhcvXtTXX3+t/Pz8mO35+flqa2sz6qrvTZkyRVu2bNHevXv15ptvqq2tTdOnT9elS5esWzNz699/ut8bklReXq533nlH+/bt07p163T48GE99thj6urqsm4tKZxzqqmp0aOPPqpx48ZJSs/7oafrIKXO/dDvVtHuze1f7eCcu2PbQFZeXh795/Hjx2vatGkaPXq0Nm/erJqaGsPO7KX7vSFJCxcujP7zuHHjNGnSJBUVFWn37t2aP3++YWfJsXz5cn366af66KOP7tiXTvfD3a5DqtwPKTETysnJ0eDBg+/4P5n29vY7/o8nnYwYMULjx4/XqVOnrFsxc+vdgdwbdwqFQioqKhqQ90dVVZV27dql/fv3x3z1S7rdD3e7Dj3pr/dDSoTQ0KFDNXHiRDU0NMRsb2ho0PTp0426stfV1aUTJ04oFApZt2KmuLhYwWAw5t64fv26mpqa0vrekKRLly6ptbV1QN0fzjktX75c27dv1759+1RcXByzP13uh2+6Dj3pt/eD4ZsiPHn33XddRkaGe+utt9xnn33mqqur3YgRI9zZs2etW+szL7zwgmtsbHRnzpxxhw4dcj/84Q9dZmbmgL8GHR0d7ujRo+7o0aNOklu/fr07evSo+89//uOcc+7ll192gUDAbd++3R07dsw99dRTLhQKuUgkYtx5YvV2HTo6OtwLL7zgDh486FpaWtz+/fvdtGnT3Le//e0BdR2ee+45FwgEXGNjo7tw4UJ0XL16NXpMOtwP33QdUul+SJkQcs653//+966oqMgNHTrUPfLIIzFvR0wHCxcudKFQyGVkZLiCggI3f/58d/z4ceu2km7//v1O0h2jsrLSOdf9ttzVq1e7YDDo/H6/mzFjhjt27Jht00nQ23W4evWqKysrc7m5uS4jI8ONGjXKVVZWunPnzlm3nVA9/fyS3KZNm6LHpMP98E3XIZXuB77KAQBgJiWeCQEABiZCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABm/g9PIA4Nog2z2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torchvision.io.read_image(path/'training/0/1775.png').reshape(28,28),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b361a920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), torch.Size([1, 28, 28]), 6131)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa=([torchvision.io.read_image(str(i)) for i in (path/'training/3').ls()])\n",
    "aaa[0].shape , aaa[1].shape , len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9efb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f92d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e853019-48ca-4bb6-8b4d-f02e248cf9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0.shape=torch.Size([5923, 1, 28, 28]) \t X1.shape=torch.Size([6742, 1, 28, 28])\n",
      "X.shape=torch.Size([12665, 784])\n",
      "y.shape=torch.Size([12665, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9953)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "# X0 train 데이터셋\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "# X1 train 데이터셋\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "print(f'X0.shape={X0.shape} \\t X1.shape={X1.shape}')\n",
    "# X0 와 X1 을 합친 X train 데이터셋\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255    # 255로 나눠줌으로서 int 에서 float로 묵시적 형변환이 됨. 이때 shape 는 12665,784\n",
    "print(f'X.shape={X.shape}')\n",
    "\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) # shape 는 12665,1\n",
    "print(f'y.shape={y.shape}')\n",
    "\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    # 중간 노드 32 \n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    # 최종 출력노드 1\n",
    "    torch.nn.Linear(32,1),\n",
    "    # 이진분류 Sigmoid\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "# 이진분류 Loss\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())        # Stochastic Gradient Descent 를 테스트 중이니 Adam은 사용하지 말자. 여기선 batch=data_size_all\n",
    "## Step3: fit  \n",
    "for epoc in range(700):     # 여기선 epoch 을 700 으로\n",
    "    # step1 \n",
    "    yhat = net(X)\n",
    "    # step2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # step3     \n",
    "    loss.backward()\n",
    "    # step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "## Step4: Predict \n",
    "((yhat > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c6749",
   "metadata": {},
   "source": [
    "`-` 0번째 X 데이터를 신경만 net 에 통과시켰더니 출력결과가 0.3489 라는말.\n",
    "- X0 은 y=0으로 , X1 는 y=1로 라벨링 했었다.\n",
    "- X[0] 의 출력결과가 0.3489 이므로, net 은 X[0] 을 0이라 추론함\n",
    "- 실제 y[0]을 확인해보면 0이 맞다. 즉 추론한 값과 실제 정답 데이터가 같으므로 잘 찾아냈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41be6416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3489]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X[0].reshape(-1,784))   # 0번째 X 데이터를 신경만 net 에 통과시켰더니 출력결과가 0.3489 라는말."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "990c4db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3bc9cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  11,  97, 253,  86,   5,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  38, 193, 253, 252, 252,  45,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  83, 240, 252, 253, 252, 252,  45,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  81, 240, 252, 252, 253, 252, 252, 139,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  53,\n",
       "         179, 244, 252, 252, 252, 253, 252, 252, 160,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 118, 243,\n",
       "         255, 249, 199, 229, 253, 255, 253, 253, 161,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9, 155, 252, 252,\n",
       "         228, 117,  13, 215, 252, 253, 252, 252, 160,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 155, 252, 252, 189,\n",
       "          32,  13, 172, 227, 183, 253, 252, 252, 160,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0, 212, 252, 252, 176,  14,\n",
       "           0,  47, 227,  50,  95, 253, 252, 252, 119,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0, 158, 253, 252, 136,   4,   0,\n",
       "           0,  78,  48,   0, 168, 253, 252, 136,   4,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0, 104, 253, 255, 144,   0,   0,   0,\n",
       "           0,   0,   0, 155, 253, 255, 249,  63,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,  45, 236, 252, 218,  33,   0,   0,   0,\n",
       "           0,   0,  66, 236, 252, 249, 117,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0, 153, 252, 252,  32,   0,   0,   0,   0,\n",
       "           0,  66, 234, 252, 252, 199,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  17, 209, 252, 157,   0,   0,   0,   0,   0,\n",
       "          62, 236, 252, 252, 157,  21,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  99, 252, 210,  22,   0,   0,   0,   0, 106,\n",
       "         243, 252, 252, 210,  22,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  13, 212, 253, 161,   0,   0,   0,   7, 160, 253,\n",
       "         255, 253, 215,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   9, 194, 252, 177,   5,   0,  30, 103, 252, 252,\n",
       "         253, 202,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 129, 252, 252, 190, 185, 228, 252, 252, 252,\n",
       "         152,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   9, 196, 252, 252, 253, 252, 252, 252, 116,\n",
       "          21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,  48, 179, 252, 253, 252, 221,  43,   2,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X[0]).reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747665dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7deabb6e3580>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X0[0].reshape(28,28),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3206b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape    # int 에서 float로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3558176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][222]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b65fc1-7d39-4721-aef8-29ae3daae42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` \"확률적\"경사하강법 -- 미니배치 쓰는 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79fd68fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9931)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n",
    "\n",
    "# batch 를 사용하기위해 Dataset과 Dataloader 생성\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "# batch size = 2048\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=2048)\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())        # Stochastic Gradient Descent 를 테스트 중이니 Adam은 사용하지 말자. 여기선 batch=2048\n",
    "# ## Step3: fit  \n",
    "for epoc in range(100):     # 원래는 epoch이 700이였는데 여기선 epoch 을 100 으로\n",
    "    for xi,yi in dl:        # len(dl) == 7    , 데이터 수 / batch_size 하면 6.xx 나오기때문.\n",
    "        # step1 \n",
    "        #yihat = net(xi)\n",
    "        # step2 \n",
    "        loss = loss_fn(net(xi),yi)\n",
    "        # step3     \n",
    "        loss.backward()\n",
    "        # step4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "# ## Step4: Predict \n",
    "((net(X) > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e5e81bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12665, torch.Size([12665, 784]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds) , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a670a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4536b6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.18408203125"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 올림 계산을 하면 6.18이 7이 됨.\n",
    "len(ds)/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c2e015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5923, 1, 28, 28]), torch.Size([6742, 1, 28, 28]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0.shape , X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f66933d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=2048 \t y_batch=2048\n",
      "x_batch=377 \t y_batch=377\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f'x_batch={len(xi.tolist())} \\t y_batch={len(yi.tolist())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa6914-d986-401d-a071-53534475d15f",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` GPU를 활용하는 \"확률적\"경사하강법 -- 실제적으로는 이게 최종알고리즘 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb360dea-f518-48d4-81c3-095a80f235b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9931)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n",
    "\n",
    "# batch 를 사용하기위해 Dataset과 Dataloader 생성\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=2048)\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())\n",
    "## Step3: fit  \n",
    "for epoc in range(100):     # 여기선 epoch 을 100 으로\n",
    "    for xi,yi in dl:        \n",
    "        # step1 \n",
    "        # step2 \n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        # step3     \n",
    "        loss.backward()\n",
    "        # step4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "# ## Step4: Predict\n",
    "net.to(\"cpu\")\n",
    "((net(X) > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "296f5c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ade57e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dca412",
   "metadata": {},
   "outputs": [],
   "source": [
    "2048*6 + 377"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581bfc7-1ddc-4c08-b48e-c7e3d935f9bf",
   "metadata": {},
   "source": [
    "# 5. 다중클래스 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a0408-7def-4176-a521-e05ba7a4da52",
   "metadata": {},
   "source": [
    "## A. 결론 (그냥 외우세요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb7b3b-acc9-4924-a07e-6c8bd38010fc",
   "metadata": {},
   "source": [
    "`-` 2개의 class를 구분하는 문제가 아니라 $k$개의 class를 구분해야 한다면? \n",
    "\n",
    "***일반적인 개념*** \n",
    "\n",
    "- 손실함수: BCE loss $\\to$ Cross Entropy loss \n",
    "- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n",
    "- **마지막층의 활성화: sig $\\to$ softmax**\n",
    "\n",
    "***파이토치 한정*** \n",
    "\n",
    "- **y의형태: (n,) vector + int형 // (n,k) one-hot encoded matrix + float형**\n",
    "- 손실함수: torch.nn.BCEWithLogitsLoss,  $\\to$ torch.nn.CrossEntropyLoss\n",
    "- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n",
    "- **마지막층의 활성화: None $\\to$ None (손실함수에 이미 마지막층의 활성화가 포함)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457d0d5-fad0-4406-93b8-09de9ee15abf",
   "metadata": {},
   "source": [
    "## B. 실습: 3개의 클래스를 구분 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d267516-87b1-44df-b52d-857fe4f415b2",
   "metadata": {},
   "source": [
    "`-` 정리된 코드1: 통계잘하는데 파이토치 못쓰는 사람의 코드\n",
    "- **y의형태 : one-hot encoded matrix + float형**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6d19628-cdc0-40ed-aebc-2f5ff95e0d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9827)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)/255    # X 의 dtype이 float임\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))).float()   #  y 의 dtype이 float임 , torch.nn.functional.one_hot 의 입력은 크기 0의 벡터다.\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,3),      # 0, 1, 2 총 3개의 클래스로 분류할거므로\n",
    "#    torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()       # 다중 분류이므로 crossentropy\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: 적합 \n",
    "for epoc in range(100):\n",
    "    ## step1 \n",
    "    netout = net(X)\n",
    "    ## step2 \n",
    "    loss = loss_fn(netout,y)\n",
    "    ## step3 \n",
    "    loss.backward()\n",
    "    ## step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "    \n",
    "## Step4: 적합 (혹은 적합결과확인)\n",
    "(netout.argmax(axis=1) == y.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2b6436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18623]), torch.Size([18623, 784]), torch.Size([18623, 3]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2)).shape , X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "946b14b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5324, -2.9470, -2.1638], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X[0])       # X[0] 이 0인건 아는거같은데 추정하는 값(2.5324이 1보다 크네? 확률이 아닌가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25f82f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5324, -2.9470, -2.1638],\n",
       "        [ 3.6886, -3.3973, -2.5815],\n",
       "        [ 4.3096, -4.9831, -2.1813],\n",
       "        ...,\n",
       "        [-2.5644,  0.4933,  1.5622],\n",
       "        [-1.2780, -3.7163,  5.1724],\n",
       "        [-2.9836, -1.5641,  5.0258]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bf48b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(net(X).argmax(axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "043297fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6501, -1.4164,  0.6702], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X[17])       # X[17] 이 2라고 잘못 추정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a61f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7deaba6b4910>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[17].reshape(28,28))    # X[17] 이 2라고 잘못 추정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d4ad30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[914, 6546, 16877]\n"
     ]
    }
   ],
   "source": [
    "print(net(X).argmax(axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0020c178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[(str(fname)) for fname in (path/'training/0').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ced32be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9827)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52a28f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.6886, -3.3973, -2.5815], grad_fn=<ViewBackward0>),\n",
       " <matplotlib.image.AxesImage at 0x7deaba4cb400>,\n",
       " tensor([1., 0., 0.]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net(X[1]), plt.imshow(X[0].reshape(28,28)), y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[1]), plt.imshow(X[-1].reshape(28,28)), y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998db61-9083-4921-84b4-86ed42cc331e",
   "metadata": {},
   "source": [
    "`-` 정리된 코드2: 파이토치를 잘하는 사람의 코드 :\n",
    "- **y의형태: (n,) vector + int형**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be8953ee-bd7a-489c-be88-0946917e949e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9827)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)/255        # X는 여전히 float 타입\n",
    "#y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))).float()\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))    # y는 int 타입인게 핵심\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,3),\n",
    "#    torch.nn.Softmax()     # 마찬가지로 softmax는 쓰지 않음. loss에서 softmax기능까지 내포\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: 적합 \n",
    "for epoc in range(100):\n",
    "    ## step1 \n",
    "    netout = net(X)\n",
    "    ## step2 \n",
    "    loss = loss_fn(netout,y)\n",
    "    ## step3 \n",
    "    loss.backward()\n",
    "    ## step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "## Step4: 적합 (혹은 적합결과확인)    \n",
    "(netout.argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f699d24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.Size([18623]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype , y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28df1d-249c-4ad0-b256-3df24edf3d76",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 완전같은코드임 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c26dc-146e-4d02-893f-de4b8d8f4b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ca91d-babe-4dca-99a6-ff7210f5710c",
   "metadata": {},
   "source": [
    "`-` 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2356aac1-893d-485c-98c6-42c800d6cce4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5324, -2.9470, -2.1638],\n",
       "        [ 3.6886, -3.3973, -2.5815],\n",
       "        [ 4.3096, -4.9831, -2.1813],\n",
       "        ...,\n",
       "        [-2.5644,  0.4933,  1.5622],\n",
       "        [-1.2780, -3.7163,  5.1724],\n",
       "        [-2.9836, -1.5641,  5.0258]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd9c8eff-8c9f-4460-b08d-2bc11812216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60565f1-c18c-4045-94a6-d10666527408",
   "metadata": {},
   "source": [
    "`-` 수식 \n",
    "\n",
    "- $\\text{sig}(u)=\\frac{e^u}{1+e^u}$\n",
    "- $\\text{softmax}({\\boldsymbol u})=\\text{softmax}([u_1,u_2,\\dots,u_k])=\\big[ \\frac{e^{u_1}}{e^{u_1}+\\dots e^{u_k}},\\dots,\\frac{e^{u_k}}{e^{u_1}+\\dots e^{u_k}}\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75eba0b-deb7-4a3f-b897-1c4ebe6d37bb",
   "metadata": {},
   "source": [
    "`-` torch.nn.Softmax() 손계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4313a62-900c-422b-ae64-24e846f5cf6c",
   "metadata": {},
   "source": [
    "(예시1) -- 잘못계산 : dim 이 잘못됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d3ee620-908d-4a66-9c2e-314f7ecfadce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6c35f4f-9b07-4de6-b7bb-6b8a09b9490c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0000, -2.0000,  0.0000],\n",
       "        [ 3.1400,  3.1400,  3.1400],\n",
       "        [ 0.0000,  0.0000,  2.0000],\n",
       "        [ 2.0000,  2.0000,  4.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터의 개수 : 5개\n",
    "netout = torch.tensor([[-2.0,-2.0,0.0],\n",
    "                        [3.14,3.14,3.14],\n",
    "                        [0.0,0.0,2.0],\n",
    "                        [2.0,2.0,4.0],\n",
    "                        [0.0,0.0,0.0]])\n",
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e65a43be-188d-4304-a8b5-ba766136bee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0041, 0.0041, 0.0115],\n",
       "        [0.7081, 0.7081, 0.2653],\n",
       "        [0.0306, 0.0306, 0.0848],\n",
       "        [0.2265, 0.2265, 0.6269],\n",
       "        [0.0306, 0.0306, 0.0115]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a934bda-8458-44f5-9f58-f1d85d19a52a",
   "metadata": {},
   "source": [
    "(예시2) -- 이게 맞게 계산되는 것임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bab3e4f5-2796-4399-b223-d910ea182e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0dafa544-0d9e-4977-8bb7-da1c945e0bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0000, -2.0000,  0.0000],\n",
       "        [ 3.1400,  3.1400,  3.1400],\n",
       "        [ 0.0000,  0.0000,  2.0000],\n",
       "        [ 2.0000,  2.0000,  4.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "525e3435-eefa-49d9-82a8-13cb40571046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1065, 0.1065, 0.7870],\n",
       "        [0.3333, 0.3333, 0.3333],\n",
       "        [0.1065, 0.1065, 0.7870],\n",
       "        [0.1065, 0.1065, 0.7870],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout) # column 끼리 합치면 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff382de5-9054-414e-92dc-91714810b9d6",
   "metadata": {},
   "source": [
    "(예시3) -- 차원을 명시안하면 맞게 계산해주고 경고 줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c7c8926-7e1e-4994-a04c-4ca3ea3f14a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8fafa70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies the Softmax function to an n-dimensional input Tensor.\n",
      "\n",
      "Rescales them so that the elements of the n-dimensional output Tensor\n",
      "lie in the range [0,1] and sum to 1.\n",
      "\n",
      "Softmax is defined as:\n",
      "\n",
      ".. math::\n",
      "    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
      "\n",
      "When the input Tensor is a sparse tensor then the unspecified\n",
      "values are treated as ``-inf``.\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*)` where `*` means, any number of additional\n",
      "      dimensions\n",
      "    - Output: :math:`(*)`, same shape as the input\n",
      "\n",
      "Returns:\n",
      "    a Tensor of the same dimension and shape as the input with\n",
      "    values in the range [0, 1]\n",
      "\n",
      "Args:\n",
      "    dim (int): A dimension along which Softmax will be computed (so every slice\n",
      "        along dim will sum to 1).\n",
      "\n",
      ".. note::\n",
      "    This module doesn't work directly with NLLLoss,\n",
      "    which expects the Log to be computed between the Softmax and itself.\n",
      "    Use `LogSoftmax` instead (it's faster and has better numerical properties).\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Softmax(dim=1)\n",
      "    >>> input = torch.randn(2, 3)\n",
      "    >>> output = m(input)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/activation.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     Softmax"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e59a46d-e2c1-4247-aae3-ff6171bb1d17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0000, -2.0000,  0.0000],\n",
       "        [ 3.1400,  3.1400,  3.1400],\n",
       "        [ 0.0000,  0.0000,  2.0000],\n",
       "        [ 2.0000,  2.0000,  4.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "040d3bb0-49d1-4bc4-9ffb-eaadbd1228bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myuser/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1065, 0.1065, 0.7870],\n",
       "        [0.3333, 0.3333, 0.3333],\n",
       "        [0.1065, 0.1065, 0.7870],\n",
       "        [0.1065, 0.1065, 0.7870],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb6df4-b9de-48c5-a0d2-65373288f3df",
   "metadata": {},
   "source": [
    "(예시4) -- 진짜 손계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e80c623f-eab4-4324-a079-16252ece7706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0000, -2.0000,  0.0000],\n",
       "        [ 3.1400,  3.1400,  3.1400],\n",
       "        [ 0.0000,  0.0000,  2.0000],\n",
       "        [ 2.0000,  2.0000,  4.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9551d3b6-a6c4-4d6a-a402-8be14fe1c0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1353,  0.1353,  1.0000],\n",
       "        [23.1039, 23.1039, 23.1039],\n",
       "        [ 1.0000,  1.0000,  7.3891],\n",
       "        [ 7.3891,  7.3891, 54.5981],\n",
       "        [ 1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(netout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b5993-06b0-469b-a8aa-dfb6ef26a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f7e12995-da3a-4c23-8146-301dbaec52dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(netout[1])/torch.exp(netout[1]).sum() # 두번째 obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff7bfcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758fd94-753e-4f54-b4b2-b680d5400bee",
   "metadata": {},
   "source": [
    "## D. CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19777a84-daf8-4868-a2d7-d802bc4b37a2",
   "metadata": {},
   "source": [
    "`-` 수식 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25900c98-21f2-47dd-818f-53b06dde35fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "***`# 2개의 카테고리`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc931b-5238-41a4-b2f7-f0c76c2a6dfe",
   "metadata": {},
   "source": [
    "`-` 예제1: BCELoss vs BCEWithLogisticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "accd1529-6526-4a8f-85a2-e068d09f025b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor([[-1.],\n",
       "         [ 0.],\n",
       "         [ 1.]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0,0,1]).reshape(-1,1).float()\n",
    "netout = torch.tensor([-1, 0, 1]).reshape(-1,1).float()\n",
    "y,netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9dfb02cb-1749-4391-8446-fdec0734f6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689],\n",
       "        [0.5000],\n",
       "        [0.7311]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산방법1: 공식암기\n",
    "sig = torch.nn.Sigmoid()\n",
    "yhat = sig(netout)\n",
    "yhat    # 각 원소마다 sigmoid를 취함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5af91fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4399)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.sum(torch.log(yhat)*y + torch.log(1-yhat)*(1-y))/3      # BCE Loss 손으로 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2768f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2689],\n",
       "         [0.5000],\n",
       "         [0.7311]]),\n",
       " tensor(False))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat , yhat.sum()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "95a04437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor(True))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y , y.sum()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "736829af-f7ab-440d-82b5-768c60bea69a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4399)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산방법2: torch.nn.BCELoss() 이용\n",
    "sig = torch.nn.Sigmoid()\n",
    "\n",
    "# netout 에 sigmoid(activation함수)를 태운걸 yhat이라 한다.\n",
    "yhat = sig(netout)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "loss_fn(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "244f971b-a33a-46b5-9c99-38e4e9638ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4399)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산방법3: torch.nn.BCEWithLogitsLoss() 이용\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# netout 에 activation을 태우지 않고 바로 loss_fn에 입력해버림\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4a3cf-7911-4b4a-b833-50541329c0b9",
   "metadata": {},
   "source": [
    "`-` 예제2: BCEWithLogisticLoss vs CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ff6ff61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689, 0.7311],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.7311, 0.2689]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([sig(netout),1-sig(netout)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e54ce802-ab28-4f1b-b061-462feb61baf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.]]),\n",
       " tensor([[3., 2.],\n",
       "         [2., 2.],\n",
       "         [5., 6.]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터의 개수 : 3개\n",
    "netout = torch.tensor([[3,2],[2,2],[5,6]]).float()\n",
    "y = torch.tensor([[1,0],[1,0],[0,1]]).float()\n",
    "y,netout #,netout[:,[1]]-netout[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f6f67805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "670b884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fcb10b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.2689],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.2689, 0.7311]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bfeba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.0000],\n",
       "        [0.5000, 0.0000],\n",
       "        [0.0000, 0.7311]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(netout)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2cc88737-fd06-450d-ad23-0084d1924e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4399)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산방법1: 공식암기\n",
    "-torch.sum(torch.log(softmax(netout))*y)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "992b1412-3913-4ca1-87e7-829c2951658d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4399)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산방법2: torch.nn.CrossEntropyLoss() 이용 + y는 one-hot으로 정리\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090abc37-f55c-42ae-8116-4e34097f2b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법3: torch.nn.CrossEntropyLoss() 이용 + y는 0,1 로 정리\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc824652-98e3-48b4-9025-c874e37c6fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b9992-40a2-4922-9803-69d6ea0e466f",
   "metadata": {},
   "source": [
    "***`# 3개의 카테고리, 데이터의 개수는 5개`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "548b72d7-3caa-43be-83fa-c920eafb9654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 2, 2, 0]),\n",
       " tensor([[0, 0, 1],\n",
       "         [0, 1, 0],\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 1],\n",
       "         [1, 0, 0]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([2,1,2,2,0])\n",
    "y_onehot = torch.nn.functional.one_hot(y)   # torch.nn.functional.one_hot 의 입력은 크기 0의 벡터\n",
    "netout = torch.tensor(\n",
    "    [[-2.0000, -2.0000,  0.0000],\n",
    "     [ 3.1400,  3.1400,  3.1400],\n",
    "     [ 0.0000,  0.0000,  2.0000],\n",
    "     [ 2.0000,  2.0000,  4.0000],\n",
    "     [ 0.0000,  0.0000,  0.0000]]\n",
    ")\n",
    "y,y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "01854de7-1bf6-424e-8365-d7a99e74994f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5832)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 방법1 -- 추천X\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y_onehot.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea9ac430-21df-4477-90ed-f875e53b9316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5832)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 방법2 -- 추천O\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "22e89404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386860b6-e2ce-49e9-8bda-dccab3f625ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 방법3 -- 공식.. (이걸 쓰는사람은 없겠지?)\n",
    "softmax = torch.nn.Softmax() \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "- torch.sum(torch.log(softmax(netout))*y_onehot)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dce89e-21ad-42b2-8a70-b35d6e10c4dc",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e05f3e-a7f1-4b31-b90a-17b4812540c2",
   "metadata": {},
   "source": [
    "`-` 계산하는 공식을 아는것도 중요한데 **torch.nn.CrossEntropyLoss() 에는 softmax 활성화함수가 이미 포함되어 있다는 것을 확인**하는 것이 더 중요함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e564a21-4c20-4ef9-ac3b-2bb66a72663e",
   "metadata": {},
   "source": [
    "`-` **torch.nn.CrossEntropyLoss()** 는 사실 **torch.nn.CEWithSoftmaxLoss()** 정도로 바꾸는 것이 더 말이 되는 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57818858-a52b-47dd-a8e9-2e84312cd8dd",
   "metadata": {},
   "source": [
    "## E. Minor Topic: 이진분류와 CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96ecda-1fbb-48f7-b5de-8cede28b9b9c",
   "metadata": {},
   "source": [
    "`-` 2개의 클래스일경우에도 CrossEntropy를 쓸 수 있지 않을까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3ebe3ec-d6b4-4f39-8ba7-84185ac104f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9983)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1]).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1))\n",
    "#y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1))).float()   #  y 의 dtype이 float임 , torch.nn.functional.one_hot 의 입력은 크기 0의 벡터다.\n",
    "\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,2),\n",
    "    #torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: fit  \n",
    "for epoc in range(70): \n",
    "    ## 1 \n",
    "    ## 2 \n",
    "    loss= loss_fn(net(X),y) \n",
    "    ## 3 \n",
    "    loss.backward()\n",
    "    ## 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad() \n",
    "## Step4: Predict \n",
    "softmax = torch.nn.Softmax()\n",
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d5b8e089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9983)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1]).reshape(-1,1*28*28)/255\n",
    "#y = torch.tensor([0]*len(X0) + [1]*len(X1))\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1))).float()   #  one_hot 방식\n",
    "\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,2),\n",
    "    #torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: fit  \n",
    "for epoc in range(70): \n",
    "    ## 1 \n",
    "    ## 2 \n",
    "    loss= loss_fn(net(X),y) \n",
    "    ## 3 \n",
    "    loss.backward()\n",
    "    ## 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad() \n",
    "## Step4: Predict \n",
    "softmax = torch.nn.Softmax()\n",
    "((net(X).argmax(axis=1))==y.argmax(axis=1)).float().mean()      # one_hot 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd7a09-5563-4565-aecc-68dd25deb87e",
   "metadata": {},
   "source": [
    "`-` 이진분류문제 = \"y=0 or y=1\" 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d270f-de1a-4653-98bb-ee87cde06921",
   "metadata": {},
   "source": [
    "`-` softmax, sigmoid\n",
    "\n",
    "- softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. \n",
    "- sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a26d3-6f3f-40e2-be5a-fdbc31ca3615",
   "metadata": {},
   "source": [
    "`-` 그런데 \"실패확률=1-성공확률\" 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9103832-a80d-4f2e-9f06-f272ca11fc2b",
   "metadata": {},
   "source": [
    "`-` 즉 아래는 같은 표현력을 가진 모형이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1f316-19fe-4843-9540-58001bc536a3",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/guebin/DL2024/cdbdf23589efc2198260ab9c749f1757f67a128d/posts/05wk-1-fig2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a371-b198-4121-b086-0e24b995701c",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/guebin/DL2024/cdbdf23589efc2198260ab9c749f1757f67a128d/posts/05wk-1-fig1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30736d-9e20-48e9-a771-f9939899864a",
   "metadata": {},
   "source": [
    "`-` 둘은 같은 표현력을 가진 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. $\\to$ sigmoid를 사용하는 모형이 비용은 싸고 효과는 동일하다는 말 $\\to$ 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. \n",
    "\n",
    "- softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac81d5-7949-44a8-87cd-ab7cb11e540a",
   "metadata": {},
   "source": [
    "## F. 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029e2bd-7e58-4446-987b-6b22c2844762",
   "metadata": {},
   "source": [
    "`-` 결론 \n",
    "\n",
    "1. 소프트맥스는 시그모이드의 확장이다. \n",
    "2. 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc630f4-5ac1-490c-82da-c5e07e0080b9",
   "metadata": {},
   "source": [
    "`-` 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (그냥 좀 비효율적인 느낌이 드는 것 뿐임. 흑백이미지를 칼라잉크로 출력하는 느낌) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f790de3-194e-4413-80f9-f9ba4430583c",
   "metadata": {},
   "source": [
    "***참고***\n",
    "\n",
    "|$y$|분포가정|마지막층의 활성화함수|손실함수|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|3.45, 4.43, ... (연속형) |정규분포|None (or Identity)|MSE|\n",
    "|0 or 1|이항분포 with $n=1$ (=베르누이) |Sigmoid| BCE|\n",
    "|[0,0,1], [0,1,0], [1,0,0]| 다항분포 with $n=1$|Softmax| Cross Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f776f61-384f-4d7e-b67d-9db98b8da39f",
   "metadata": {},
   "source": [
    "# 6. HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90580d98-35f1-4c48-89bb-7e1b55519091",
   "metadata": {
    "tags": []
   },
   "source": [
    "아래와 같은 자료가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46503051-c102-43ea-853a-9d369fd6d82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1]).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618ef1e-be5c-473f-8ea4-034cca59ef61",
   "metadata": {},
   "source": [
    "`(1)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- 손실함수를 `torch.nn.BCELoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09610a12-aeae-4c1e-b6aa-bff9dcb7f83f",
   "metadata": {},
   "source": [
    "`(2)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- 손실함수를 `torch.nn.BCEWithLogitsLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c45eb-8c7a-41b1-803b-d5e9b7d6462f",
   "metadata": {},
   "source": [
    "`(3)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- y를 one_hot 인코딩하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff23187-feaa-4c16-afd6-e0b993e9b992",
   "metadata": {
    "tags": []
   },
   "source": [
    "**hint** 원핫인코딩을 위해 아래의 함수를 사용하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229bc2-780f-46da-853a-2adf85dde805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y, torch.nn.functional.one_hot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b85d1-a3ba-4e24-983a-c57d2682a570",
   "metadata": {},
   "source": [
    "`(4)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a90f5b-514b-4df3-9683-62005cdbc216",
   "metadata": {},
   "source": [
    "`(5)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- batch_size = 1024 로 설정한뒤 mini_batch를 이용한 학습을 하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라.  \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- 총 iteration 수가 325가 되도록 적절하게 epoch 을 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fae3bc-20b7-4674-977e-b8a3c143a6de",
   "metadata": {},
   "source": [
    "`(6)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- batch_size = 512 로 설정한뒤 mini_batch를 이용한 학습을 하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- 총 iteration 수가 325가 되도록 적절하게 epoch 을 설정하라. \n",
    "- GPU를 활용하여 학습하라."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
