{
 "cells": [
  {
   "cell_type": "raw",
   "id": "51e1d740-b82a-4af2-a0d9-394ed0756728",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d"
   },
   "source": [
    "---\n",
    "title: \"05wk-1: 깊은 신경망 (4) -- GPU 사용법, SGD, Softmax와 CrossEntropy\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/01/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d0a99-db20-4891-b2e2-14080c1d8a43",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/05wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85595e0d-d9a6-4e79-a1e6-8ce6d7465d08",
   "metadata": {
    "id": "4d47a7c9",
    "tags": []
   },
   "source": [
    "# 1. 강의영상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a52785-46dd-46f5-a7b5-9879287cc580",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-wjNGgd4gmQJbQ66NLjUC2y&si=dusDZAwGOJS9TOKJ >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53144898-ba7e-4f40-b9c4-77020801a055",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84306e3-564a-472e-ae0a-5322379da476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import fastai.data.all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cbe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59a45c-2115-47c9-bbc7-f07f66d9d6d7",
   "metadata": {},
   "source": [
    "# 3. CPU vs GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618df51-1ff6-4c58-94df-ced67d4c37df",
   "metadata": {},
   "source": [
    "`-` 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f9185-d898-4bbb-906a-bb752800a434",
   "metadata": {},
   "source": [
    "`-` 코랩에서 GPU설정하는 방법: (아래영상참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a83013-ee9b-47d0-a043-665c19337c62",
   "metadata": {},
   "source": [
    "{{<video https://youtu.be/GyBL2YBCzoc >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9403b4-9751-430e-8810-8ca8f2302dac",
   "metadata": {},
   "source": [
    "## A. GPU 사용방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb51ffe-27e0-4dde-a471-bc06f3926c1e",
   "metadata": {},
   "source": [
    "`-` cpu 연산이 가능한 메모리에 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d54e22e-e238-4206-a7b6-ad0a42ea3048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(43052)\n",
    "x_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \n",
    "y_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \n",
    "net_cpu = torch.nn.Linear(1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95efdc-7889-4892-b0ec-00189110af24",
   "metadata": {},
   "source": [
    "`-` gpu 연산이 가능한 메모리에 데이터 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91fc404a-ec68-4306-8c44-10f453373da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 15:32:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   56C    P3              88W / 420W |   1052MiB / 24576MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22b2e9c-9e14-4ad2-84ef-8885ffcea830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(43052)\n",
    "x_gpu = x_cpu.to(\"cuda:0\")\n",
    "y_gpu = y_cpu.to(\"cuda:0\")\n",
    "net_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9601e845-9a4b-4bec-9053-879be0749db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 15:33:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   58C    P2             135W / 420W |   1321MiB / 24576MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fbd2e-84fc-47cb-ad1d-6f4b26f137e0",
   "metadata": {},
   "source": [
    "> GPU에 메모리를 올리면 GPU메모리가 점유된다! (26MiB -> 287MiB) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53fcb8-275d-41c4-b3dc-d56d5f5a96fc",
   "metadata": {},
   "source": [
    "`-` cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed37ca9-745f-4e78-bea6-c17993c9fcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.1000],\n",
       "         [0.2000]]),\n",
       " tensor([[0.0000],\n",
       "         [0.2000],\n",
       "         [0.4000]]),\n",
       " Parameter containing:\n",
       " tensor([[-0.3467]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.8470], requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cpu, y_cpu, net_cpu.weight, net_cpu.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7cb375-73ca-4bdd-a290-ecedfc2bf7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.1000],\n",
       "         [0.2000]], device='cuda:0'),\n",
       " tensor([[0.0000],\n",
       "         [0.2000],\n",
       "         [0.4000]], device='cuda:0'),\n",
       " Parameter containing:\n",
       " tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.8470], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpu, y_gpu, net_gpu.weight, net_gpu.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5b0f1-7529-47ec-bab5-8ed91640a1ac",
   "metadata": {},
   "source": [
    "`-` gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c42fa-5f99-411b-9655-0711a2375220",
   "metadata": {},
   "source": [
    "(예시1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9843f929-e8e0-46b3-a799-c99ba1e46fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8470],\n",
       "        [-0.8817],\n",
       "        [-0.9164]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_cpu(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fbba8-de43-4442-83f0-ab19642039b0",
   "metadata": {},
   "source": [
    "(예시2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bdd165-cb2c-4c9b-9d16-e03c2346f273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8470],\n",
       "        [-0.8817],\n",
       "        [-0.9164]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_gpu(x_gpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ee03b-ddcd-479e-a1de-681c629bd241",
   "metadata": {},
   "source": [
    "(예시3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae322-4185-43e4-92c2-8b3229153934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_cpu(x_gpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu.to(\"cuda:0\")\n",
    "net_gpu(x_cpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_gpu(x_cpu.to(\"cuda:0\")\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d649b43-9d57-4ea7-a5be-41bf78292a98",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "#### 강의중 net을 재선언한 이유\n",
    "\n",
    "`-` 아래와 같이 `x_cpu` 혹은 `y_cpu`에 `.to(\"cuda:0\")`메소드를 쓸 경우 \n",
    "\n",
    "```Python\n",
    "x_cpu.to(\"cuda:0\")\n",
    "y_cpu.to(\"cuda:0\")\n",
    "```\n",
    "\n",
    "`x_cpu`와 `y_cpu`는 cpu에 그대로 있음. \n",
    "\n",
    "`-` 그런데 아래와 같이 `net_cpu`에서 `.to(\"cuda:0\")`메소드를 쓸 경우 \n",
    "\n",
    "```Python\n",
    "net_cpu.to(\"cuda:0\")\n",
    "```\n",
    "\n",
    "`net_cpu` 자체가 gpu에 올라가게 됨. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a42c6-4fea-4011-9940-5eb438d3d937",
   "metadata": {},
   "source": [
    "(예시4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd836ac-5da1-4d85-ad0b-1de6c16120cf",
   "metadata": {},
   "source": [
    "(예시5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538e301-9c48-40f2-b5bd-6e599a1521cc",
   "metadata": {},
   "source": [
    "(예시6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72472217-0944-4268-8a3f-6bee4b8c8b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2068, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y_gpu-net_gpu(x_gpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1fe1e-ade1-4fdf-b814-97b14b9a05e4",
   "metadata": {},
   "source": [
    "(예시7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc92139-3108-47c5-89b1-b01266c9d19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43my_gpu\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnet_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.mean((y_gpu-net_cpu(x_cpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12465e2-8b7a-4a28-8f0b-51c6aa716af8",
   "metadata": {},
   "source": [
    "(예시8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07158f86-6034-411e-8530-633758eaad34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43my_cpu\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnet_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gpu\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.mean((y_cpu-net_gpu(x_gpu))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25a3a7a-bc4e-4007-83cb-d2f8c3f9a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "net_gpu(x_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63858552-cc37-453f-8cf8-cac3d3d267c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.mean((y_cpu-net_cpu(x_cpu))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd12bfa-6e85-4265-962e-8619fe0b2105",
   "metadata": {},
   "source": [
    "## B. 시간측정 (예비학습) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19a108c-fa7a-411d-8a3e-67d40fe149aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbdf1cd-a97a-44aa-93f1-fe3fa2b3c76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436f27b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d8cc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30453991889953613"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e185b-733a-452e-9221-d5cdfad0ddbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. CPU vs GPU (512 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb010616-edf0-418d-ab29-5ad06b7cb5d5",
   "metadata": {},
   "source": [
    "`-` CPU (512 nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f6b1282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3479437828063965"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85e79b-564b-4e22-b35b-3d50de40a64d",
   "metadata": {},
   "source": [
    "`-` GPU (512 nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5300c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972074bf-14bf-4c38-8f27-171a21e58699",
   "metadata": {
    "tags": []
   },
   "source": [
    "- CPU가 더 빠르다??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f64f6-bca2-42d6-87b3-d9609729d338",
   "metadata": {},
   "source": [
    "## D. CPU vs GPU (20,480 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994456c9-cfbd-4164-8d5f-23199460924b",
   "metadata": {},
   "source": [
    "`-` CPU (20,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ebfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,20480),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20480,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea7890-1ec1-4d6f-af39-1392882ebd24",
   "metadata": {},
   "source": [
    "`-` GPU (20,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632bc05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,20480),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20480,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94ff63-02a6-4de3-98c7-4905f1c8f44f",
   "metadata": {},
   "source": [
    "- 왜 이런 차이가 나는가? \n",
    "- 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e6aa5-069b-4069-9699-a16451da140f",
   "metadata": {},
   "source": [
    "## E. CPU vs GPU (204,800 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf5035-822b-4dcd-8023-1cb59d8a1cd7",
   "metadata": {},
   "source": [
    "`-` CPU (204,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567de51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1)\n",
    "y=torch.randn(100).reshape(-1,1)*0.01\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,204800),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(204800,1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0476c5-8e67-4258-b4b8-61b2a4419f04",
   "metadata": {},
   "source": [
    "`-` GPU (204,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76da7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5) \n",
    "x=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\n",
    "y=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n",
    "#---#\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,204800),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(204800,1)\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "t1 = time.time()\n",
    "for epoc in range(1000):\n",
    "    # 1 \n",
    "    yhat = net(x)\n",
    "    # 2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # 3 \n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b8bed-a7c0-408f-86a2-dcc55f4ee356",
   "metadata": {},
   "source": [
    "# 4. \"확률적\" 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd71642-4b9d-4e05-b617-9559249bb814",
   "metadata": {},
   "source": [
    "## A. 의문: 좀 이상하지 않아요? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f7132-5af5-4930-872f-078a4c5082c0",
   "metadata": {},
   "source": [
    "`-` 국민상식: GPU 비싸요.. <https://bbs.ruliweb.com/community/board/300143/read/61066881>\n",
    "\n",
    "- GPU 메모리 많아봐야 24GB, 그래도 비싸요.. <http://shop.danawa.com/virtualestimate/?controller=estimateMain&methods=index&marketPlaceSeq=16>\n",
    "- GPU 메모리가 80GB일 경우 가격: <https://prod.danawa.com/info/?pcode=21458333>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a05215-135e-463f-bbbf-771b5f65b0ea",
   "metadata": {},
   "source": [
    "`-` 우리가 분석하는 데이터: 빅데이터..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e8920-2867-49f5-bb5c-7d88f7ebf771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-10,10,100000).reshape(-1,1)\n",
    "eps = torch.randn(100000).reshape(-1,1)\n",
    "y = x*2 + eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70984f-d684-4ddb-b948-113fb16d4f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'o',alpha=0.05)\n",
    "plt.plot(x,2*x,'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418df34-7660-4e95-9080-85d4e976a813",
   "metadata": {},
   "source": [
    "`-` 데이터의 크기가 커지는 순간 `X.to(\"cuda:0\")`, `y.to(\"cuda:0\")` 쓰면 난리나겠는걸? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea0878-c743-4930-bf34-76abaf578783",
   "metadata": {},
   "source": [
    "`-` 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0a457-d71c-4959-a357-4d53d43f3e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x[::100],y[::100],'o',alpha=0.05)\n",
    "plt.plot(x,2*x,'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[::1000].shape # 1000개씩 점프하면서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba287425-1536-40de-92ab-bd47c1744d75",
   "metadata": {},
   "source": [
    "- 대충 이거만 가지고 적합해도 충분히 정확할것 같은데?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbad39b-ca43-48d4-98dd-4abd5ba19890",
   "metadata": {},
   "source": [
    "## B. X,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d7735-3663-4ca2-99f2-2829b9dd9c66",
   "metadata": {},
   "source": [
    "`-` 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395028f-300c-4d92-8ee8-e18cd8c6c28a",
   "metadata": {},
   "source": [
    "`-` 아래의 알고리즘을 생각해보자. \n",
    "\n",
    "1. 데이터를 반으로 나눈다. \n",
    "2. 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. \n",
    "3. yhat, loss, grad, update 수행\n",
    "4. 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. \n",
    "5. yhat, loss, grad, update 수행 \n",
    "6. 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. \n",
    "7. 반복 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484eb1d",
   "metadata": {},
   "source": [
    "> 이러면 되는거아니야???? ---> 맞아요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685c5eb-e7c3-4533-8ca2-5e1d1507a3b5",
   "metadata": {},
   "source": [
    "## C. 경사하강법, 확률적경사하강법, 미니배치 경사하강법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f3b2e-3405-4992-86d3-ab928e8f2785",
   "metadata": {},
   "source": [
    "10개의 샘플이 있다고 가정. $\\{(x_i,y_i)\\}_{i=1}^{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6ccf-7523-43da-826a-1891d10ee663",
   "metadata": {},
   "source": [
    "`# ver1` --  모든 샘플을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f45b4-ee4a-4960-b537-4a8342eebe60",
   "metadata": {},
   "source": [
    "(epoch 1) $loss=\\sum_{i=1}^{10}(y_i-w_0-w_1x_i)^2 \\to slope  \\to update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9caaf97-f13a-4742-9966-f982058c0a00",
   "metadata": {},
   "source": [
    "(epoch 2) $loss=\\sum_{i=1}^{10}(y_i-w_0-w_1x_i)^2 \\to slope  \\to update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45cefa-9807-4437-83f3-db58cf80ebd5",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376f5ff-5786-45a8-a3a5-272a93d3901c",
   "metadata": {},
   "source": [
    "> 우리가 항상 이렇게 했죠!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700db88-6fbd-4e2f-a1d8-8861bb6674d1",
   "metadata": {},
   "source": [
    "`# ver2` -- 하나의 샘플만을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a184d8-bf1b-4abe-a723-27b1e94e848d",
   "metadata": {},
   "source": [
    "(epoch 1) \n",
    "\n",
    "- $loss=(y_1-w_0-w_1x_1)^2 \\to slope \\to update$\n",
    "- $loss=(y_2-w_0-w_1x_2)^2 \\to slope \\to update$\n",
    "- ...\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee15892-09b4-4f29-aa05-c720ca6a3777",
   "metadata": {},
   "source": [
    "(epoch 2) \n",
    "\n",
    "- $loss=(y_1-w_0-w_1x_1)^2  \\to slope  \\to  update$\n",
    "- $loss=(y_2-w_0-w_1x_2)^2  \\to slope  \\to  update$\n",
    "- ...\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab1cd8-be7b-4ee5-8c97-c196f9de88fd",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9dc8d0-b7ea-4778-966e-c2c8e8cba3e1",
   "metadata": {},
   "source": [
    "`# ver3` -- $m (\\leq n)$ 개의 샘플을 이용하여 slope 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626bb7d-07e2-4078-a9e2-c9db8ee13f3d",
   "metadata": {},
   "source": [
    "$m=3$이라고 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3b3aa-cbc0-48c0-bbf5-40ba698a1e34",
   "metadata": {},
   "source": [
    "(epoch 1) \n",
    "\n",
    "- $loss=\\sum_{i=1}^{3}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=4}^{6}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=7}^{9}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1e55b-045c-4389-81fd-69ef9ab9b122",
   "metadata": {},
   "source": [
    "(epoch 2) \n",
    "\n",
    "- $loss=\\sum_{i=1}^{3}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=4}^{6}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=\\sum_{i=7}^{9}(y_i-w_0-w_1x_i)^2  \\to  slope  \\to  update$\n",
    "- $loss=(y_{10}-w_0-w_1x_{10})^2  \\to  slope  \\to  update$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ff5b2-8fa4-4756-ae22-21995eca38f7",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5071655-fd59-4193-a765-40011743de77",
   "metadata": {},
   "source": [
    "## D. 용어의 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eff37b-d7aa-4468-9098-908dc16538cb",
   "metadata": {},
   "source": [
    "**옛날**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cb47c-cba9-41b0-977d-93bbef211aca",
   "metadata": {},
   "source": [
    "`-` ver1: gradient descent, batch gradient descent\n",
    "\n",
    "`-` ver2: stochastic gradient descent (sample 1개)\n",
    "\n",
    "`-` ver3: mini-batch gradient descent, mini-batch stochastic gradient descent (sample n개 == batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd197cc-e449-421c-b38e-15270423aed6",
   "metadata": {},
   "source": [
    "**요즘**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf62fb9-69da-4fbd-a4fb-4790fb635c9c",
   "metadata": {},
   "source": [
    "`-` ver1: gradient descent\n",
    "\n",
    "`-` ver2: stochastic gradient descent with batch size = 1\n",
    "\n",
    "`-` **ver3: stochastic gradient descent**\n",
    "- https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648dc2f-b339-4500-9898-aac476469b53",
   "metadata": {},
   "source": [
    "## E. Dataset(`ds`), DataLoader(`dl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc8064",
   "metadata": {},
   "source": [
    "> 취지는 알겠으나, C의 과정을 실제 구현하려면 진짜 힘들것 같아요.. (입코딩과 손코딩의 차이) --> 이걸 해결하기 위해서 파이토치에서는 DataLoader라는 오브젝트를 준비했음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bfcf7-8401-407d-843d-64280a88dcb5",
   "metadata": {},
   "source": [
    "`-` ds: 섭스크립터블함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd133a9e-03df-4a28-80a1-eb07f327da25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 1.],\n",
       "        [2., 1.],\n",
       "        [3., 1.],\n",
       "        [4., 1.],\n",
       "        [5., 0.],\n",
       "        [6., 0.],\n",
       "        [7., 0.],\n",
       "        [8., 0.],\n",
       "        [9., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(range(10)).float().reshape(-1,1)\n",
    "y=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\n",
    "torch.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08062bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([x,y],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b195f8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Dataset wrapping tensors.\n",
      "\n",
      "Each sample will be retrieved by indexing tensors along the first dimension.\n",
      "\n",
      "Args:\n",
      "    *tensors (Tensor): tensors that have the same size of the first dimension.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/utils/data/dataset.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch.utils.data.TensorDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b339974a-d346-4aef-b991-eb737f4dbbba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7496a300ed00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "ds=torch.utils.data.TensorDataset(x,y)\n",
    "ds  # 뭐 정보가 없네.. 어쩌란거지..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac0ac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_protocol',\n",
       " 'tensors']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7a0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08623d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'__iter__'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "130c639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'__getitem__'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a881d",
   "metadata": {},
   "source": [
    "__iter__ 이외에도 __getitem__ 이 있으면 for 문을 사용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "279cd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.]), tensor([1.]))\n",
      "(tensor([1.]), tensor([1.]))\n",
      "(tensor([2.]), tensor([1.]))\n",
      "(tensor([3.]), tensor([1.]))\n",
      "(tensor([4.]), tensor([1.]))\n",
      "(tensor([5.]), tensor([0.]))\n",
      "(tensor([6.]), tensor([0.]))\n",
      "(tensor([7.]), tensor([0.]))\n",
      "(tensor([8.]), tensor([0.]))\n",
      "(tensor([9.]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "for i in ds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f1739b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([4.])\n",
      "tensor([5.])\n",
      "tensor([6.])\n",
      "tensor([7.])\n",
      "tensor([8.])\n",
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "for i in ds:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20e5d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<iterator at 0x7496a300e670>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c31e7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.]), tensor([1.]))\n",
      "(tensor([1.]), tensor([1.]))\n",
      "(tensor([2.]), tensor([1.]))\n",
      "(tensor([3.]), tensor([1.]))\n",
      "(tensor([4.]), tensor([1.]))\n",
      "(tensor([5.]), tensor([0.]))\n",
      "(tensor([6.]), tensor([0.]))\n",
      "(tensor([7.]), tensor([0.]))\n",
      "(tensor([8.]), tensor([0.]))\n",
      "(tensor([9.]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # __getitem__ 메서드를 사용하여 인덱스를 통해 요소에 접근\n",
    "        # print(index)\n",
    "        item = ds.__getitem__(index)\n",
    "        print(item)\n",
    "        index += 1\n",
    "    except IndexError:\n",
    "        # 더 이상 요소가 없으면 IndexError가 발생하므로 반복을 종료\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6298cfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.]), tensor([1.])),\n",
       " (tensor([1.]), tensor([1.])),\n",
       " (tensor([7.]), tensor([0.])))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set(dir(ds)) & {'__getitem__'}\n",
    "# __getitem__ 이 있다는건 리스트처럼 사용하는게 가능하다는 것! , 추가로 for 문도 가능함.\n",
    "ds[0] ,ds[1] , ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ef067a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensors'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds)) & {'tensors'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5084ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38c41614-ece3-4067-8092-4ff8720279bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e125dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a,_b=ds.tensors \n",
    "b_numpy = _b.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "745784fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(b_numpy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5f64a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(ds.tensors)) & {'__getitem__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be51d15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __getitem__ 이 있으면 List 처럼 사용가능하기도 하고\n",
    "_aa,_bb= ds.tensors \n",
    "_aa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "784356d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(_aa)) & {'__getitem__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e62992bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "0.0\n",
      "[1.0]\n",
      "1.0\n",
      "[2.0]\n",
      "2.0\n",
      "[3.0]\n",
      "3.0\n",
      "[4.0]\n",
      "4.0\n",
      "[5.0]\n",
      "5.0\n",
      "[6.0]\n",
      "6.0\n",
      "[7.0]\n",
      "7.0\n",
      "[8.0]\n",
      "8.0\n",
      "[9.0]\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# __getitem__ 이 있으면 for문에서  사용가능하기도 한다.\n",
    "for i in _aa:\n",
    "    print(i.tolist())\n",
    "    print(i[0].tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d93c4c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.]), tensor([1.])),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0],(x,y)[0] # (x,y) 튜플자체는 아님.. 인덱싱이 다르게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a06af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf03c4-e2c8-412b-a5b2-9a747a47733e",
   "metadata": {
    "tags": []
   },
   "source": [
    "::: {.callout-note}\n",
    "여기서 제가 `__iter__` 가 숨겨져 있는 오브젝트일 경우만 `for`문이 동작한다고 설명 했는데요, `__getitem__`이 있는 경우도 동작한다고 합니다. **제가 잘못 알고 있었어요. 혼란을 드려 죄송합니다**. \n",
    "\n",
    "- 그래도 `dl`은 for 를 돌리기위해서 만든 오브젝트라는 설명은 맞는 설명입니다. \n",
    "- `ds`역시 독특한 방식의 인덱싱을 지원하도록 한 오브젝트라는 설명도 맞는 설명입니다. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9418f1f-cdab-45c6-a4b1-d8203cedcb2c",
   "metadata": {},
   "source": [
    "`-` dl: 섭스크립터블하지 않지만 이터러블함 \n",
    "- __getitem__ 이 없어서 섭스크립터블하지 않지만\n",
    "- __iter__ 이 있어서 이터러블하긴 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5613856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05b05710-3ee4-4f65-b6cf-d0702c884166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## dataset\n",
    "# ds=torch.utils.data.TensorDataset(x,y)\n",
    "\n",
    "# dataloader\n",
    "dl=torch.utils.data.DataLoader(dataset=ds,batch_size=3)\n",
    "# dl = torch.utils.data.DataLoader(dataset=ds, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "#set(dir(dl)) & {'__iter__'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eaabb58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(dl)) & {'__getitem__'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224a765",
   "metadata": {},
   "source": [
    "__iter__ 가 있으니 for 문을 돌릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "daf49862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__iter__'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dir(dl)) & {'__iter__'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02bb6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])]\n",
      "[tensor([[3.],\n",
      "        [4.],\n",
      "        [5.]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])]\n",
      "[tensor([[6.],\n",
      "        [7.],\n",
      "        [8.]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])]\n",
      "[tensor([[9.]]), tensor([[0.]])]\n"
     ]
    }
   ],
   "source": [
    "for i in dl:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66b824b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    # print(f\"x_batch:{xi} \\t y_batch:{yi}\")\n",
    "    print(type(xi))\n",
    "    print(type(yi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cedd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]]) \t y_batch:tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "x_batch:tensor([[3.],\n",
      "        [4.],\n",
      "        [5.]]) \t y_batch:tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "x_batch:tensor([[6.],\n",
      "        [7.],\n",
      "        [8.]]) \t y_batch:tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "x_batch:tensor([[9.]]) \t y_batch:tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f\"x_batch:{xi} \\t y_batch:{yi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07cf4f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:[[0.0], [1.0], [2.0]] \t y_batch:[[1.0], [1.0], [1.0]]\n",
      "x_batch:[[3.0], [4.0], [5.0]] \t y_batch:[[1.0], [1.0], [0.0]]\n",
      "x_batch:[[6.0], [7.0], [8.0]] \t y_batch:[[0.0], [0.0], [0.0]]\n",
      "x_batch:[[9.0]] \t y_batch:[[0.0]]\n"
     ]
    }
   ],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f\"x_batch:{xi.tolist()} \\t y_batch:{yi.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ad0071f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04bfbd",
   "metadata": {},
   "source": [
    "`-` 마지막관측치는 뭔데 단독으로 업데이트하냐?? 편향되는 위험이 있는거 아냐?? --> shuffle True 같이 자잘한 옵션도 있음.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35537fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch=[[9.0], [2.0], [6.0]] \t y_batch=[[0.0], [1.0], [0.0]]\n",
      "x_batch=[[0.0], [8.0], [4.0]] \t y_batch=[[1.0], [0.0], [1.0]]\n",
      "x_batch=[[1.0], [7.0], [5.0]] \t y_batch=[[1.0], [0.0], [0.0]]\n",
      "x_batch=[[3.0]] \t y_batch=[[1.0]]\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True)\n",
    "for xi,yi in dl:\n",
    "    print(f'x_batch={xi.tolist()} \\t y_batch={yi.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76043e9d",
   "metadata": {},
   "source": [
    "`-` DataLoader의 주요 파라미터\n",
    "- dataset: 로드할 데이터셋. Dataset 클래스의 인스턴스\n",
    "- batch_size: 배치 크기. 한 번에 로드할 데이터 포인트의 수\n",
    "- shuffle: 매 에폭(epoch)마다 데이터를 섞을지 여부.\n",
    "    - epoch: 전체 데이터셋이 학습 알고리즘을 통과하는 한 번의 완전한 사이클을 의미\n",
    "    - 일반적으로 학습 데이터셋에는 True를, 검증/테스트 데이터셋에는 False를 사용\n",
    "- num_workers: 데이터 로딩에 사용할 서브 프로세스의 수. 더 빠른 데이터 로딩을 위해 0보다 큰 값을 설정할 수 있습니다.\n",
    "    - 이 값이 0이면, 메인 프로세스가 모든 데이터 로딩 작업을 수행합니다.\n",
    "- collate_fn: 배치 데이터를 어떻게 처리할지 정의하는 함수입니다. 사용자 정의 데이터 처리를 위해 커스터마이즈할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32381ec5-1ac2-4621-8314-de10e2088c27",
   "metadata": {},
   "source": [
    "## F. ds, dl을 이용한 MNIST 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3ec7c",
   "metadata": {},
   "source": [
    "`-` 목표: 확률적경사하강법과 그냥 경사하강법의 성능을 \"동일 반복횟수\"로 비교해보자.\n",
    "\n",
    "- batch_size = 2048로 설정할것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6b0dd-1223-440b-8a6d-1a21b68f34bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 그냥 경사하강법 -- 미니배치 안쓰는 학습, 우리가 맨날하는 그거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f5a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c09423ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/myuser/.fastai/data/mnist_png')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3866735",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining/0/1775.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/_ops.py:921\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m         )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "torchvision.io.read_image(path/'training/0/1775.png').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27490aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda\n",
      " - defaults\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - fastai\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torchvisio\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda\n",
      "  - defaults\n",
      "  - https://conda.anaconda.org/conda-forge/linux-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://conda.anaconda.org/nvidia/linux-64\n",
      "  - https://conda.anaconda.org/fastai/noarch\n",
      "  - https://conda.anaconda.org/pytorch/linux-64\n",
      "  - https://conda.anaconda.org/pytorch/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvisio -c conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7f582d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining/0/1775.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m),cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/_ops.py:921\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m         )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "plt.imshow(torchvision.io.read_image(path/'training/0/1775.png').reshape(28,28),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b361a920",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aaa\u001b[38;5;241m=\u001b[39m([torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(\u001b[38;5;28mstr\u001b[39m(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n\u001b[1;32m      2\u001b[0m aaa[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape , aaa[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \n",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aaa\u001b[38;5;241m=\u001b[39m([\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n\u001b[1;32m      2\u001b[0m aaa[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape , aaa[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/_ops.py:921\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m         )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "aaa=([torchvision.io.read_image(str(i)) for i in (path/'training/3').ls()])\n",
    "aaa[0].shape , aaa[1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4f9efb16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43maaa\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
     ]
    }
   ],
   "source": [
    "len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f92d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0e853019-48ca-4bb6-8b4d-f02e248cf9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m fastai\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexternal\u001b[38;5;241m.\u001b[39muntar_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# X0 train 데이터셋\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(\u001b[38;5;28mstr\u001b[39m(fname)) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/0\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# X1 train 데이터셋\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(\u001b[38;5;28mstr\u001b[39m(fname)) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n",
      "Cell \u001b[0;32mIn[117], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m fastai\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexternal\u001b[38;5;241m.\u001b[39muntar_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# X0 train 데이터셋\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/0\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# X1 train 데이터셋\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(\u001b[38;5;28mstr\u001b[39m(fname)) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m (path\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mls()])\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/dl2024/lib/python3.8/site-packages/torch/_ops.py:921\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m         )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "# X0 train 데이터셋\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "# X1 train 데이터셋\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "print(f'X0.shape={X0.shape} \\t X1.shape={X1.shape}')\n",
    "# X0 와 X1 을 합친 X train 데이터셋\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255    # int 에서 float로 , shape 는 12665,784\n",
    "print(f'X.shape={X.shape}')\n",
    "\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1) # shape 는 12665,1\n",
    "print(f'y.shape={y.shape}')\n",
    "\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    # 중간 노드 32 \n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    # 최종 출력노드 1\n",
    "    torch.nn.Linear(32,1),\n",
    "    # 이진분류 Sigmoid\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "# 이진분류 Loss\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())        # Stochastic Gradient Descent 를 테스트 중이니 Adam은 사용하지 말자. 여기선 batch=data_size_all\n",
    "## Step3: fit  \n",
    "for epoc in range(700):     # 여기선 epoch 을 700 으로\n",
    "    # step1 \n",
    "    yhat = net(X)\n",
    "    # step2 \n",
    "    loss = loss_fn(yhat,y)\n",
    "    # step3     \n",
    "    loss.backward()\n",
    "    # step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "## Step4: Predict \n",
    "((yhat > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[0].reshape(-1,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[0]).reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747665dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X0[0].reshape(28,28),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3206b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape    # int 에서 float로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea717ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl 다시 복습\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=3)\n",
    "for xi,yi in dl:\n",
    "    print(f'x_batch={xi.tolist()} \\t y_batch={yi.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b65fc1-7d39-4721-aef8-29ae3daae42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` \"확률적\"경사하강법 -- 미니배치 쓰는 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd68fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n",
    "\n",
    "# batch 를 사용하기위해 Dataset과 Dataloader 생성\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "# batch size = 2048\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=2048)\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())        # Stochastic Gradient Descent 를 테스트 중이니 Adam은 사용하지 말자. 여기선 batch=2048\n",
    "# ## Step3: fit  \n",
    "for epoc in range(100):     # 여기선 epoch 을 100 으로\n",
    "    for xi,yi in dl:        # len(dl) == 7    , 데이터 수 / batch_size 하면 6.xx 나오기때문.\n",
    "        # step1 \n",
    "        #yihat = net(xi)\n",
    "        # step2 \n",
    "        loss = loss_fn(net(xi),yi)\n",
    "        # step3     \n",
    "        loss.backward()\n",
    "        # step4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "# ## Step4: Predict \n",
    "((net(X) > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a670a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66933d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi,yi in dl:\n",
    "    print(f'x_batch={len(xi.tolist())} \\t y_batch={len(yi.tolist())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa6914-d986-401d-a071-53534475d15f",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` GPU를 활용하는 \"확률적\"경사하강법 -- 실제적으로는 이게 최종알고리즘 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb360dea-f518-48d4-81c3-095a80f235b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n",
    "\n",
    "# batch 를 사용하기위해 Dataset과 Dataloader 생성\n",
    "ds = torch.utils.data.TensorDataset(X,y)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=2048)\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(1)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(\"cuda:0\")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.SGD(net.parameters())\n",
    "## Step3: fit  \n",
    "for epoc in range(100):     # 여기선 epoch 을 100 으로\n",
    "    for xi,yi in dl:        \n",
    "        # step1 \n",
    "        # step2 \n",
    "        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n",
    "        # step3     \n",
    "        loss.backward()\n",
    "        # step4 \n",
    "        optimizr.step()\n",
    "        optimizr.zero_grad()\n",
    "# ## Step4: Predict\n",
    "net.to(\"cpu\")\n",
    "((net(X) > 0.5)*1.0 ==  y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "type([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dca412",
   "metadata": {},
   "outputs": [],
   "source": [
    "2048*6 + 377"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581bfc7-1ddc-4c08-b48e-c7e3d935f9bf",
   "metadata": {},
   "source": [
    "# 5. 다중클래스 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a0408-7def-4176-a521-e05ba7a4da52",
   "metadata": {},
   "source": [
    "## A. 결론 (그냥 외우세요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb7b3b-acc9-4924-a07e-6c8bd38010fc",
   "metadata": {},
   "source": [
    "`-` 2개의 class를 구분하는 문제가 아니라 $k$개의 class를 구분해야 한다면? \n",
    "\n",
    "***일반적인 개념*** \n",
    "\n",
    "- 손실함수: BCE loss $\\to$ Cross Entropy loss \n",
    "- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n",
    "- **마지막층의 활성화: sig $\\to$ softmax**\n",
    "\n",
    "***파이토치 한정*** \n",
    "\n",
    "- **y의형태: (n,) vector + int형 // (n,k) one-hot encoded matrix + float형**\n",
    "- 손실함수: torch.nn.BCEWithLogitsLoss,  $\\to$ torch.nn.CrossEntropyLoss\n",
    "- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n",
    "- **마지막층의 활성화: None $\\to$ None (손실함수에 이미 마지막층의 활성화가 포함)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457d0d5-fad0-4406-93b8-09de9ee15abf",
   "metadata": {},
   "source": [
    "## B. 실습: 3개의 클래스를 구분 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d267516-87b1-44df-b52d-857fe4f415b2",
   "metadata": {},
   "source": [
    "`-` 정리된 코드1: 통계잘하는데 파이토치 못쓰는 사람의 코드\n",
    "- **y의형태 : one-hot encoded matrix + float형**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d19628-cdc0-40ed-aebc-2f5ff95e0d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)/255\n",
    "y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))).float()\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,3),\n",
    "#    torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: 적합 \n",
    "for epoc in range(100):\n",
    "    ## step1 \n",
    "    netout = net(X)\n",
    "    ## step2 \n",
    "    loss = loss_fn(netout,y)\n",
    "    ## step3 \n",
    "    loss.backward()\n",
    "    ## step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "    \n",
    "## Step4: 적합 (혹은 적합결과확인)\n",
    "(netout.argmax(axis=1) == y.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[(str(fname)) for fname in (path/'training/0').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(net(X).argmax(axis=1) == y.argmax(axis=1)).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a28f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[1]), plt.imshow(X[0].reshape(28,28)), y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X[1]), plt.imshow(X[-1].reshape(28,28)), y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998db61-9083-4921-84b4-86ed42cc331e",
   "metadata": {},
   "source": [
    "`-` 정리된 코드2: 파이토치를 잘하는 사람의 코드 :\n",
    "- **y의형태: (n,) vector + int형**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8953ee-bd7a-489c-be88-0946917e949e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\n",
    "X = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)/255\n",
    "#y = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))).float()\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))    # int 타입\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,3),\n",
    "#    torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: 적합 \n",
    "for epoc in range(100):\n",
    "    ## step1 \n",
    "    netout = net(X)\n",
    "    ## step2 \n",
    "    loss = loss_fn(netout,y)\n",
    "    ## step3 \n",
    "    loss.backward()\n",
    "    ## step4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()\n",
    "## Step4: 적합 (혹은 적합결과확인)    \n",
    "(netout.argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype , y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28df1d-249c-4ad0-b256-3df24edf3d76",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 완전같은코드임 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c26dc-146e-4d02-893f-de4b8d8f4b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ca91d-babe-4dca-99a6-ff7210f5710c",
   "metadata": {},
   "source": [
    "`-` 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356aac1-893d-485c-98c6-42c800d6cce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c8eff-8c9f-4460-b08d-2bc11812216c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60565f1-c18c-4045-94a6-d10666527408",
   "metadata": {},
   "source": [
    "`-` 수식 \n",
    "\n",
    "- $\\text{sig}(u)=\\frac{e^u}{1+e^u}$\n",
    "- $\\text{softmax}({\\boldsymbol u})=\\text{softmax}([u_1,u_2,\\dots,u_k])=\\big[ \\frac{e^{u_1}}{e^{u_1}+\\dots e^{u_k}},\\dots,\\frac{e^{u_k}}{e^{u_1}+\\dots e^{u_k}}\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75eba0b-deb7-4a3f-b897-1c4ebe6d37bb",
   "metadata": {},
   "source": [
    "`-` torch.nn.Softmax() 손계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4313a62-900c-422b-ae64-24e846f5cf6c",
   "metadata": {},
   "source": [
    "(예시1) -- 잘못계산 : dim 이 잘못됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ee620-908d-4a66-9c2e-314f7ecfadce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c35f4f-9b07-4de6-b7bb-6b8a09b9490c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터의 개수 : 5개\n",
    "netout = torch.tensor([[-2.0,-2.0,0.0],\n",
    "                        [3.14,3.14,3.14],\n",
    "                        [0.0,0.0,2.0],\n",
    "                        [2.0,2.0,4.0],\n",
    "                        [0.0,0.0,0.0]])\n",
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a43be-188d-4304-a8b5-ba766136bee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax(netout) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a934bda-8458-44f5-9f58-f1d85d19a52a",
   "metadata": {},
   "source": [
    "(예시2) -- 이게 맞게 계산되는 것임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3e4f5-2796-4399-b223-d910ea182e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafa544-0d9e-4977-8bb7-da1c945e0bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e3435-eefa-49d9-82a8-13cb40571046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax(netout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff382de5-9054-414e-92dc-91714810b9d6",
   "metadata": {},
   "source": [
    "(예시3) -- 차원을 명시안하면 맞게 계산해주고 경고 줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c8926-7e1e-4994-a04c-4ca3ea3f14a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafa70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59a46d-e2c1-4247-aae3-ff6171bb1d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d3bb0-49d1-4bc4-9ffb-eaadbd1228bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax(netout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb6df4-b9de-48c5-a0d2-65373288f3df",
   "metadata": {},
   "source": [
    "(예시4) -- 진짜 손계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c623f-eab4-4324-a079-16252ece7706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551d3b6-a6c4-4d6a-a402-8be14fe1c0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.exp(netout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b5993-06b0-469b-a8aa-dfb6ef26a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e12995-da3a-4c23-8146-301dbaec52dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.exp(netout[1])/torch.exp(netout[1]).sum() # 두번째 obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(netout)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758fd94-753e-4f54-b4b2-b680d5400bee",
   "metadata": {},
   "source": [
    "## D. CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19777a84-daf8-4868-a2d7-d802bc4b37a2",
   "metadata": {},
   "source": [
    "`-` 수식 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25900c98-21f2-47dd-818f-53b06dde35fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "***`# 2개의 카테고리`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc931b-5238-41a4-b2f7-f0c76c2a6dfe",
   "metadata": {},
   "source": [
    "`-` 예제1: BCELoss vs BCEWithLogisticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd1529-6526-4a8f-85a2-e068d09f025b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([0,0,1]).reshape(-1,1).float()\n",
    "netout = torch.tensor([-1, 0, 1]).reshape(-1,1).float()\n",
    "y,netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb02cb-1749-4391-8446-fdec0734f6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법1: 공식암기\n",
    "sig = torch.nn.Sigmoid()\n",
    "yhat = sig(netout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af91fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "- torch.sum(torch.log(yhat)*y + torch.log(1-yhat)*(1-y))/3      # BCE Loss 손으로 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat , yhat.sum()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a04437",
   "metadata": {},
   "outputs": [],
   "source": [
    "y , y.sum()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736829af-f7ab-440d-82b5-768c60bea69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법2: torch.nn.BCELoss() 이용\n",
    "sig = torch.nn.Sigmoid()\n",
    "yhat = sig(netout)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "loss_fn(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f971b-a33a-46b5-9c99-38e4e9638ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법3: torch.nn.BCEWithLogitsLoss() 이용\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4a3cf-7911-4b4a-b833-50541329c0b9",
   "metadata": {},
   "source": [
    "`-` 예제2: BCEWithLogisticLoss vs CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6ff61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.concat([sig(netout),1-sig(netout)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ce802-ab28-4f1b-b061-462feb61baf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터의 개수 : 3개\n",
    "netout = torch.tensor([[3,2],[2,2],[5,6]]).float()\n",
    "y = torch.tensor([[1,0],[1,0],[0,1]]).float()\n",
    "y,netout #,netout[:,[1]]-netout[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f67805",
   "metadata": {},
   "outputs": [],
   "source": [
    "netout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb10b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax(netout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfeba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(netout)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc88737-fd06-450d-ad23-0084d1924e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법1: 공식암기\n",
    "-torch.sum(torch.log(softmax(netout))*y)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b1412-3913-4ca1-87e7-829c2951658d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법2: torch.nn.CrossEntropyLoss() 이용 + y는 one-hot으로 정리\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090abc37-f55c-42ae-8116-4e34097f2b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 계산방법3: torch.nn.CrossEntropyLoss() 이용 + y는 0,1 로 정리\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc824652-98e3-48b4-9025-c874e37c6fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b9992-40a2-4922-9803-69d6ea0e466f",
   "metadata": {},
   "source": [
    "***`# 3개의 카테고리, 데이터의 개수는 5개`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b72d7-3caa-43be-83fa-c920eafb9654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([2,1,2,2,0])\n",
    "y_onehot = torch.nn.functional.one_hot(y)\n",
    "netout = torch.tensor(\n",
    "    [[-2.0000, -2.0000,  0.0000],\n",
    "     [ 3.1400,  3.1400,  3.1400],\n",
    "     [ 0.0000,  0.0000,  2.0000],\n",
    "     [ 2.0000,  2.0000,  4.0000],\n",
    "     [ 0.0000,  0.0000,  0.0000]]\n",
    ")\n",
    "y,y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01854de7-1bf6-424e-8365-d7a99e74994f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 방법1 -- 추천X\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y_onehot.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ac430-21df-4477-90ed-f875e53b9316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 방법2 -- 추천O\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(netout,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e89404",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386860b6-e2ce-49e9-8bda-dccab3f625ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 방법3 -- 공식.. (이걸 쓰는사람은 없겠지?)\n",
    "softmax = torch.nn.Softmax() \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "- torch.sum(torch.log(softmax(netout))*y_onehot)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dce89e-21ad-42b2-8a70-b35d6e10c4dc",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e05f3e-a7f1-4b31-b90a-17b4812540c2",
   "metadata": {},
   "source": [
    "`-` 계산하는 공식을 아는것도 중요한데 torch.nn.CrossEntropyLoss() 에는 softmax 활성화함수가 이미 포함되어 있다는 것을 확인하는 것이 더 중요함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e564a21-4c20-4ef9-ac3b-2bb66a72663e",
   "metadata": {},
   "source": [
    "`-` torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57818858-a52b-47dd-a8e9-2e84312cd8dd",
   "metadata": {},
   "source": [
    "## E. Minor Topic: 이진분류와 CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96ecda-1fbb-48f7-b5de-8cede28b9b9c",
   "metadata": {},
   "source": [
    "`-` 2개의 클래스일경우에도 CrossEntropy를 쓸 수 있지 않을까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebe3ec-d6b4-4f39-8ba7-84185ac104f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1]).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1))\n",
    "## Step2: 학습가능한 오브젝트 생성\n",
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(784,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,2),\n",
    "    #torch.nn.Softmax()\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "## Step3: fit  \n",
    "for epoc in range(70): \n",
    "    ## 1 \n",
    "    ## 2 \n",
    "    loss= loss_fn(net(X),y) \n",
    "    ## 3 \n",
    "    loss.backward()\n",
    "    ## 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad() \n",
    "## Step4: Predict \n",
    "softmax = torch.nn.Softmax()\n",
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd7a09-5563-4565-aecc-68dd25deb87e",
   "metadata": {},
   "source": [
    "`-` 이진분류문제 = \"y=0 or y=1\" 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d270f-de1a-4653-98bb-ee87cde06921",
   "metadata": {},
   "source": [
    "`-` softmax, sigmoid\n",
    "\n",
    "- softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. \n",
    "- sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a26d3-6f3f-40e2-be5a-fdbc31ca3615",
   "metadata": {},
   "source": [
    "`-` 그런데 \"실패확률=1-성공확률\" 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9103832-a80d-4f2e-9f06-f272ca11fc2b",
   "metadata": {},
   "source": [
    "`-` 즉 아래는 같은 표현력을 가진 모형이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1f316-19fe-4843-9540-58001bc536a3",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/guebin/DL2024/cdbdf23589efc2198260ab9c749f1757f67a128d/posts/05wk-1-fig2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a371-b198-4121-b086-0e24b995701c",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/guebin/DL2024/cdbdf23589efc2198260ab9c749f1757f67a128d/posts/05wk-1-fig1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30736d-9e20-48e9-a771-f9939899864a",
   "metadata": {},
   "source": [
    "`-` 둘은 같은 표현력을 가진 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. $\\to$ sigmoid를 사용하는 모형이 비용은 싸고 효과는 동일하다는 말 $\\to$ 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. \n",
    "\n",
    "- softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac81d5-7949-44a8-87cd-ab7cb11e540a",
   "metadata": {},
   "source": [
    "## F. 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029e2bd-7e58-4446-987b-6b22c2844762",
   "metadata": {},
   "source": [
    "`-` 결론 \n",
    "\n",
    "1. 소프트맥스는 시그모이드의 확장이다. \n",
    "2. 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc630f4-5ac1-490c-82da-c5e07e0080b9",
   "metadata": {},
   "source": [
    "`-` 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (그냥 좀 비효율적인 느낌이 드는 것 뿐임. 흑백이미지를 칼라잉크로 출력하는 느낌) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f790de3-194e-4413-80f9-f9ba4430583c",
   "metadata": {},
   "source": [
    "***참고***\n",
    "\n",
    "|$y$|분포가정|마지막층의 활성화함수|손실함수|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|3.45, 4.43, ... (연속형) |정규분포|None (or Identity)|MSE|\n",
    "|0 or 1|이항분포 with $n=1$ (=베르누이) |Sigmoid| BCE|\n",
    "|[0,0,1], [0,1,0], [1,0,0]| 다항분포 with $n=1$|Softmax| Cross Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f776f61-384f-4d7e-b67d-9db98b8da39f",
   "metadata": {},
   "source": [
    "# 6. HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90580d98-35f1-4c48-89bb-7e1b55519091",
   "metadata": {
    "tags": []
   },
   "source": [
    "아래와 같은 자료가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46503051-c102-43ea-853a-9d369fd6d82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Step1: 데이터준비 \n",
    "path = fastai.data.external.untar_data('https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz')\n",
    "X0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\n",
    "X1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\n",
    "X = torch.concat([X0,X1]).reshape(-1,1*28*28)/255\n",
    "y = torch.tensor([0]*len(X0) + [1]*len(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618ef1e-be5c-473f-8ea4-034cca59ef61",
   "metadata": {},
   "source": [
    "`(1)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- 손실함수를 `torch.nn.BCELoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09610a12-aeae-4c1e-b6aa-bff9dcb7f83f",
   "metadata": {},
   "source": [
    "`(2)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- 손실함수를 `torch.nn.BCEWithLogitsLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c45eb-8c7a-41b1-803b-d5e9b7d6462f",
   "metadata": {},
   "source": [
    "`(3)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- y를 one_hot 인코딩하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff23187-feaa-4c16-afd6-e0b993e9b992",
   "metadata": {
    "tags": []
   },
   "source": [
    "**hint** 원핫인코딩을 위해 아래의 함수를 사용하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229bc2-780f-46da-853a-2adf85dde805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y, torch.nn.functional.one_hot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b85d1-a3ba-4e24-983a-c57d2682a570",
   "metadata": {},
   "source": [
    "`(4)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- epoch = 325로 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a90f5b-514b-4df3-9683-62005cdbc216",
   "metadata": {},
   "source": [
    "`(5)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- batch_size = 1024 로 설정한뒤 mini_batch를 이용한 학습을 하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라.  \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- 총 iteration 수가 325가 되도록 적절하게 epoch 을 설정하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fae3bc-20b7-4674-977e-b8a3c143a6de",
   "metadata": {},
   "source": [
    "`(6)` 세부사항에 맞추어 위의 자료를 학습하고, accuracy를 구하라. \n",
    "\n",
    "- 네트워크는 1개의 은닉층을 가지도록 하고, 은닉노드수는 32개로 설정하라. 은닉층의 활성화함수는 ReLU로 설정하라. \n",
    "- batch_size = 512 로 설정한뒤 mini_batch를 이용한 학습을 하라. \n",
    "- y를 (one-hot 인코딩 하지 않고) lenght-$n$인 벡터로 유지하라. \n",
    "- 손실함수를 `torch.nn.CrossEntropyLoss()`로 설정하라.\n",
    "- 총 iteration 수가 325가 되도록 적절하게 epoch 을 설정하라. \n",
    "- GPU를 활용하여 학습하라."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
