{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c3a1f547-5d0b-43e7-8761-4166fa5999e2",
   "metadata": {
    "id": "87b5cded-346b-4915-acf5-b5ec93a5207d"
   },
   "source": [
    "---\n",
    "title: \"06wk-1: 합성공신경망\"\n",
    "author: \"최규빈\"\n",
    "date: \"04/03/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48a8d4-700c-4e06-96c5-941d675903f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c935256-3a4c-4cd1-97b0-aa12467c8bd0",
   "metadata": {},
   "source": [
    "# A1. DNN, ANN, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06d6f4-daa0-42d3-bea7-ee6f5bf642e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` DNN 은 깊은신경망, ANN 은 인공신경망, MLP 는 다층퍼셉트론이라 번역된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10226b53-836b-4333-a979-47bb77186457",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 하지만 MLP, DNN 이라 볼 수는 없다. \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e68946-a64e-44d7-bb4a-9501172c361f",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 2개 있으므로 MLP라고 볼 수 있다. DNN 이라 보기는 애매하다. (그래서 이걸 얕은신경망(shallow network)이라고 표현하기도 합니다)\n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3284db7-6c44-4038-8af8-4950e876b08d",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 7개 있으므로 MLP라고 볼 수 있다. 이 정도면 깊어보이니까 DNN 이라 주장할 수 있어보인다. \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),\n",
    "    torch.nn.Sigmoid(),    \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88800f5a-e496-42ba-8dd0-774ddb32bf0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 아래의 네트워크는 ANN이라 볼 수 있다. 또한 레이어가 3개 있으므로 MLP라고 볼 수 있다. 이건 DNN이라고 봐야하나? 깊다기 보다는 넓은 신경망인데... \n",
    "\n",
    "```Python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1),\n",
    "    torch.nn.Sigmoid(),    \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af24913-1c5b-4b87-bf02-a9a938892c8e",
   "metadata": {},
   "source": [
    "`-` 야매개념: 요즘은 거의 ANN $\\approx$ MLP $\\approx$ DNN 의 느낌으로 이해해도 무방함\n",
    "\n",
    "- 어지간한 모형은 다 ANN이라 우길 수 있다. 회귀분석도, 로지스틱분석도 마음먹으면 ANN으로 우길 수 있다. 그래서 \"ANN을 썼다\"라는건 엄청 모호한 말이다. 이런 이유로 사람들은 거의 MLP를 쓴 경우에 ANN을 썼다고 하고, 회귀모형을 쓴 경우에는 굳이 ANN을 썼다고 표현하지 않는다. \n",
    "- MLP과 DNN은 구분이 모호하다. 하나이상의 은닉층만 포함하고 있으면 MLP라고 부를 수 있다. 적은 노드수를 유지하면서 은닉층을 여러개 쓰면 깊은 신경망이라고 하고, 많은 노드를 사용하면서 은닉층을 얇게, 그리고 노드를 많이 쓰면 넓은신경망이라고 한다. 노드수와 관계없이 층이 얇은 경우는 얕은신경망이라고 한다.^[저는 이 표현 너무 싫어해요] 즉 MLP의 모양에 따라서 \"깊은신경망\", \"얕은신경망\", \"넓은신경망\" 등의 용어를 사용한다. \n",
    "- 일반적으로 은닉층이 1개있으면 얕은신경망, 2개 이상이면 깊은신경망이라고 부른다고 합의되어있다. (은닉층이 2층까지 얕은신경망이라고 부르는 사람도 존재함) 얼마나 많은 노드부터 넓은신경망이라고 부르는지는 (제가 아는 한) 합의된바가 없다. \n",
    "얼마나 깊을때 DNN으로 부를지 명확한 합의가 되어있지 않다. (3층-MLP부터 DNN으로 부르는 방식이 지지를 얻는듯. 그렇지만 4층-MLP 부터 DNN으로 부르는 사람도 존재함.)\n",
    "- MLP의 정의가 가장 깔끔하다고 생각하지만 요즘 잘 쓰는 용어는 아니다. (MLP의 논문은 너무 예전임. 층을 세는것도 다름)\n",
    "- **제 결론**: 따지고 보자면 DNN $\\subset$ MLP $\\subset$ ANN 이다. 그렇지만 MLP이지만 DNN은 아닌 네트워크를 지칭한다든가, ANN 이지만 MLP는 아닌 네트워크를 지칭하는 일은 흔하지 않으며, 지칭하더라도 부연설명을 친절하게 해준다. 따라서 부연설명 없이 ANN, MLP, DNN 을 지칭한다면 거의 DNN을 의미한다고 봐도 무방하다. 즉 ANN $\\approx$ MLP $\\approx$ DNN 라고 보면 된다. (엄밀하게는 틀린개념이죠) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5a20e-fa85-4f97-803e-8c28bd4810cf",
   "metadata": {},
   "source": [
    "`-` 해석..\n",
    "\n",
    "- <https://www.ibm.com/kr-ko/topics/neural-networks>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
