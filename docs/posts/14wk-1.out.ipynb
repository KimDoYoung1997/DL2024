{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-1: 강화학습 (2) – 4x4 Grid World\n",
        "\n",
        "최규빈  \n",
        "2024-06-03\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/14wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "a8fde88f-05c1-4c02-a9f3-4f219f0c5032"
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zHvVuJ92xfdypwHwDFgg8k&si=iI4IhthblTsJTmIv >}}"
      ],
      "id": "1dfb3796-7623-4061-bc81-82305c2d4d59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "5ba848c4-61ac-4df3-ba55-9575006f887a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "5b1cfd48-d338-4ddd-8b59-f8ab593ebda9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Game2: 4x4 Grid World\n",
        "\n",
        "`-` 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에\n",
        "도달하도록 학습하는 방법\n",
        "\n",
        "`-` GridWorld에서 사용되는 주요변수\n",
        "\n",
        "1.  **`State`**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **`Action`**: 에이전트는 현재상태에서 다음상태로 이동하기 위해\n",
        "    상,하,좌,우 중 하나의 행동을 취할 수 있음.\n",
        "3.  **`Reward`**: 에이전트가 현재상태에서 특정 action을 하면 얻어지는\n",
        "    보상.\n",
        "4.  **`Terminated`**: 하나의 에피소드가 종료되었음을 나타내는 상태.\n",
        "\n",
        "# 4. 예비학습\n",
        "\n",
        "## A. `gym.spaces`\n",
        "\n",
        "`-` 예시1"
      ],
      "id": "a958da23-1a0a-484b-977c-f6b6ed38b17b"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "state_space = gym.spaces.Discrete(4) \n",
        "state_space "
      ],
      "id": "86af6553-1d04-4f6a-b8ea-e91cb779e90e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "[state_space.sample() for _ in range(5)]"
      ],
      "id": "365100ab-8550-4ceb-a79c-76b0537a34b3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "0 in state_space"
      ],
      "id": "0a6049ab-0309-4f6b-a053-1aa56390c1c2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "4 in state_space"
      ],
      "id": "cc93a879-f95e-4108-a349-d1664634205f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 예시2"
      ],
      "id": "95839672-4fe8-4970-a511-d59827879851"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "state_space"
      ],
      "id": "19c4f90d-33ed-4a6f-bf36-070ac192d890"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "[state_space.sample() for _ in range(5)]"
      ],
      "id": "3f815cbc-036a-4497-965a-2727842f7ec5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,3]) in state_space"
      ],
      "id": "04da0368-8f85-43f2-adb1-eb5dd8465806"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,4]) in state_space"
      ],
      "id": "13f530d6-d1a7-40cb-89ec-b97c27b8339c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 시각화"
      ],
      "id": "63167a73-5e46-4fab-aab8-e523e71dc500"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "    def update(t):\n",
        "        if states[t] in state_space:\n",
        "            s1,s2 = states[t]\n",
        "            states[t] = [s2,s1]\n",
        "            sc.set_offsets(states[t])\n",
        "        else:\n",
        "            s1,s2 = states[t]\n",
        "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
        "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
        "            states[t] = [s2,s1]       \n",
        "            sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "0b3ec243-70a1-46e7-bd95-2bb4a5338898"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "show([[0,0],\n",
        "      [1,0],\n",
        "      [2,0],\n",
        "      [3,0],\n",
        "      [4,0]])"
      ],
      "id": "5379f2af-62d5-43ff-b16f-4689e7a53fe5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Env 클래스 구현"
      ],
      "id": "da7b066b-fefd-4ce4-b7a5-7f2a70e61739"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action = 3\n",
        "current_state = np.array([1,1])"
      ],
      "id": "aedc105c-7a89-46c1-8f5b-100d938085a5"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_to_direction = {\n",
        "    0 : np.array([1, 0]), # row+, down\n",
        "    1 : np.array([0, 1]), # col+, right\n",
        "    2 : np.array([-1 ,0]), # row-, up\n",
        "    3 : np.array([0, -1]) # col-, left\n",
        "}\n",
        "action_to_direction2 = {0: 'down', 1: 'right', 2: 'up', 3: 'left'} # 당장쓰진 않지만 하는김에 "
      ],
      "id": "1d0cef25-bd48-4bb1-bb2f-da817f162215"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "next_state = current_state + action_to_direction[action]\n",
        "next_state"
      ],
      "id": "e50e1ecd-765b-4764-9ac4-30c4234352e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` Class 구현: 아래와 같은 느낌의 클래스를 구현해보자."
      ],
      "id": "6be35885-53c9-4edb-b83a-cdf701d59529"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.action_space = gym.spaces.Discrete(4) \n",
        "        self._action_to_direction = { \n",
        "            0 : np.array([1, 0]), # x+ \n",
        "            1 : np.array([0, 1]), # y+ \n",
        "            2 : np.array([-1 ,0]), # x-\n",
        "            3 : np.array([0, -1]) # y- \n",
        "        }\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.state = self.state + direction\n",
        "        if self.state not in self.state_space: # 4x4 그리드 밖에 있는 경우\n",
        "            reward = -10 \n",
        "            terminated = True\n",
        "        elif np.array_equal(self.state, np.array([3,3])): # 목표지점에 도달할 경우 \n",
        "            reward = 100 \n",
        "            terminated = True\n",
        "        else: \n",
        "            reward = -1 \n",
        "            terminated = False         \n",
        "        return self.state, reward, terminated\n",
        "    def reset(self):\n",
        "        self.agent_action = None \n",
        "        self.state = np.array([0,0])        \n",
        "        return self.state "
      ],
      "id": "414b9e1a-c494-4c0f-abc3-16fe43d7ede5"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = GridWorld()"
      ],
      "id": "d90d8d11-e438-4528-ac96-107fe32c39be"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [] \n",
        "state = env.reset()\n",
        "states.append(state) \n",
        "for t in range(50):\n",
        "    action = env.action_space.sample() \n",
        "    state,reward,terminated = env.step(action)\n",
        "    states.append(state) \n",
        "    if terminated: break "
      ],
      "id": "f8bcc04d-d188-4c44-86bb-bf2394cd66d8"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "886e5543-619a-4488-a39f-77bc8a2fa254"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. `AgentRandom`\n",
        "\n",
        "## A. 에이전트 클래스 설계\n",
        "\n",
        "`-` 우리가 구현하고 싶은 기능\n",
        "\n",
        "-   `.act()`: 액션을 결정 –\\> 여기서는 그냥 랜덤액션\n",
        "-   `.save_experience()`: 데이터를 저장 –\\> 여기에 일단 초점을 맞추자\n",
        "-   `.learn()`: 데이터로에서 학습 –\\> 패스\n",
        "\n",
        "`-` 첫번째 시도"
      ],
      "id": "b40b215c-8c18-4376-9ab8-02f357649d93"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentRandom:\n",
        "    def __init__(self,env):\n",
        "        self.action_space = env.action_space\n",
        "        self.state_spcae = env.state_space \n",
        "        self.n_experiences = 0 \n",
        "        self.n_episodes = 0 \n",
        "        self.score = 0 \n",
        "        \n",
        "        # episode-wise info \n",
        "        self.scores = [] \n",
        "        self.playtimes = []\n",
        "\n",
        "        # time-wise info\n",
        "        self.current_state = None \n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.next_state = None         \n",
        "        self.terminated = None \n",
        "\n",
        "        # replay_buffer \n",
        "        self.actions = []\n",
        "        self.current_states = [] \n",
        "        self.rewards = []\n",
        "        self.next_states = [] \n",
        "        self.terminations = [] \n",
        "\n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample() \n",
        "\n",
        "    def save_experience(self):\n",
        "        self.actions.append(self.action) \n",
        "        self.current_states.append(self.current_state)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated) \n",
        "        self.n_experiences += 1 \n",
        "        self.score = self.score + self.reward \n",
        "        \n",
        "    def learn(self):\n",
        "        pass "
      ],
      "id": "e146d5e8-4c91-45ce-990f-f501db858092"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "6de610a7-6709-4ca4-a0f0-a1325e6367cf"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 1  Score: -11  Playtime: 2\n",
            "Epsiode: 2  Score: -10  Playtime: 1\n",
            "Epsiode: 3  Score: -11  Playtime: 2\n",
            "Epsiode: 4  Score: -10  Playtime: 1\n",
            "Epsiode: 5  Score: 87   Playtime: 14\n",
            "Epsiode: 6  Score: -22  Playtime: 13\n",
            "Epsiode: 7  Score: -10  Playtime: 1\n",
            "Epsiode: 8  Score: -10  Playtime: 1\n",
            "Epsiode: 9  Score: -11  Playtime: 2\n",
            "Epsiode: 10     Score: -10  Playtime: 1\n",
            "Epsiode: 11     Score: -10  Playtime: 1\n",
            "Epsiode: 12     Score: -10  Playtime: 1\n",
            "Epsiode: 13     Score: -13  Playtime: 4\n",
            "Epsiode: 14     Score: -10  Playtime: 1\n",
            "Epsiode: 15     Score: -17  Playtime: 8\n",
            "Epsiode: 16     Score: -10  Playtime: 1\n",
            "Epsiode: 17     Score: -19  Playtime: 10\n",
            "Epsiode: 18     Score: -10  Playtime: 1\n",
            "Epsiode: 19     Score: -13  Playtime: 4\n",
            "Epsiode: 20     Score: -12  Playtime: 3"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentRandom(env) \n",
        "for _ in range(20):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        # agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    #--#\n",
        "    print(\n",
        "        f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "        f\"Score: {agent.scores[-1]} \\t\"\n",
        "        f\"Playtime: {agent.playtimes[-1]}\"\n",
        "    )   "
      ],
      "id": "1d53c7dc-4e20-4e65-9d10-56b071baef9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용결과 시각화"
      ],
      "id": "e37f6150-eb37-4932-8726-784e0973b85d"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(agent.playtimes[:18])"
      ],
      "id": "39b306fe-1afb-4a96-9908-f931d1e96e8c"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(agent.playtimes[:19])"
      ],
      "id": "c93e1dd8-ceff-4789-8809-244f639a2d09"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[76:84]\n",
        "show(states)"
      ],
      "id": "e251a785-472d-45ce-9aa8-9e5086fdd5d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   우연히 잘맞춘 케이스\n",
        "\n",
        "# 7. `AgentGreedy`\n",
        "\n",
        "## A. 환경의 이해\n",
        "\n",
        "`-` 무작위로 10000판을 진행해보자."
      ],
      "id": "b1aa0d07-e456-47df-a35f-c010cbac9ec0"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for _ in range(10000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        # agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 "
      ],
      "id": "40b12099-567e-419d-b629-d34f01810511"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "ceba972e-030e-4bd1-ae1d-5367cea52107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "1a6b5310-35a2-4b05-8d99-508fee06e8b6"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[0]} / {agent.actions[0]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[0]} / {agent.next_states[0]}\")"
      ],
      "id": "907a43bc-1644-4ad2-ba8f-f9296035721a"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 2\n",
            "환경: 보상/다음상태 = -10 / [-1  1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[1]} / {agent.actions[1]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[1]} / {agent.next_states[1]}\")"
      ],
      "id": "57ae01f6-3178-477f-ba2b-b708a91338c4"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / 3\n",
            "환경: 보상/다음상태 = -10 / [ 0 -1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[2]} / {agent.actions[2]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[2]} / {agent.next_states[2]}\")"
      ],
      "id": "9fdc44e3-02df-4b0b-a4e4-39765d50e7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[3]} / {agent.actions[3]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[3]} / {agent.next_states[3]}\")"
      ],
      "id": "69c21c21-3995-4167-8741-becd4ac9a2a1"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 2\n",
            "환경: 보상/다음상태 = -10 / [-1  1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[4]} / {agent.actions[4]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[4]} / {agent.next_states[4]}\")"
      ],
      "id": "4288e684-decd-4056-83bd-9f2a0b79ec35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "bd5d672b-7275-472f-9179-25c6e8e87d00"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i] \n",
        "    a = agent.actions[i] \n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a] + agent.rewards[i] \n",
        "    count[s1,s2,a] = count[s1,s2,a] + 1 "
      ],
      "id": "dc434942-f67f-4c5f-8315-6a6f2da7a175"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "count[count == 0] = 0.01 \n",
        "q_table = q_table/count"
      ],
      "id": "6a2698f4-75cd-463b-9256-c84a23793b1a"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,3]"
      ],
      "id": "b0da7c22-98c3-421f-888c-a441da28700d"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(3)}\\n\" \n",
        ")"
      ],
      "id": "c7575209-a13d-4851-a793-dc61bd46f5da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2)"
      ],
      "id": "7301f64a-7f6f-4be2-9405-73e02f20201b"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_hat = q_table[s1,s2,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n",
        "    q = agent.rewards[i] # 실제 답 \n",
        "    diff = q - q_hat # 실제답과 풀이한값의 차이 = 오차피드백값 \n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 "
      ],
      "id": "914e7423-5d32-4b5a-9a2c-3cf0324e721a"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,0]"
      ],
      "id": "ee91f34f-f6d3-4a49-9d99-9a087d2275a5"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    99.15]\n",
            " [ -9.98 -10.    -9.8    0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[-1.   -1.   -1.   -9.96]\n",
            " [-1.   -1.   -1.   -9.98]\n",
            " [-1.   -1.   -1.   -9.91]\n",
            " [-1.   -1.   99.38  0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10.   -10.   -10.    -9.98]\n",
            " [ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    -0.98]\n",
            " [ -1.    -1.    -0.99   0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.    -1.    -1.    -1.  ]\n",
            " [-10.    -1.    -1.    -1.  ]\n",
            " [-10.    -1.    -1.    -0.99]\n",
            " [ -9.99  -1.    -0.99   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "0d2778f3-b24b-4a2b-9abd-78f1a687e9ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경의 깊은 이해\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "a71c1f97-c0e8-4913-90f3-60de5797636d"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,1]"
      ],
      "id": "a1214644-c901-4eb9-8d32-563df474baf2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "2d4b66a6-d28a-496e-ba00-bf50cc648dce"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,1]"
      ],
      "id": "08ca1c41-211a-4c55-85e9-bce10f20b3d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 –\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "e4b0c5a8-e316-4962-9d7c-852d530a65cb"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,1,1]"
      ],
      "id": "0586f726-9795-4435-89aa-f73dd145c176"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 –\\> 합리적일까??\n",
        "\n",
        "`-` 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지\n",
        "합리적이지 못함.\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n",
        "-   0을 쓸때와 1을 쓸때 보상이 다름\n",
        "-   무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면\n",
        "    10만원을 보상을 준다는 것을 “알게 되었음”\n",
        "-   이때 빈 종이의 가치는 5만원인가? 10만원인가? –\\> 10만원아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$ $a=1$에서 추정된(esitated) 값은\n",
        "`q_table[3,1,1]` $\\approx$ -1 이지만[1], 현실적으로는 “실제보상(-1)과\n",
        "잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n",
        "\n",
        "[1] 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음"
      ],
      "id": "34f00aa0-bd76-477e-a888-2cec2f392849"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_hat = q_table[3,1,1]\n",
        "q_hat"
      ],
      "id": "cfa4a389-21a8-4505-a864-fe28600a15c5"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q = (-1) + 0.99 * 100 \n",
        "q"
      ],
      "id": "c9c2af60-dfdb-4053-8609-3aa02e559007"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이\n",
        "    $\\approx$ 십만원 으로 생각한다는 의미)\n",
        "-   0.99는 보통 $\\gamma$라는 기호로 표기하며 `discount rate`이라고\n",
        "    표현한다. (외우세여)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s$, $a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을\n",
        "좀 더 엄밀하게 쓰면 (terminated, not-terminated 로 나누어 쓰면) 아래와\n",
        "같다.\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases}  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{$s$ is not in terminated state} \\\\ \\text{reward}(s,a) & \\text{$s$ is in terminated-state} \\end{cases}$$\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 대충 설명하면서 넘어갔지만 이 수식을 **벨만방정식**이라고 부른다.\n",
        "> (외우세여) 위의 식은 강화학습에서 가장 중요한 식이며 원래 버전은\n",
        "> 아래와 같다.\n",
        ">\n",
        "> $$Q^\\star(s,a) = R(s,a) +\\gamma\\sum_{s'}P(s'|s,a)\\max_{a}Q(s',a)$$\n",
        ">\n",
        "> 여기에서 $P(s'|s,a)$ 는 상태 $s \\in {\\cal S}$에서 행동\n",
        "> $a \\in {\\cal A}$를 했을때 $s'$에 있을 확률이다. 이러한 확률은\n",
        "> “바람,소용돌이” 등의 외부의 확률적인 요소가 있는 환경에서 의미가\n",
        "> 있으며 우리의 예제에서는 의미가 없다."
      ],
      "id": "33ed7442-0005-4cb8-91e8-1944cfb5fc00"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    ss1,ss2 = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_hat = q_table[s1,s2,a] \n",
        "    if agent.terminations[i]:\n",
        "        q = agent.rewards[i]\n",
        "    else:\n",
        "        future_reward = q_table[ss1,ss2,:].max()\n",
        "        q = agent.rewards[i] + 0.99 * future_reward\n",
        "    diff = q - q_hat\n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff "
      ],
      "id": "4d6ffc83-3edb-42a7-a12c-23dc77e849cc"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[-0.47 -0.14 -0.05  0.  ]\n",
            " [-0.1  -0.05  0.   -0.05]\n",
            " [-0.05 -0.05  0.    5.  ]\n",
            " [ 0.   -0.5   0.    0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[-0.61 -0.14  0.    0.  ]\n",
            " [-0.1  -0.05 -0.1  -0.5 ]\n",
            " [-0.1  -0.1   0.    0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-3.02 -3.02  0.    0.  ]\n",
            " [-0.17 -0.1   0.    0.  ]\n",
            " [ 0.    0.   -0.05  0.  ]\n",
            " [-0.05  0.    0.    0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-2.26 -0.39 -0.1   0.  ]\n",
            " [-3.02 -0.1  -0.05  0.  ]\n",
            " [-0.5  -0.05 -0.05  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "ff789b47-1166-40f2-af86-1403f91a3643"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "4f826f1c-fdeb-4a9a-823c-20fcc1219e21"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:]"
      ],
      "id": "4f17808b-c852-4027-baa0-f364d2dd3ee0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "4b8db641-7812-4f33-a0b5-d9f7c4e48aa5"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:]"
      ],
      "id": "b0a53547-7ae8-4950-b10c-2146755a87b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "63674239-1f88-4e59-adbc-ea3bfd9461c8"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:]"
      ],
      "id": "92aac9f3-5e47-42bb-8e62-5bf64f36cc29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동1을 하는게 유리함\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "f70a4c85-93ba-4148-861e-109442a57f57"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:].argmax()"
      ],
      "id": "f1b692c3-94b6-4c9e-8a92-ff0bb836c83b"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:].argmax()"
      ],
      "id": "982e1340-a7d2-4e2a-84d8-f4d41072227b"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:].argmax()"
      ],
      "id": "ede24e2e-ed08-405e-870e-f9b780ec5451"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "92195a18-cd98-43dc-a779-eadc1e43eb9c"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "a9cd5f06-8a65-46e7-9f74-92ec7c2d9d59"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for s1 in range(4):\n",
        "    for s2 in range(4):\n",
        "        policy[s1,s2] = action_to_direction2[q_table[s1,s2,:].argmax()]\n",
        "policy"
      ],
      "id": "c2c85140-c119-4f09-8529-d293f75662e8"
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table.max(axis=-1)"
      ],
      "id": "699a6066-80e7-415c-aa87-57036e07979f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 에이전트 클래스 설계"
      ],
      "id": "756a4dc5-1aa4-48dd-a3ce-272c47d999b2"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentGreedy(AgentRandom):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q_table = np.zeros([4,4,4]) \n",
        "    def learn(self):\n",
        "        s1,s2 = self.current_state\n",
        "        ss2,ss2 = self.next_state\n",
        "        a = self.action \n",
        "        q_hat = self.q_table[s1,s2,a] \n",
        "        if self.terminated:\n",
        "            q = self.reward\n",
        "        else:\n",
        "            future_reward = q_table[ss1,ss2,:].max()\n",
        "            q = self.reward + 0.99 * future_reward \n",
        "        diff = q - q_hat \n",
        "        self.q_table[s1,s2,a] = q_hat + 0.05 * diff \n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            s1,s2 = self.current_state \n",
        "            self.action = self.q_table[s1,s2,:].argmax()"
      ],
      "id": "fbd05240-8d9f-45d9-a704-f4aff617d46c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. 환경과 상호작용"
      ],
      "id": "871a5b9f-fa09-49d3-a6cc-f8347563d507"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.64   Playtime: 2.74  \n",
            "Epsiode: 1000   Score: 87.27    Playtime: 12.63 \n",
            "Epsiode: 1500   Score: 90.66    Playtime: 10.34 \n",
            "Epsiode: 2000   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 2500   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 3000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 3500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 5000   Score: 95.0     Playtime: 6.0   "
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentGreedy(env) \n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "        )     "
      ],
      "id": "3bf71d4d-f5b3-48f7-a437-dd0eedb4d211"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. 상호작용결과 시각화"
      ],
      "id": "090046ee-5515-490f-a221-f440f064f999"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "3cc1eb34-48f5-4096-8bdb-d9af4cab8e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. `AgentExplorer`\n",
        "\n",
        "## A. 클래스 설계"
      ],
      "id": "ac853811-6c37-45c1-bb58-a28308e52bcc"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentExplorer(AgentGreedy):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0 \n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            super().act()"
      ],
      "id": "50575733-9c15-4c33-81fd-9a8b321a54d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "251ea0b5-191a-4a6d-ad7d-066abd318004"
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.28   Playtime: 3.48  Epsilon:  0.61\n",
            "Epsiode: 1000   Score: -1.61    Playtime: 4.71  Epsilon:  0.37\n",
            "Epsiode: 1500   Score: 38.27    Playtime: 8.83  Epsilon:  0.22\n",
            "Epsiode: 2000   Score: 59.41    Playtime: 9.69  Epsilon:  0.14\n",
            "Epsiode: 2500   Score: 82.61    Playtime: 7.39  Epsilon:  0.08\n",
            "Epsiode: 3000   Score: 80.89    Playtime: 6.91  Epsilon:  0.05\n",
            "Epsiode: 3500   Score: 92.8     Playtime: 6.0   Epsilon:  0.03\n",
            "Epsiode: 4000   Score: 92.71    Playtime: 6.09  Epsilon:  0.02\n",
            "Epsiode: 4500   Score: 93.85    Playtime: 6.05  Epsilon:  0.01\n",
            "Epsiode: 5000   Score: 92.86    Playtime: 5.94  Epsilon:  0.01"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentExplorer(env) \n",
        "agent.eps = 1\n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    agent.eps = agent.eps * 0.999\n",
        "    #--#\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "            f\"Epsilon: {agent.eps : .2f}\"\n",
        "        )   "
      ],
      "id": "50ba284c-2a6f-4218-b521-533352867e68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용 결과 시각화"
      ],
      "id": "08429ffc-41b0-4e38-bb13-a8ae8202fe97"
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "c1513a2f-7822-4b77-af8f-6a11be9eb192"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  }
}