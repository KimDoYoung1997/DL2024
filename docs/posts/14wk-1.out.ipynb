{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-1: 강화학습 (2) – 4x4 Grid World (`AgentRandom`)\n",
        "\n",
        "최규빈  \n",
        "2024-06-03\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/14wk-1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "34957a99-fc01-4621-b642-73074ea187b9"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zHvVuJ92xfdypwHwDFgg8k&si=iI4IhthblTsJTmIv >}}"
      ],
      "id": "1dfb3796-7623-4061-bc81-82305c2d4d59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "785cf777-a9fe-4001-8332-46329e9c6626"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install gymnasium\n",
        "#---#\n",
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "5b1cfd48-d338-4ddd-8b59-f8ab593ebda9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Game2: 4x4 Grid World\n",
        "\n",
        "`-` 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에\n",
        "도달하도록 학습하는 방법\n",
        "\n",
        "`-` GridWorld에서 사용되는 주요변수\n",
        "\n",
        "1.  **`State`**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **`Action`**: 에이전트는 현재상태에서 다음상태로 이동하기 위해\n",
        "    상,하,좌,우 중 하나의 행동을 취할 수 있음.\n",
        "3.  **`Reward`**: 에이전트가 현재상태에서 특정 action을 하면 얻어지는\n",
        "    보상.\n",
        "4.  **`Terminated`**: 하나의 에피소드가 종료되었음을 나타내는 상태.\n",
        "\n",
        "# 4. 예비학습\n",
        "\n",
        "## A. `gym.spaces`\n",
        "\n",
        "`-` 예시1"
      ],
      "id": "fcfd7603-5fc5-496b-9338-15a3e1738078"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_space = gym.spaces.Discrete(4) \n",
        "action_space "
      ],
      "id": "86af6553-1d04-4f6a-b8ea-e91cb779e90e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "[action_space.sample() for _ in range(5)]"
      ],
      "id": "365100ab-8550-4ceb-a79c-76b0537a34b3"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "0 in action_space"
      ],
      "id": "0a6049ab-0309-4f6b-a053-1aa56390c1c2"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "4 in action_space"
      ],
      "id": "cc93a879-f95e-4108-a349-d1664634205f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 예시2"
      ],
      "id": "0ec8647d-3fb3-4428-b428-556e73e2e8df"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "state_space"
      ],
      "id": "19c4f90d-33ed-4a6f-bf36-070ac192d890"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "[state_space.sample() for _ in range(5)]"
      ],
      "id": "3f815cbc-036a-4497-965a-2727842f7ec5"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "np.array([0,1]) in state_space"
      ],
      "id": "e0b969d0"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,3]) in state_space"
      ],
      "id": "04da0368-8f85-43f2-adb1-eb5dd8465806"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "np.array([3,4]) in state_space"
      ],
      "id": "13f530d6-d1a7-40cb-89ec-b97c27b8339c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 시각화"
      ],
      "id": "e99d8bc2-48cd-4432-b46d-7d7be95df4a1"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "    def update(t):\n",
        "        if states[t] in state_space:\n",
        "            s1,s2 = states[t]\n",
        "            states[t] = [s2,s1]\n",
        "            sc.set_offsets(states[t])\n",
        "        else:\n",
        "            s1,s2 = states[t]\n",
        "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
        "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
        "            states[t] = [s2,s1]       \n",
        "            sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "0b3ec243-70a1-46e7-bd95-2bb4a5338898"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "show([[0,0],[1,0],[2,0],[3,0],[4,0]])"
      ],
      "id": "5379f2af-62d5-43ff-b16f-4689e7a53fe5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Env 클래스 구현"
      ],
      "id": "749298bb-5ce1-4033-82fe-dcfbe763dc79"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_to_direction = {\n",
        "    0 : np.array([1, 0]), # row+, down\n",
        "    1 : np.array([0, 1]), # col+, right\n",
        "    2 : np.array([-1 ,0]), # row-, up\n",
        "    3 : np.array([0, -1]) # col-, left\n",
        "}\n",
        "action_to_direction2 = {0: 'down', 1: 'right', 2: 'up', 3: 'left'} # 당장쓰진 않지만 하는김에 "
      ],
      "id": "23c93695"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = action_space.sample()"
      ],
      "id": "249cf618"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "direction = action_to_direction[action]"
      ],
      "id": "326085f3"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_state = state_space.sample()\n",
        "next_state = current_state + direction\n",
        "current_state, direction, next_state"
      ],
      "id": "d36b3a0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` Class 구현: 아래와 같은 느낌의 클래스를 구현해보자."
      ],
      "id": "43cf3b88-26a1-4775-8996-0e448f1ab465"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.action_space = gym.spaces.Discrete(4) \n",
        "        self._action_to_direction = {\n",
        "            0 : np.array([1, 0]), # row+, down\n",
        "            1 : np.array([0, 1]), # col+, right\n",
        "            2 : np.array([-1 ,0]), # row-, up\n",
        "            3 : np.array([0, -1]) # col-, left\n",
        "        }\n",
        "        self.reset()\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.state = self.state + direction\n",
        "        if np.array_equal(self.state,np.array([3,3])): \n",
        "            reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state not in state_space:\n",
        "            reward = -10\n",
        "            self.terminated = True\n",
        "        else:\n",
        "            reward = -1 \n",
        "        return self.state, reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False   \n",
        "        return self.state "
      ],
      "id": "e89495dc"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "state = env.reset()\n",
        "states = [] \n",
        "states.append(state)\n",
        "for t in range(50):\n",
        "    action = env.action_space.sample() \n",
        "    state,reward,terminated = env.step(action)\n",
        "    states.append(state)\n",
        "    if terminated: break "
      ],
      "id": "a2358dfa"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "886e5543-619a-4488-a39f-77bc8a2fa254"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   처음에 바로 죽는 경우가 많아 몇번 시도하고 위의 애니메이션을 얻음\n",
        "\n",
        "# 6. `AgentRandom`\n",
        "\n",
        "## A. 에이전트 클래스 설계\n",
        "\n",
        "`-` 우리가 구현하고 싶은 기능\n",
        "\n",
        "-   `.act()`: 액션을 결정 –\\> 여기서는 그냥 랜덤액션\n",
        "-   `.save_experience()`: 데이터를 저장 –\\> 여기에 일단 초점을 맞추자\n",
        "-   `.learn()`: 데이터로에서 학습 –\\> 패스"
      ],
      "id": "b66470bd-c793-484a-91d9-697dff1a35e0"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentRandom: \n",
        "    def __init__(self,env):\n",
        "        # define spaces \n",
        "        self.action_space = env.action_space\n",
        "        self.state_space = env.state_space\n",
        "        #--# replay buffer \n",
        "        self.action = None \n",
        "        self.actions = [] \n",
        "        self.current_state =  None \n",
        "        self.current_states = [] \n",
        "        self.reward = None \n",
        "        self.rewards = [] \n",
        "        self.next_state =  None \n",
        "        self.next_states = [] \n",
        "        self.terminated = None \n",
        "        self.terminations = []\n",
        "        #--# other information\n",
        "        self.n_episodes = 0         \n",
        "        self.n_experiences = 0\n",
        "        self.score = 0        \n",
        "        self.playtimes = [] \n",
        "        self.scores = []    \n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def learn(self):\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        self.current_states.append(self.current_state)        \n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)  \n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        #--#\n",
        "        self.n_experiences = self.n_experiences + 1 \n",
        "        self.score = self.score + self.reward"
      ],
      "id": "b6a2a93a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "b761cc99-249c-4050-b6e8-9800d68e775d"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에피소드: 1     점수(에피소드): -12   게임시간(에피소드): 3   경험수: 3\n",
            "에피소드: 2     점수(에피소드): -15   게임시간(에피소드): 6   경험수: 9\n",
            "에피소드: 3     점수(에피소드): -11   게임시간(에피소드): 2   경험수: 11\n",
            "에피소드: 4     점수(에피소드): -10   게임시간(에피소드): 1   경험수: 12\n",
            "에피소드: 5     점수(에피소드): -10   게임시간(에피소드): 1   경험수: 13\n",
            "에피소드: 6     점수(에피소드): -12   게임시간(에피소드): 3   경험수: 16\n",
            "에피소드: 7     점수(에피소드): -11   게임시간(에피소드): 2   경험수: 18\n",
            "에피소드: 8     점수(에피소드): -18   게임시간(에피소드): 9   경험수: 27\n",
            "에피소드: 9     점수(에피소드): -10   게임시간(에피소드): 1   경험수: 28\n",
            "에피소드: 10    점수(에피소드): 91    게임시간(에피소드): 10  경험수: 38\n",
            "에피소드: 11    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 39\n",
            "에피소드: 12    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 40\n",
            "에피소드: 13    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 42\n",
            "에피소드: 14    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 43\n",
            "에피소드: 15    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 44\n",
            "에피소드: 16    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 45\n",
            "에피소드: 17    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 46\n",
            "에피소드: 18    점수(에피소드): -15   게임시간(에피소드): 6   경험수: 52\n",
            "에피소드: 19    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 54\n",
            "에피소드: 20    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 55\n",
            "에피소드: 21    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 56\n",
            "에피소드: 22    점수(에피소드): -12   게임시간(에피소드): 3   경험수: 59\n",
            "에피소드: 23    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 61\n",
            "에피소드: 24    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 63\n",
            "에피소드: 25    점수(에피소드): -13   게임시간(에피소드): 4   경험수: 67\n",
            "에피소드: 26    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 68\n",
            "에피소드: 27    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 69\n",
            "에피소드: 28    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 71\n",
            "에피소드: 29    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 72\n",
            "에피소드: 30    점수(에피소드): -13   게임시간(에피소드): 4   경험수: 76\n",
            "에피소드: 31    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 77\n",
            "에피소드: 32    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 78\n",
            "에피소드: 33    점수(에피소드): -18   게임시간(에피소드): 9   경험수: 87\n",
            "에피소드: 34    점수(에피소드): -13   게임시간(에피소드): 4   경험수: 91\n",
            "에피소드: 35    점수(에피소드): -18   게임시간(에피소드): 9   경험수: 100\n",
            "에피소드: 36    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 101\n",
            "에피소드: 37    점수(에피소드): -15   게임시간(에피소드): 6   경험수: 107\n",
            "에피소드: 38    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 108\n",
            "에피소드: 39    점수(에피소드): -13   게임시간(에피소드): 4   경험수: 112\n",
            "에피소드: 40    점수(에피소드): -17   게임시간(에피소드): 8   경험수: 120\n",
            "에피소드: 41    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 121\n",
            "에피소드: 42    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 122\n",
            "에피소드: 43    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 123\n",
            "에피소드: 44    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 124\n",
            "에피소드: 45    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 125\n",
            "에피소드: 46    점수(에피소드): -17   게임시간(에피소드): 8   경험수: 133\n",
            "에피소드: 47    점수(에피소드): -10   게임시간(에피소드): 1   경험수: 134\n",
            "에피소드: 48    점수(에피소드): -13   게임시간(에피소드): 4   경험수: 138\n",
            "에피소드: 49    점수(에피소드): -12   게임시간(에피소드): 3   경험수: 141\n",
            "에피소드: 50    점수(에피소드): -11   게임시간(에피소드): 2   경험수: 143"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = AgentRandom(env)\n",
        "#--#\n",
        "for _ in range(50):\n",
        "    agent.current_state = env.reset()\n",
        "    agent.score = 0 \n",
        "    for t in range(100):\n",
        "        # step1: 행동\n",
        "        agent.act()\n",
        "        # step2: 보상\n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습\n",
        "        agent.save_experience()\n",
        "        agent.learn()\n",
        "        # step4: \n",
        "        agent.current_state = agent.next_state\n",
        "        if agent.terminated: break\n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    #---#\n",
        "    print(\n",
        "        f\"에피소드: {agent.n_episodes} \\t\"\n",
        "        f\"점수(에피소드): {agent.scores[-1]} \\t\" \n",
        "        f\"게임시간(에피소드): {agent.playtimes[-1]}\\t\"\n",
        "        f\"경험수: {agent.n_experiences}\"\n",
        "    )"
      ],
      "id": "86aa43ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용결과 시각화"
      ],
      "id": "26bdeded-4109-4a43-8caa-0ae5bad3106b"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "[np.array([0,0])] + agent.next_states[28:38] # 에피소드10"
      ],
      "id": "dc816c17"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "show([np.array([0,0])] + agent.next_states[28:38]) # 에피소드5"
      ],
      "id": "d2767667"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. `AgentGreedy`\n",
        "\n",
        "## A. 환경의 이해\n",
        "\n",
        "`-` 무작위로 10000판을 진행해보자."
      ],
      "id": "19f28ae0-9050-40a1-af6a-95e89f401d75"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for _ in range(20000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 "
      ],
      "id": "40b12099-567e-419d-b629-d34f01810511"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "ceba972e-030e-4bd1-ae1d-5367cea52107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "778cd67c-81f7-4b8e-af09-c47346f023e2"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[0]} / {agent.actions[0]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[0]} / {agent.next_states[0]}\")"
      ],
      "id": "907a43bc-1644-4ad2-ba8f-f9296035721a"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 2]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[1]} / {agent.actions[1]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[1]} / {agent.next_states[1]}\")"
      ],
      "id": "57ae01f6-3178-477f-ba2b-b708a91338c4"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 2] / 3\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[2]} / {agent.actions[2]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[2]} / {agent.next_states[2]}\")"
      ],
      "id": "9fdc44e3-02df-4b0b-a4e4-39765d50e7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 0\n",
            "환경: 보상/다음상태 = -1 / [1 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[3]} / {agent.actions[3]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[3]} / {agent.next_states[3]}\")"
      ],
      "id": "69c21c21-3995-4167-8741-becd4ac9a2a1"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [1 1] / 3\n",
            "환경: 보상/다음상태 = -1 / [1 0]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[4]} / {agent.actions[4]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[4]} / {agent.next_states[4]}\")"
      ],
      "id": "4288e684-decd-4056-83bd-9f2a0b79ec35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "ca300dd8-b832-4af3-885e-da6f2a38a6e9"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i] \n",
        "    a = agent.actions[i] \n",
        "    r = agent.rewards[i] \n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n",
        "    count[s1,s2,a] = count[s1,s2,a] + 1 "
      ],
      "id": "dc434942-f67f-4c5f-8315-6a6f2da7a175"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "count[count == 0] = 0.01 \n",
        "q_table = q_table/count"
      ],
      "id": "6a2698f4-75cd-463b-9256-c84a23793b1a"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,3]"
      ],
      "id": "b0da7c22-98c3-421f-888c-a441da28700d"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(3)}\\n\" \n",
        ")"
      ],
      "id": "c7575209-a13d-4851-a793-dc61bd46f5da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2)"
      ],
      "id": "ee901c4d-8d31-4613-9363-b06e3639e21e"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    r = agent.rewards[i]\n",
        "    q_hat = q_table[s1,s2,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n",
        "    q = r # 실제 답 \n",
        "    diff = q - q_hat # 실제답과 풀이한값의 차이 = 오차피드백값 \n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 "
      ],
      "id": "914e7423-5d32-4b5a-9a2c-3cf0324e721a"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,0]"
      ],
      "id": "ee91f34f-f6d3-4a49-9d99-9a087d2275a5"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "0d2778f3-b24b-4a2b-9abd-78f1a687e9ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경의 깊은 이해\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "852a6c4e-88d6-4d14-9732-33ba760858f4"
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,1]"
      ],
      "id": "a1214644-c901-4eb9-8d32-563df474baf2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "0a059dd0-b8c2-4caf-bfcd-2b5d6901adb3"
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,1]"
      ],
      "id": "08ca1c41-211a-4c55-85e9-bce10f20b3d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 –\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "2e6c14fa-ae86-4f36-a3a1-3561778228ec"
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,1,1]"
      ],
      "id": "0586f726-9795-4435-89aa-f73dd145c176"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 –\\> 합리적일까??\n",
        "\n",
        "`-` 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지\n",
        "합리적이지 못함.\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n",
        "-   0을 쓸때와 1을 쓸때 보상이 다름\n",
        "-   무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면\n",
        "    10만원을 보상을 준다는 것을 “알게 되었음”\n",
        "-   이때 빈 종이의 가치는 5만원인가? 10만원인가? –\\> 10만원아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$ $a=1$에서 추정된(esitated) 값은\n",
        "`q_table[3,1,1]` $\\approx$ -1 이지만[1], 현실적으로는 “실제보상(-1)과\n",
        "잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n",
        "\n",
        "[1] 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음"
      ],
      "id": "286ca894-4d25-44ac-97cd-a01a72d02a83"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_hat = q_table[3,1,1]\n",
        "q_hat"
      ],
      "id": "cfa4a389-21a8-4505-a864-fe28600a15c5"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q = (-1) + 0.99 * 100 \n",
        "q"
      ],
      "id": "c9c2af60-dfdb-4053-8609-3aa02e559007"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이\n",
        "    $\\approx$ 십만원 으로 생각한다는 의미)\n",
        "-   0.99는 보통 $\\gamma$라는 기호로 표기하며 `discount rate`이라고\n",
        "    표현한다. (외우세여)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s$, $a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을\n",
        "좀 더 엄밀하게 쓰면 (terminated, not-terminated 로 나누어 쓰면) 아래와\n",
        "같다.\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases}  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated} \\\\ \\text{reward}(s,a) & \\text{terminated} \\end{cases}$$\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 대충 설명하면서 넘어갔지만 이 수식을 **벨만방정식**이라고 부른다.\n",
        "> (외우세여) 위의 식은 강화학습에서 가장 중요한 식이며 원래 버전은\n",
        "> 아래와 같다.\n",
        ">\n",
        "> $$Q^\\star(s,a) = R(s,a) +\\gamma\\sum_{s'}P(s'|s,a)\\max_{a}Q(s',a)$$\n",
        ">\n",
        "> 여기에서 $P(s'|s,a)$ 는 상태 $s \\in {\\cal S}$에서 행동\n",
        "> $a \\in {\\cal A}$를 했을때 $s'$에 있을 확률이다. 이러한 확률은\n",
        "> “바람,소용돌이” 등의 외부의 확률적인 요소가 있는 환경에서 의미가\n",
        "> 있으며 우리의 예제에서는 의미가 없다."
      ],
      "id": "4cd46ca7-04f5-448f-b229-96acbde929ea"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    ss1,ss2 = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_hat = q_table[s1,s2,a] \n",
        "    if agent.terminations[i]:\n",
        "        q = agent.rewards[i]\n",
        "    else:\n",
        "        future_reward = q_table[ss1,ss2,:].max()\n",
        "        q = agent.rewards[i] + 0.99 * future_reward\n",
        "    diff = q - q_hat\n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff "
      ],
      "id": "4d6ffc83-3edb-42a7-a12c-23dc77e849cc"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[-0.47 -0.14 -0.05  0.  ]\n",
            " [-0.1  -0.05  0.   -0.05]\n",
            " [-0.05 -0.05  0.    5.  ]\n",
            " [ 0.   -0.5   0.    0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[-0.61 -0.14  0.    0.  ]\n",
            " [-0.1  -0.05 -0.1  -0.5 ]\n",
            " [-0.1  -0.1   0.    0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-3.02 -3.02  0.    0.  ]\n",
            " [-0.17 -0.1   0.    0.  ]\n",
            " [ 0.    0.   -0.05  0.  ]\n",
            " [-0.05  0.    0.    0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-2.26 -0.39 -0.1   0.  ]\n",
            " [-3.02 -0.1  -0.05  0.  ]\n",
            " [-0.5  -0.05 -0.05  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "ff789b47-1166-40f2-af86-1403f91a3643"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "417a6e1f-b1b6-46fb-9503-063cef076c92"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:]"
      ],
      "id": "4f17808b-c852-4027-baa0-f364d2dd3ee0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "95adac57-31be-49c7-b560-4484e1f2e73c"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:]"
      ],
      "id": "b0a53547-7ae8-4950-b10c-2146755a87b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "81116005-99bb-4057-bc5d-f01da287fc02"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:]"
      ],
      "id": "92aac9f3-5e47-42bb-8e62-5bf64f36cc29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동1을 하는게 유리함\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "ca705a2d-7570-400f-b4d3-e1b206245fb9"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:].argmax()"
      ],
      "id": "f1b692c3-94b6-4c9e-8a92-ff0bb836c83b"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:].argmax()"
      ],
      "id": "982e1340-a7d2-4e2a-84d8-f4d41072227b"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:].argmax()"
      ],
      "id": "ede24e2e-ed08-405e-870e-f9b780ec5451"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "d7416770-0d58-49fe-bf15-3c498feb6357"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "a9cd5f06-8a65-46e7-9f74-92ec7c2d9d59"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for s1 in range(4):\n",
        "    for s2 in range(4):\n",
        "        policy[s1,s2] = action_to_direction2[q_table[s1,s2,:].argmax()]\n",
        "policy"
      ],
      "id": "c2c85140-c119-4f09-8529-d293f75662e8"
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table.max(axis=-1)"
      ],
      "id": "699a6066-80e7-415c-aa87-57036e07979f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 에이전트 클래스 설계"
      ],
      "id": "98295208-7256-4eee-b17c-fd66be6a93f8"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentGreedy(AgentRandom):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q_table = np.zeros([4,4,4]) \n",
        "    def learn(self):\n",
        "        s1,s2 = self.current_state\n",
        "        ss2,ss2 = self.next_state\n",
        "        a = self.action \n",
        "        q_hat = self.q_table[s1,s2,a] \n",
        "        if self.terminated:\n",
        "            q = self.reward\n",
        "        else:\n",
        "            future_reward = q_table[ss1,ss2,:].max()\n",
        "            q = self.reward + 0.99 * future_reward \n",
        "        diff = q - q_hat \n",
        "        self.q_table[s1,s2,a] = q_hat + 0.05 * diff \n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            s1,s2 = self.current_state \n",
        "            self.action = self.q_table[s1,s2,:].argmax()"
      ],
      "id": "fbd05240-8d9f-45d9-a704-f4aff617d46c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. 환경과 상호작용"
      ],
      "id": "d1cda377-476d-47b6-a918-2887efe39506"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.64   Playtime: 2.74  \n",
            "Epsiode: 1000   Score: 87.27    Playtime: 12.63 \n",
            "Epsiode: 1500   Score: 90.66    Playtime: 10.34 \n",
            "Epsiode: 2000   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 2500   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 3000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 3500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 5000   Score: 95.0     Playtime: 6.0   "
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentGreedy(env) \n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "        )     "
      ],
      "id": "3bf71d4d-f5b3-48f7-a437-dd0eedb4d211"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. 상호작용결과 시각화"
      ],
      "id": "5f877204-8d53-4b4f-acad-24de668c542c"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "3cc1eb34-48f5-4096-8bdb-d9af4cab8e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. `AgentExplorer`\n",
        "\n",
        "## A. 클래스 설계"
      ],
      "id": "bad7c476-782c-4bee-ba27-c7cd95006786"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentExplorer(AgentGreedy):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0 \n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            super().act()"
      ],
      "id": "50575733-9c15-4c33-81fd-9a8b321a54d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "7cb159e4-40f4-45ed-97b2-5cc9feed5070"
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.28   Playtime: 3.48  Epsilon:  0.61\n",
            "Epsiode: 1000   Score: -1.61    Playtime: 4.71  Epsilon:  0.37\n",
            "Epsiode: 1500   Score: 38.27    Playtime: 8.83  Epsilon:  0.22\n",
            "Epsiode: 2000   Score: 59.41    Playtime: 9.69  Epsilon:  0.14\n",
            "Epsiode: 2500   Score: 82.61    Playtime: 7.39  Epsilon:  0.08\n",
            "Epsiode: 3000   Score: 80.89    Playtime: 6.91  Epsilon:  0.05\n",
            "Epsiode: 3500   Score: 92.8     Playtime: 6.0   Epsilon:  0.03\n",
            "Epsiode: 4000   Score: 92.71    Playtime: 6.09  Epsilon:  0.02\n",
            "Epsiode: 4500   Score: 93.85    Playtime: 6.05  Epsilon:  0.01\n",
            "Epsiode: 5000   Score: 92.86    Playtime: 5.94  Epsilon:  0.01"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentExplorer(env) \n",
        "agent.eps = 1\n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    agent.eps = agent.eps * 0.999\n",
        "    #--#\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "            f\"Epsilon: {agent.eps : .2f}\"\n",
        "        )   "
      ],
      "id": "50ba284c-2a6f-4218-b521-533352867e68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용 결과 시각화"
      ],
      "id": "1d5dfe13-9b18-41cb-80f2-27e6224c6d02"
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "c1513a2f-7822-4b77-af8f-6a11be9eb192"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  }
}