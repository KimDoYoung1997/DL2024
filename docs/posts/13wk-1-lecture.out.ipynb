{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13wk-1: 순환신경망 (4)\n",
        "\n",
        "최규빈  \n",
        "2024-05-27\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "# 2. Import"
      ],
      "id": "aa2e0a73-47a5-4ef3-95bc-d9dcf322291e"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "7b9d91c0-5d8b-419c-8bd1-900b958ab38b"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "soft = torch.nn.Softmax(dim=1)\n",
        "sig = torch.nn.Sigmoid()\n",
        "tanh = torch.nn.Tanh()"
      ],
      "id": "94b48461-2be7-4dc9-9c8b-6538bf629cc2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Data – `abcabC`"
      ],
      "id": "de4d4904-587e-44ae-bc40-b0fd09c4952c"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "txt = list('abcabC')*50\n",
        "txt[:10]"
      ],
      "id": "01e05d7e-833a-400f-80ac-68b24310ab6a"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame({'x':txt[:-1], 'y':txt[1:]})\n",
        "df_train[:5]"
      ],
      "id": "b4937931-a9f2-4efa-95c2-39bed26cd779"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(df_train.x.map({'a':0,'b':1,'c':2,'C':3}))\n",
        "y = torch.tensor(df_train.y.map({'a':0,'b':1,'c':2,'C':3}))\n",
        "X = torch.nn.functional.one_hot(x).float()\n",
        "y = torch.nn.functional.one_hot(y).float()"
      ],
      "id": "a8bc9363-4c7f-41dd-b5d4-c1537937979f"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "x"
      ],
      "id": "4be7666f-d970-4cc5-85b1-0a9e2b6c23e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. `torch.nn.LSTMCell` vs 직접구현\n",
        "\n",
        "### ***t=0 $\\to$ t=1***\n",
        "\n",
        "`-` `lstmcell`을 이용하여 $t=0 \\to t=1$을 구현해보자. (결과비교용)"
      ],
      "id": "a11ad468-a36f-4c20-a3ca-f84a5c8e39db"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(43052)\n",
        "lstmcell = torch.nn.LSTMCell(4,2)\n",
        "cook = torch.nn.Linear(2,4)\n",
        "#--#\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizr = torch.optim.Adam(list(lstmcell.parameters())+list(cook.parameters()),lr=0.1)\n",
        "#--#\n",
        "L = len(X)\n",
        "for epoc in range(1):\n",
        "    # step1~2\n",
        "    ht,ct = torch.zeros(2),torch.zeros(2)\n",
        "    loss = 0 \n",
        "    for t in range(1):\n",
        "        Xt,yt = X[t],y[t]\n",
        "        ht,ct = lstmcell(Xt,(ht,ct))\n",
        "        netout_t = cook(ht)  # 원래는 ot 로 썼는데, 여기서는 기호가 겹쳐서..\n",
        "        loss = loss + loss_fn(netout_t,yt)\n",
        "    loss = loss/L \n",
        "    # step3\n",
        "    loss.backward()\n",
        "    # step4 \n",
        "    optimizr.step()\n",
        "    optimizr.zero_grad()"
      ],
      "id": "d4cf1d70-d8f1-4efe-adf9-c2e1461f2a4c"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht,ct"
      ],
      "id": "e9f8b90c-3a07-46ce-8aa7-375520e8fac1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이런결과를 어떻게 만드는걸까?\n",
        "-   <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n",
        "\n",
        "`-` 직접계산 (ifgo)"
      ],
      "id": "d7042de0-5032-44fe-b9d0-dd882ef67606"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "lstmcell = torch.nn.LSTMCell(4,2)"
      ],
      "id": "b1054f6a-ca97-4cb7-8231-6e4c57ebfd34"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0053, -0.2723, -0.0628, -0.6755,  0.2795,  0.2568, -0.1140, -0.4452],\n",
            "        [ 0.3793,  0.1896,  0.1871, -0.4683,  0.4243,  0.5872,  0.0748, -0.1790],\n",
            "        [-0.5820, -0.0140, -0.2137, -0.2915, -0.4794, -0.1455,  0.6403, -0.2756],\n",
            "        [-0.5204,  0.5607, -0.1390,  0.0262, -0.3079,  0.5291, -0.6560,  0.6109]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Parameter containing:\n",
            "tensor([-0.3136, -0.0255,  0.4522,  0.7030,  0.2806,  0.0955,  0.4741, -0.4163],\n",
            "       requires_grad=True)"
          ]
        }
      ],
      "source": [
        "print(lstmcell.weight_ih.T)\n",
        "print(lstmcell.bias_ih)"
      ],
      "id": "c1243742-be10-4f86-b6e8-0ce06d073775"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4583, -0.4940, -0.4128,  0.3155,  0.0372,  0.1196, -0.5109,  0.4461],\n",
            "        [-0.3255, -0.6622,  0.6078,  0.3427, -0.3625, -0.6602, -0.3645,  0.4146]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Parameter containing:\n",
            "tensor([ 0.1318, -0.5482, -0.4901, -0.3653,  0.3199,  0.2844, -0.4189,  0.2136],\n",
            "       requires_grad=True)"
          ]
        }
      ],
      "source": [
        "print(lstmcell.weight_hh.T)\n",
        "print(lstmcell.bias_hh)"
      ],
      "id": "b976df17-bcc8-46f8-b121-4c9f1aa145c7"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "Wih02 = lstmcell.weight_ih.T[:,0:2] # (4,2)\n",
        "Wih24 = lstmcell.weight_ih.T[:,2:4] # (4,2)\n",
        "Wih46 = lstmcell.weight_ih.T[:,4:6] # (4,2)\n",
        "Wih68 = lstmcell.weight_ih.T[:,6:8] # (4,2)\n",
        "#--#\n",
        "Whh02 = lstmcell.weight_hh.T[:,0:2] # (2,2)\n",
        "Whh24 = lstmcell.weight_hh.T[:,2:4] # (2,2)\n",
        "Whh46 = lstmcell.weight_hh.T[:,4:6] # (2,2)\n",
        "Whh68 = lstmcell.weight_hh.T[:,6:8] # (2,2) \n",
        "#--#\n",
        "bih02 = lstmcell.bias_ih[0:2] \n",
        "bih24 = lstmcell.bias_ih[2:4] \n",
        "bih46 = lstmcell.bias_ih[4:6] \n",
        "bih68 = lstmcell.bias_ih[6:8]\n",
        "#--#\n",
        "bhh02 = lstmcell.bias_hh[0:2] \n",
        "bhh24 = lstmcell.bias_hh[2:4] \n",
        "bhh46 = lstmcell.bias_hh[4:6] \n",
        "bhh68 = lstmcell.bias_hh[6:8] "
      ],
      "id": "98dacd3b-ed84-4762-be20-4a472ca31ac2"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht = torch.zeros(2)\n",
        "ct = torch.zeros(2)\n",
        "#--#\n",
        "it = sig((Xt@Wih02 + bih02) + (ht@Whh02 + bhh02))\n",
        "ft = sig((Xt@Wih24 + bih24) + (ht@Whh24 + bhh24))\n",
        "gt = tanh((Xt@Wih46 + bih46) + (ht@Whh46 + bhh46))\n",
        "ot = sig((Xt@Wih68 + bih68) + (ht@Whh68 + bhh68))"
      ],
      "id": "580dbe8d-65c3-4fb0-8874-2e510bb4dd26"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "it,ft,gt,ot"
      ],
      "id": "8e534810-3dd8-4a35-a114-1bb700e632d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "그런데 아래와 같이 계산할수도 있음."
      ],
      "id": "438584ea-d554-45f3-9207-dcbdf2677b4f"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "i = (Xt@Wih02 + bih02) + (ht@Whh02 + bhh02)\n",
        "f = (Xt@Wih24 + bih24) + (ht@Whh24 + bhh24)\n",
        "g = (Xt@Wih46 + bih46) + (ht@Whh46 + bhh46)\n",
        "o = (Xt@Wih68 + bih68) + (ht@Whh68 + bhh68)"
      ],
      "id": "f6a981bb-c0f5-46de-90de-30cb0191e433"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.concat([i,f,g,o],axis=0)"
      ],
      "id": "4ea6298a-f9b8-4731-ba06-07c645128cdc"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ifgo = Xt@ lstmcell.weight_ih.T + lstmcell.bias_ih + ht@ lstmcell.weight_hh.T + lstmcell.bias_hh\n",
        "ifgo"
      ],
      "id": "d701c1d3-bc69-4b08-b5a3-e477c0fe8f19"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "sig(ifgo[0:2]), sig(ifgo[2:4]), tanh(ifgo[4:6]), sig(ifgo[6:8])"
      ],
      "id": "aeaf54c5-1747-4aa6-920f-e9606b847d3e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 직접계산 (ch)"
      ],
      "id": "a7dd47c7-b032-4ee6-8e09-1ca5a6d388fc"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht = torch.zeros(2)\n",
        "ct = torch.zeros(2)"
      ],
      "id": "6821aeae-bb47-4bc1-b856-c3bbcf4e1945"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ct = it*gt + ft*ct\n",
        "ht = ot*tanh(ct)"
      ],
      "id": "11dcca4e-c6d4-4d71-b93e-0efb84a64644"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht,ct"
      ],
      "id": "95585404-9c46-4dfa-a11a-4c05e47c4a86"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `#` ***t=0 $\\to$ t=L***\n",
        "\n",
        "`-` `lstmcell`을 이용하여 구현해보자. (결과비교용)"
      ],
      "id": "6c345170-9997-4c18-8d9c-fc0b6ffb44b6"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(43052)\n",
        "lstmcell = torch.nn.LSTMCell(4,2)\n",
        "cook = torch.nn.Linear(2,4)\n",
        "#--#\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizr = torch.optim.Adam(list(lstmcell.parameters())+list(cook.parameters()),lr=0.1)\n",
        "#--#\n",
        "L = len(X)\n",
        "for epoc in range(1):\n",
        "    # step1~2\n",
        "    ht,ct = torch.zeros(2),torch.zeros(2)\n",
        "    loss = 0 \n",
        "    for t in range(L):\n",
        "        Xt,yt = X[t],y[t]\n",
        "        ht,ct = lstmcell(Xt,(ht,ct))\n",
        "        netout_t = cook(ht) \n",
        "        loss = loss + loss_fn(netout_t,yt)\n",
        "    loss = loss/L \n",
        "    # step3\n",
        "    loss.backward()\n",
        "    # step4 \n",
        "    optimizr.step()\n",
        "    optimizr.zero_grad()"
      ],
      "id": "12df1e95-28ff-4070-89d7-80d6fb5a60b9"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht,ct"
      ],
      "id": "c73ed40b-b72c-43c9-8fa2-d908e01f9bd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 직접구현"
      ],
      "id": "dbcb596c-7017-413c-aa6f-141c1cd9f06d"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(43052)\n",
        "lstmcell = torch.nn.LSTMCell(4,2)\n",
        "cook = torch.nn.Linear(2,4)\n",
        "#--#\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizr = torch.optim.Adam(list(lstmcell.parameters())+list(cook.parameters()),lr=0.1)\n",
        "#--#\n",
        "L = len(X)\n",
        "for epoc in range(1):\n",
        "    # step1~2\n",
        "    ht,ct = torch.zeros(2),torch.zeros(2)\n",
        "    loss = 0 \n",
        "    for t in range(L):\n",
        "        Xt,yt = X[t],y[t]\n",
        "        ifgo = Xt@lstmcell.weight_ih.T + lstmcell.bias_ih +\\\n",
        "                ht@lstmcell.weight_hh.T + lstmcell.bias_hh\n",
        "        it,ft,gt,ot = sig(ifgo[0:2]), sig(ifgo[2:4]), tanh(ifgo[4:6]), sig(ifgo[6:8])\n",
        "        ct = it*gt + ft*ct\n",
        "        ht = ot*tanh(ct)\n",
        "        netout_t = cook(ht) \n",
        "        loss = loss + loss_fn(netout_t,yt)\n",
        "    loss = loss/L \n",
        "    # step3\n",
        "    loss.backward()\n",
        "    # step4 \n",
        "    optimizr.step()\n",
        "    optimizr.zero_grad()"
      ],
      "id": "2e5add57-c622-4b27-b8f0-18d7db9edde7"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "ht,ct"
      ],
      "id": "64621670-9a1f-49e5-8125-e7cfea4c9d84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. `torch.nn.LSTM`\n",
        "\n",
        "# 5. LSTM은 왜 강한가?\n",
        "\n",
        "## A. 적합 및 시각화\n",
        "\n",
        "`-` 적합\n",
        "\n",
        "`-` 시각화"
      ],
      "id": "3a1ab8f8-ff69-4da3-a966-c897fe828731"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# plt.axvline(x=3.5,color=\"lime\")\n",
        "# plt.xticks(\n",
        "#     ticks=range(mat.shape[-1]),\n",
        "#     labels=[r\"$g_0$\",r\"$g_1$\",r\"$h_0$\",r\"$h_1$\",\n",
        "#             r\"$P_a$\",r\"$P_b$\",r\"$P_c$\",r\"$P_C$\"]\n",
        "# )\n",
        "# plt.colorbar()"
      ],
      "id": "f0a2a0db-0a9a-4e95-8f34-f1310f01b9aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 시각화1: $({\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}) \\to {\\boldsymbol c}_{t}$"
      ],
      "id": "6ae9386f-af04-43e9-9237-16db3a4eadac"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# fig,ax = plt.subplots(1,3,figsize=(10,10))\n",
        "# ax[0].matshow(mat1,cmap='bwr',vmin=-1,vmax=1);\n",
        "# ax[0].axvline(x=1.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[0].axvline(x=3.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[0].set_xticks(ticks= [0.5,2.5,4.5],labels=[r'${\\bf g}_t$',r'${\\bf i}_t$',r'${\\bf g}_t \\odot {\\bf i}_t$']);\n",
        "# ax[1].matshow(mat2,cmap='bwr',vmin=-1,vmax=1);\n",
        "# ax[1].axvline(x=1.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[1].axvline(x=3.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[1].set_xticks(ticks= [0.5,2.5,4.5],labels=[r'${\\bf c}_{t-1}$',r'${\\bf f}_t$',r'${\\bf c}_{t-1} \\odot {\\bf f}_t$']);\n",
        "# ax[2].matshow(mat3,cmap='bwr',vmin=-1,vmax=1);\n",
        "# ax[2].axvline(x=1.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[2].axvline(x=3.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# ax[2].set_xticks(ticks= [0.5,2.5,4.5],labels=[r'${\\bf g}_t \\odot {\\bf i}_t$',r'${\\bf c}_{t-1} \\odot {\\bf f}_t$',r'${\\bf c}_t$']);\n",
        "# fig.tight_layout()"
      ],
      "id": "21930975-d835-4198-a750-4fca0254614c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` ${\\boldsymbol g}_t$ 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어\n",
        "있다. (마치 RNN의 hidden node처럼!)\n",
        "\n",
        "-   $\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})$\n",
        "\n",
        "`-` ${\\boldsymbol c}_t$ 특징: ${\\boldsymbol g}_t$와 매우 비슷하지만 약간\n",
        "다른값을 가진다. 그래서 ${\\boldsymbol g}_t$와는 달리 -1,1 이외의 값도\n",
        "종종 등장.\n",
        "\n",
        "-   ${\\boldsymbol c}_t$의 값은 이론상 제한이 없음. (꼭 -1,1 사이에 있지\n",
        "    않음)\n",
        "\n",
        "## C. 시각화2: ${\\boldsymbol g}_t \\to {\\boldsymbol h}_{t}$"
      ],
      "id": "045bffc3-202a-4b84-83f8-86513366d7a1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# plt.xticks([0.5,2.5,4.5,6.5,8.5],[r'${\\bf g}_t$',r'${\\bf c}_t$',r'${\\bf o}_t$',r'${\\bf o}_t \\odot {\\bf c}_t$',r'${\\bf h}_t$']);\n",
        "# plt.axvline(x=1.5,color=\"lime\",linewidth=3)\n",
        "# plt.axvline(x=3.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# plt.axvline(x=5.5,linestyle=\"dashed\",color=\"lime\")\n",
        "# plt.axvline(x=7.5,color=\"lime\",linewidth=3)"
      ],
      "id": "cd867a54-e1df-43c8-8a58-1152acea1f52"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` ${\\boldsymbol h}_t$ 특징: (1) ${\\boldsymbol c}_t$에서 원하는 것만\n",
        "선택적으로 특징으로 삼은 느낌. (2) $c_t$보다 훨씬 값을 다양하게 가진다.\n",
        "($\\odot$ 의 효과 )\n",
        "\n",
        "## D. LSTM의 알고리즘 리뷰 I (수식위주)\n",
        "\n",
        "**(step1)** calculate ${\\tt ifgo}$\n",
        "\n",
        "${\\tt ifgo} = {\\boldsymbol x}_t  \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1}  \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias$\n",
        "\n",
        "$=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg}  ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias$\n",
        "\n",
        "참고: 위의 수식은 아래코드에 해당하는 부분\n",
        "\n",
        "``` python\n",
        "ifgo = xt @ lstm_cell.weight_ih.T +\\\n",
        "       ht @ lstm_cell.weight_hh.T +\\\n",
        "       lstm_cell.bias_ih + lstm_cell.bias_hh\n",
        "```\n",
        "\n",
        "**(step2)** decompose ${\\tt ifgo}$ and get ${\\boldsymbol i}_t$,\n",
        "${\\boldsymbol f}_t$, ${\\boldsymbol g}_t$, ${\\boldsymbol o}_t$\n",
        "\n",
        "${\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )$\n",
        "\n",
        "${\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )$\n",
        "\n",
        "${\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )$\n",
        "\n",
        "${\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )$\n",
        "\n",
        "**(step3)** calculate ${\\boldsymbol c}_t$ and ${\\boldsymbol h}_t$\n",
        "\n",
        "${\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}$\n",
        "\n",
        "${\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)$\n",
        "\n",
        "## E. LSTM의 알고리즘 리뷰 II (느낌위주)\n",
        "\n",
        "-   이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n",
        "\n",
        "`-` 느낌: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은\n",
        "콩물에서 간장을 단계를 나누어 숙성하는 느낌이다.\n",
        "\n",
        "-   RNN:\n",
        "    ${\\boldsymbol x}_t \\overset{{\\boldsymbol h}_{t-1}}{\\longrightarrow} {\\boldsymbol h}_t$\n",
        "-   LSTM:\n",
        "    ${\\boldsymbol x}_t \\overset{{\\boldsymbol h}_{t-1}}{\\longrightarrow} {\\boldsymbol g}_t \\overset{{\\boldsymbol c}_{t-1}}{\\longrightarrow} \\Big({\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\Big)$\n",
        "\n",
        "`-` ${\\boldsymbol g}_t$에 대하여\n",
        "\n",
        "-   과거와 현재의 결합 (선형변환): ${\\boldsymbol x}_t$와\n",
        "    ${\\boldsymbol h}_{t-1}$를 ${\\bf W}_{ig}, {\\bf W}_{hg}$를 이용해\n",
        "    선형결합\n",
        "-   숙성(비선형변환): $\\tanh$\n",
        "-   느낌: RNN에서 간장을 만들던 그 수식에서 $h_t$를 $g_t$로 바꾼것\n",
        "    그래서 RNN의 간장과 비슷하다고 생각하면 된다.\n",
        "-   노트: RNN의 간장은 한계를 가지고 있는데 그 한계를 극복하기 위해 만든\n",
        "    것이 ${\\boldsymbol c}_t$\n",
        "\n",
        "`-` ${\\boldsymbol c}_t$에 대하여\n",
        "\n",
        "-   과거와 현재의 결합 (선형변환): ${\\boldsymbol g}_{t}$와\n",
        "    ${\\boldsymbol c}_{t-1}$를 요소별로 선택하고 더하는 과정\n",
        "-   숙성 (비선형변환): 없음.\n",
        "-   느낌: 과거와 현재의 정보중 유리한것만 기억하여 선택적으로 결합함.\n",
        "    이때 결합방식에 대한 노하우는 ${\\tt input-gate}$,\n",
        "    ${\\tt forget-gate}$ 에 있으며 그러한 결합의 결과가\n",
        "    ${\\boldsymbol c}_t$에 있음. 이 ${\\boldsymbol c}_t$에 대한 정보는\n",
        "    그대로 “냉동보관(?)”되어 다음세대로 내려옴.\n",
        "-   비고: ${\\boldsymbol c}_t$는 사실상 LSTM 알고리즘의 꽃이라 할 수\n",
        "    있음. LSTM은 long short term memory의 약자임. 기존의 RNN은\n",
        "    장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘\n",
        "    활용함. LSTM이 장기기억을 잘 활용하는 비법은 바로\n",
        "    ${\\boldsymbol c}_t$에 있다.\n",
        "\n",
        "`-` ${\\boldsymbol h}_t$에 대하여\n",
        "\n",
        "-   과거와 현재의 결합 (선형변환): 없음\n",
        "-   숙성 (비선형변환): $\\tanh({\\boldsymbol c}_t)$를 요소별로 선택하여\n",
        "    숙성\n",
        "\n",
        "> RNN은 기억할 과거정보가 ${\\boldsymbol h}_{t-1}$ 하나이지만 LSTM은\n",
        "> ${\\boldsymbol c}_{t-1}$, ${\\boldsymbol h}_{t-1}$ 2개이다.\n",
        "\n",
        "## F. LSTM이 강한이유\n",
        "\n",
        "`-` 답변1: LSTM이 장기기억에 유리함. 그 이유는 input, forget, output\n",
        "gate 들이 장기기억을 위한 역할을 하기 때문.\n",
        "\n",
        "-   비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭\n",
        "    LSTM같은 구조일 필요는 없음. (게이트는 꼭3개이어야 하는지?)\n",
        "\n",
        "`-` 답변2: 아키텍처상으로 LSTM은 RNN을 포함함. 그래서 이론적으로 LSTM의\n",
        "성능이 RNN보다 떨어질 이유는 없음.\n",
        "\n",
        "`-` 답변3: 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다.\n",
        "\n",
        "-   실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n",
        "-   그 이유: RNN은 ${\\boldsymbol h}_t$의 값이 -1 혹은 1로 결정되는\n",
        "    경우가 많았음. 그러나 경우에 따라서는 ${\\boldsymbol h}_t$이 -1~1의\n",
        "    값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한\n",
        "    방식으로 학습되는 경우가 많았음.\n",
        "-   왜 LSTM의 ${\\boldsymbol h}_t$은 -1,1 이외의 값을 쉽게 가질 수\n",
        "    있는가? $\\odot$ 때문에.. 즉 게이트때문에..\n",
        "\n",
        "# 6. Ref\n",
        "\n",
        "`-` 참고자료들\n",
        "\n",
        "-   <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n",
        "-   <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n",
        "-   <https://arxiv.org/abs/1402.1128>"
      ],
      "id": "8387735c-d76f-4372-89af-35b7aaaa1cc7"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  }
}