{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13wk-1: 강화학습 (1) – Bandit\n",
        "\n",
        "최규빈  \n",
        "2024-05-29\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/13wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "3149b762-56ee-4c51-add2-ff075280751d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#{{<video https://youtu.be/playlist?list=PLQqh36zP38-zoOHd7w3N5q9Jc5P34Ux8X&si=MdJTHM3a27MCAssp >}}"
      ],
      "id": "4685c071-46e9-4bcb-84a7-ece876eb4654"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "9890da91-3df9-46a8-86fc-c4c23712201b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "da124c05-12c5-4e50-b7c2-c7156427cc82"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 강화학습 Intro\n",
        "\n",
        "`-` 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를\n",
        "학습하는 과업\n",
        "\n",
        "`-` 딥마인드: breakout $\\to$ 알파고\n",
        "\n",
        "-   <https://www.youtube.com/watch?v=TmPfTpjtdgg>\n",
        "\n",
        "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n",
        "\n",
        "# 4. Game1: `Bandit` 게임\n",
        "\n",
        "## A. 게임설명 및 원시코드\n",
        "\n",
        "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을,\n",
        "`버튼1`을 누르면 10의 보상을 준다고 가정\n",
        "\n",
        "`-` 처음에 어떤 행동을 해야 하는가?\n",
        "\n",
        "-   처음에는 아는게 없음\n",
        "-   일단 “아무거나” 눌러보자.\n",
        "\n",
        "`-` 버튼을 아무거나 누르는 코드를 작성해보자.\n",
        "\n",
        "> `action_space` 와 `action` 이라는 용어를 기억할 것\n",
        "\n",
        "`-` 버튼을 누른 행위에 따른 보상을 구현하자."
      ],
      "id": "08f9f2bb-444c-4301-af05-41ab3d8c7dec"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "if action == '버튼0': # button0을 눌렀다면 \n",
        "    reward = 1\n",
        "else: # button1을 눌렀다면 \n",
        "    reward = 10"
      ],
      "id": "61b66eff-ec7c-4884-9e55-0a61279495a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `reward`라는 용어를 기억할 것"
      ],
      "id": "2463038b-c0b8-4b39-b1f1-01af5468c44c"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "reward"
      ],
      "id": "d81e4baa-7bb5-4dfd-a786-488ac021a364"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 아무버튼이나 10번정도 눌러보면서 데이터를 쌓아보자."
      ],
      "id": "70a3374b-b358-4001-a623-68e1fc3ec02e"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼0 1\n",
            "버튼1 10\n",
            "버튼0 1\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼0 1\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10"
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "    action = np.random.choice(action_space)\n",
        "    if action == '버튼0': \n",
        "        reward = 1 \n",
        "    else: \n",
        "        reward = 10    \n",
        "    print(action,reward) "
      ],
      "id": "d1e676d0-b3b8-427d-ace9-a887ff371c9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 깨달았음: `버튼0`을 누르면 1점을 받고, `버튼1`을 누르면 10점을 받는\n",
        "“환경(environment)”이구나? $\\to$ `버튼1`을 누르는 “동작(=action)”을\n",
        "해야하는 상황이구나?\n",
        "\n",
        "-   여기에서 $\\to$의 과정을 체계화 시킨 학문이 강화학습\n",
        "\n",
        "> `environment`라는 용어를 기억할 것"
      ],
      "id": "7f350a6a-a74e-487c-873e-f06e167faa40"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10\n",
            "버튼1 10"
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "    action = action_space[1]\n",
        "    if action == '버튼0': \n",
        "        reward = 1\n",
        "    else: \n",
        "        reward = 10   \n",
        "    print(action,reward) "
      ],
      "id": "84bb28cb-bae0-4364-b208-43f88a1e8690"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   게임 클리어\n",
        "\n",
        "`-` 강화학습: 환경(environment)을 이해 $\\to$ 에이전트(agent)가\n",
        "행동(action)을 결정\n",
        "\n",
        "> `agent`라는 용어를 기억할 것\n",
        "\n",
        "***위의 과정이 잘 되었다는 의미로 사용하는 문장들***\n",
        "\n",
        "-   강화학습이 성공적으로 잘 되었다.\n",
        "-   에이전트가 환경의 과제를 완료했다.\n",
        "-   에이전트가 환경에서 성공적으로 학습했다.\n",
        "-   에이전트가 올바른 행동을 학습했다.\n",
        "-   게임 클리어 (비공식)\n",
        "\n",
        "`-` 게임이 클리어 되었다는 것을 의미하는 지표를 정하고 싶다.\n",
        "\n",
        "-   첫 생각: `버튼1`을 누르는 순간 게임클리어로 보면 되지 않나?\n",
        "-   두번째 생각: 아니지? 우연히 누를수도 있잖아?\n",
        "-   게임클리어조건: (1) 20번은 그냥 진행 (2) 최근 20번의 보상의 평균이\n",
        "    95점 이상이면 게임이 클리어 되었다고 생각하자.[1]\n",
        "\n",
        "`-` 원시코드1: 환경을 이해하지 못한 에이전트 – 게임을 클리어할 수 없다.\n",
        "\n",
        "[1] `버튼1`을 눌러야 하는건 맞지만 몇번의 실수는 눈감아 주자는 의미"
      ],
      "id": "2fe3e184-8b6a-43ec-bddb-f4a563fcd96a"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_try = 1   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 2   action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 3   action = 1  reward = 10 mean(recent_rewards) = 7.0\n",
            "n_try = 4   action = 1  reward = 10 mean(recent_rewards) = 7.75\n",
            "n_try = 5   action = 1  reward = 10 mean(recent_rewards) = 8.2\n",
            "n_try = 6   action = 0  reward = 1  mean(recent_rewards) = 7.0\n",
            "n_try = 7   action = 0  reward = 1  mean(recent_rewards) = 6.142857142857143\n",
            "n_try = 8   action = 1  reward = 10 mean(recent_rewards) = 6.625\n",
            "n_try = 9   action = 0  reward = 1  mean(recent_rewards) = 6.0\n",
            "n_try = 10  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 11  action = 0  reward = 1  mean(recent_rewards) = 5.090909090909091\n",
            "n_try = 12  action = 0  reward = 1  mean(recent_rewards) = 4.75\n",
            "n_try = 13  action = 0  reward = 1  mean(recent_rewards) = 4.461538461538462\n",
            "n_try = 14  action = 1  reward = 10 mean(recent_rewards) = 4.857142857142857\n",
            "n_try = 15  action = 1  reward = 10 mean(recent_rewards) = 5.2\n",
            "n_try = 16  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 17  action = 0  reward = 1  mean(recent_rewards) = 5.235294117647059\n",
            "n_try = 18  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 19  action = 0  reward = 1  mean(recent_rewards) = 5.2631578947368425\n",
            "n_try = 20  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "--\n",
            "n_try = 21  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 22  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 23  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 24  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 25  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 26  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 27  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 28  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 29  action = 1  reward = 10 mean(recent_rewards) = 5.95\n",
            "n_try = 30  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 31  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 32  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 33  action = 0  reward = 1  mean(recent_rewards) = 6.4\n",
            "n_try = 34  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 35  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 36  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 37  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 38  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 39  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 40  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 41  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 42  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 43  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 44  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 45  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 46  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 47  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 48  action = 0  reward = 1  mean(recent_rewards) = 4.6\n",
            "n_try = 49  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 50  action = 0  reward = 1  mean(recent_rewards) = 4.15"
          ]
        }
      ],
      "source": [
        "action_space = [0,1] \n",
        "rewards = [] \n",
        "for t in range(1,51): # 10000번을 해도 못깸  \n",
        "    action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n",
        "    if action == 0: \n",
        "        reward = 1 \n",
        "        rewards.append(reward)\n",
        "    else: \n",
        "        reward = 10\n",
        "        rewards.append(reward)\n",
        "    #--# \n",
        "    print(\n",
        "        f\"n_try = {t}\\t\"\n",
        "        f\"action = {action}\\t\"\n",
        "        f\"reward = {reward}\\t\"\n",
        "        f\"mean(recent_rewards) = {np.mean(rewards[-20:])}\"\n",
        "    )\n",
        "    #--#\n",
        "    if t < 20:\n",
        "        pass\n",
        "    elif t == 20:\n",
        "        print('--')\n",
        "    else:\n",
        "        if np.mean(rewards[-20:]) >= 95:\n",
        "            print('Game Clear')\n",
        "            break "
      ],
      "id": "88406414-89f3-44bc-bdd7-bc1b43788f74"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 원시코드2: 환경을 깨달은 에이전트 – 게임클리어"
      ],
      "id": "b876ef2b-e00b-436f-98bd-bf8bc99fa907"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_try = 1   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 2   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 3   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 4   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 5   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 6   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 7   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 8   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 9   action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 10  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 11  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 12  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 13  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 14  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 15  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 16  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 17  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 18  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 19  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "n_try = 20  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "--\n",
            "n_try = 21  action = 1  reward = 10 mean(recent_rewards) = 10.0\n",
            "Game Clear"
          ]
        }
      ],
      "source": [
        "action_space = [0,1]\n",
        "rewards = [] \n",
        "for t in range(1,51): \n",
        "    #action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n",
        "    action = 1 # 환경을 이해한 에이전트의 행동\n",
        "    if action == 0: \n",
        "        reward = 1 \n",
        "        rewards.append(reward)\n",
        "    else: \n",
        "        reward = 10\n",
        "        rewards.append(reward)\n",
        "    #--# \n",
        "    print(\n",
        "        f\"n_try = {t}\\t\"\n",
        "        f\"action = {action}\\t\"\n",
        "        f\"reward = {reward}\\t\"\n",
        "        f\"mean(recent_rewards) = {np.mean(rewards[-20:])}\"\n",
        "    )\n",
        "    #--#\n",
        "    if t < 20:\n",
        "        pass\n",
        "    elif t == 20:\n",
        "        print('--')\n",
        "    else:\n",
        "        if np.mean(rewards[-20:]) >= 9.5:\n",
        "            print('Game Clear')\n",
        "            break "
      ],
      "id": "a2f7b8d7-ba44-4bc7-987d-bd2660890203"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 수정1: `Env` 구현\n",
        "\n",
        "`-` `Bandit` 클래스 선언 + `.step()` 구현"
      ],
      "id": "d63a1df2-af38-4cd0-845c-d7ccd259f4c7"
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Bandit: \n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            return 1 \n",
        "        else: \n",
        "            return 10"
      ],
      "id": "c6091a16-b766-4546-aa94-877ed2653c52"
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_try = 1   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 2   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 3   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 4   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 5   action = 1  reward = 10 mean(recent_rewards) = 2.8\n",
            "n_try = 6   action = 1  reward = 10 mean(recent_rewards) = 4.0\n",
            "n_try = 7   action = 0  reward = 1  mean(recent_rewards) = 3.5714285714285716\n",
            "n_try = 8   action = 0  reward = 1  mean(recent_rewards) = 3.25\n",
            "n_try = 9   action = 1  reward = 10 mean(recent_rewards) = 4.0\n",
            "n_try = 10  action = 1  reward = 10 mean(recent_rewards) = 4.6\n",
            "n_try = 11  action = 1  reward = 10 mean(recent_rewards) = 5.090909090909091\n",
            "n_try = 12  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 13  action = 1  reward = 10 mean(recent_rewards) = 5.846153846153846\n",
            "n_try = 14  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 15  action = 0  reward = 1  mean(recent_rewards) = 5.2\n",
            "n_try = 16  action = 0  reward = 1  mean(recent_rewards) = 4.9375\n",
            "n_try = 17  action = 1  reward = 10 mean(recent_rewards) = 5.235294117647059\n",
            "n_try = 18  action = 0  reward = 1  mean(recent_rewards) = 5.0\n",
            "n_try = 19  action = 0  reward = 1  mean(recent_rewards) = 4.7894736842105265\n",
            "n_try = 20  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "---\n",
            "n_try = 21  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 22  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 23  action = 1  reward = 10 mean(recent_rewards) = 5.95\n",
            "n_try = 24  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 25  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 26  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 27  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 28  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 29  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 30  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 31  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 32  action = 0  reward = 1  mean(recent_rewards) = 4.6\n",
            "n_try = 33  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 34  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 35  action = 1  reward = 10 mean(recent_rewards) = 4.6\n",
            "n_try = 36  action = 0  reward = 1  mean(recent_rewards) = 4.6\n",
            "n_try = 37  action = 1  reward = 10 mean(recent_rewards) = 4.6\n",
            "n_try = 38  action = 0  reward = 1  mean(recent_rewards) = 4.6\n",
            "n_try = 39  action = 0  reward = 1  mean(recent_rewards) = 4.6\n",
            "n_try = 40  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 41  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 42  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 43  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 44  action = 0  reward = 1  mean(recent_rewards) = 3.7\n",
            "n_try = 45  action = 0  reward = 1  mean(recent_rewards) = 3.7\n",
            "n_try = 46  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 47  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 48  action = 0  reward = 1  mean(recent_rewards) = 4.15\n",
            "n_try = 49  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 50  action = 0  reward = 1  mean(recent_rewards) = 3.7"
          ]
        }
      ],
      "source": [
        "action_space = [0,1]\n",
        "env = Bandit()\n",
        "rewards = []\n",
        "for t in range(1,51): \n",
        "    action = np.random.choice(action_space)\n",
        "    #action = 1\n",
        "    reward = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    #--# \n",
        "    print(\n",
        "        f\"n_try = {t}\\t\"\n",
        "        f\"action = {action}\\t\"\n",
        "        f\"reward = {reward}\\t\"\n",
        "        f\"mean(recent_rewards) = {np.mean(rewards[-20:])}\"\n",
        "    )\n",
        "    #--#\n",
        "    if t < 20:\n",
        "        pass\n",
        "    elif t == 20:\n",
        "        print(\"---\")\n",
        "    else:\n",
        "        if np.mean(rewards[-20:]) >= 9.5:\n",
        "            print(\"Game Clear\")\n",
        "            break  "
      ],
      "id": "28fb2c07-c995-4270-8f44-352b16fa9ca0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 수정2: `Agent` 구현 (인간지능)\n",
        "\n",
        "`-` Agent 클래스 설계\n",
        "\n",
        "-   액션을 하고, 본인의 행동과 환경에서 받은 reward를 기억\n",
        "-   `.act()`함수와 `.save_experience()`함수 구현"
      ],
      "id": "95febe4c-6a93-40f1-8eb2-daaf5196de2e"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.action_space = [0,1]\n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.actions = [] \n",
        "        self.rewards = []\n",
        "    def act(self):\n",
        "        self.action = np.random.choice(self.action_space) # 무지한자 \n",
        "        #self.action = 1 # 깨달은 자\n",
        "    def save_experience(self):\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)"
      ],
      "id": "0e5d87ae-8fe3-417d-848a-f49786c2b2ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "— 대충 아래와 같은 느낌으로 코드가 돌아가요 —\n",
        "\n",
        "**시점0**: init"
      ],
      "id": "7d80d05a-dac9-4235-a673-5b7fdf860788"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = Bandit()\n",
        "agent = Agent() "
      ],
      "id": "6ee01a47-2ecc-41c9-9344-419f06f89d09"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.action, agent.reward"
      ],
      "id": "43397ed8-1846-4db8-85e5-feab67bb12f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**시점1**: agent \\>\\> env"
      ],
      "id": "0262a9b4-0cfd-4e05-8fa7-a63fe3adbdbe"
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.act()"
      ],
      "id": "94b910e3-9d0d-4e32-bfe4-1ea5bc7a537b"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.action, agent.reward"
      ],
      "id": "88ca0c67-6c67-461b-adad-5781c23e366a"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "env.agent_action = agent.action"
      ],
      "id": "390746fe-70f2-42ff-97e7-02872bf725fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**시점2**: agent \\<\\< env"
      ],
      "id": "e66bf368-e330-44d7-8ac5-93b403d92a93"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.reward = env.step(env.agent_action)"
      ],
      "id": "0c06f08a-96f0-454f-bb0e-76f3b80650a0"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.action, agent.reward, env.agent_action"
      ],
      "id": "92121832-fc74-40ca-885b-1b7ebbc0b2ef"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.actions,agent.rewards"
      ],
      "id": "5de5658f-352b-4080-a452-c7b9ceb32181"
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.save_experience()"
      ],
      "id": "5e3ea513-38ff-42a0-b298-c2b17347e30c"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.actions,agent.rewards"
      ],
      "id": "89f2ead3-14ff-4db7-91b7-e7444900a2ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "– 전체코드 –"
      ],
      "id": "1f58a0ce-d705-44bc-a690-0a69aa692535"
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_try = 1   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 2   action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 3   action = 1  reward = 10 mean(recent_rewards) = 7.0\n",
            "n_try = 4   action = 1  reward = 10 mean(recent_rewards) = 7.75\n",
            "n_try = 5   action = 0  reward = 1  mean(recent_rewards) = 6.4\n",
            "n_try = 6   action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 7   action = 1  reward = 10 mean(recent_rewards) = 6.142857142857143\n",
            "n_try = 8   action = 1  reward = 10 mean(recent_rewards) = 6.625\n",
            "n_try = 9   action = 0  reward = 1  mean(recent_rewards) = 6.0\n",
            "n_try = 10  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 11  action = 1  reward = 10 mean(recent_rewards) = 5.909090909090909\n",
            "n_try = 12  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 13  action = 0  reward = 1  mean(recent_rewards) = 5.153846153846154\n",
            "n_try = 14  action = 0  reward = 1  mean(recent_rewards) = 4.857142857142857\n",
            "n_try = 15  action = 1  reward = 10 mean(recent_rewards) = 5.2\n",
            "n_try = 16  action = 0  reward = 1  mean(recent_rewards) = 4.9375\n",
            "n_try = 17  action = 1  reward = 10 mean(recent_rewards) = 5.235294117647059\n",
            "n_try = 18  action = 0  reward = 1  mean(recent_rewards) = 5.0\n",
            "n_try = 19  action = 1  reward = 10 mean(recent_rewards) = 5.2631578947368425\n",
            "n_try = 20  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 21  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 22  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 23  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 24  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 25  action = 1  reward = 10 mean(recent_rewards) = 5.95\n",
            "n_try = 26  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 27  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 28  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 29  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 30  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 31  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 32  action = 1  reward = 10 mean(recent_rewards) = 6.85\n",
            "n_try = 33  action = 0  reward = 1  mean(recent_rewards) = 6.85\n",
            "n_try = 34  action = 0  reward = 1  mean(recent_rewards) = 6.85\n",
            "n_try = 35  action = 0  reward = 1  mean(recent_rewards) = 6.4\n",
            "n_try = 36  action = 0  reward = 1  mean(recent_rewards) = 6.4\n",
            "n_try = 37  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 38  action = 0  reward = 1  mean(recent_rewards) = 5.95\n",
            "n_try = 39  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 40  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 41  action = 0  reward = 1  mean(recent_rewards) = 5.05\n",
            "n_try = 42  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 43  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 44  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 45  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 46  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 47  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 48  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 49  action = 0  reward = 1  mean(recent_rewards) = 5.5\n",
            "n_try = 50  action = 1  reward = 10 mean(recent_rewards) = 5.5"
          ]
        }
      ],
      "source": [
        "env = Bandit() \n",
        "agent = Agent()\n",
        "for t in range(1,51): \n",
        "    # step1: agent >> env \n",
        "    agent.act() \n",
        "    env.agent_action = agent.action\n",
        "    # step2: agent << env \n",
        "    agent.reward = env.step(env.agent_action)\n",
        "    agent.save_experience()  \n",
        "    #--# \n",
        "    print(\n",
        "        f\"n_try = {t}\\t\"\n",
        "        f\"action = {agent.action}\\t\"\n",
        "        f\"reward = {agent.reward}\\t\"\n",
        "        f\"mean(recent_rewards) = {np.mean(agent.rewards[-20:])}\"\n",
        "    )\n",
        "    #--#\n",
        "    if t < 20:\n",
        "        pass\n",
        "    else:\n",
        "        if np.mean(agent.rewards[-20:]) >= 9.5:\n",
        "            print(\"Game Clear\")\n",
        "            break  "
      ],
      "id": "94d5f2d2-b38f-472c-97b1-a64c58f68455"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 수정3: `Agent` 구현 (인공지능)\n",
        "\n",
        "`-` 지금까지 풀이의 한계\n",
        "\n",
        "-   사실 강화학습은 “환경을 이해 $\\to$ 행동을 결정” 의 과정에서\n",
        "    “$\\to$”의 과정을 수식화 한 것이다.\n",
        "-   그런데 지금까지 했던 코드는 환경(environment)를 이해하는 순간\n",
        "    에이전트(agent)가 최적의 행동(action)[1]을 **“직관적으로”**\n",
        "    결정하였으므로 기계가 스스로 학습을 했다고 볼 수 없다.\n",
        "\n",
        "`-` 에이전트가 데이터를 보고 스스로 학습할 수 있도록 설계 – 부제:\n",
        "`agent.learn()`을 설계하자.\n",
        "\n",
        "1.  데이터를 모아서 `q_table` 를 만든다. `q_table`은 아래와 같은 내용을\n",
        "    포함한다.\n",
        "\n",
        "|      행동      | 보상(추정값) |\n",
        "|:--------------:|:------------:|\n",
        "| 버튼0 ($=a_0$) |  1 ($=q_0$)  |\n",
        "| 버튼1 ($=a_1$) | 100 ($=q_1$) |\n",
        "\n",
        "1.  `q_table`을 바탕으로 적절한 정책(=`policy`)을 설정한다.\n",
        "\n",
        "-   이 예제에서는 버튼0과 버튼1을 각각\n",
        "    $\\big(\\frac{q_0}{q_0+q_1},\\frac{q_1}{q_0+q_1}\\big)$ 의 확률로\n",
        "    선택하는 “정책”을 이용하면 충분할 듯\n",
        "\n",
        "> 여기에서 `q_table`, `policy`라는 용어를 기억하세요.\n",
        "\n",
        "`-` `q_table`을 계산하는 코드 예시\n",
        "\n",
        "[1] `버튼1`을 누른다"
      ],
      "id": "cc5f80fa-cf4a-479d-af2b-bc3ba2dde827"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.actions = [0, 1, 1,  0, 1,   0, 0] \n",
        "agent.rewards = [1, 9, 10, 1, 9.5, 1, 1.2] \n",
        "actions = np.array(agent.actions)\n",
        "rewards = np.array(agent.rewards)"
      ],
      "id": "f725954b-d1fa-47e9-ab79-46ec3a53eef2"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q0 = rewards[actions == 0].mean()\n",
        "q1 = rewards[actions == 1].mean()"
      ],
      "id": "2ca4712f-f522-431c-b71d-816820b5f6b2"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.q = np.array([q0,q1]) \n",
        "agent.q"
      ],
      "id": "0f400b80-ff55-4b0c-bb7c-6bf4eded24f6"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "prob = agent.q / agent.q.sum()\n",
        "prob "
      ],
      "id": "3d52e620-b8ed-42de-a1a1-7d5961ccbccb"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action = np.random.choice([0,1], p= prob)\n",
        "action"
      ],
      "id": "3ef3f7fe-29a4-473b-bfac-3bb00c45b045"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 최종코드정리"
      ],
      "id": "a63d1587-8c34-41b1-802c-ad16009facd1"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Bandit: \n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            return 1 \n",
        "        else: \n",
        "            return 10\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.action_space = [0,1]\n",
        "        self.action = None \n",
        "        self.reward = None \n",
        "        self.actions = [] \n",
        "        self.rewards = []\n",
        "        self.q_table = np.array([0,0]) \n",
        "        self.n_experience = 0 \n",
        "    def act(self):\n",
        "        if self.n_experience < 20: \n",
        "            self.action = np.random.choice(self.action_space)\n",
        "        else: \n",
        "            prob = self.q_table / self.q_table.sum()\n",
        "            self.action = np.random.choice(self.action_space, p= prob)\n",
        "    def save_experience(self):\n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.n_experience += 1 \n",
        "    def learn(self):\n",
        "        if self.n_experience<20: \n",
        "            pass \n",
        "        else: \n",
        "            actions = np.array(self.actions)\n",
        "            rewards = np.array(self.rewards)\n",
        "            q0 = rewards[actions == 0].mean()\n",
        "            q1 = rewards[actions == 1].mean()\n",
        "            self.q_table = np.array([q0,q1]) "
      ],
      "id": "a218caa5-b032-4e57-ab22-cca6b2c8ab02"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_try = 1   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 2   action = 0  reward = 1  mean(recent_rewards) = 1.0\n",
            "n_try = 3   action = 1  reward = 10 mean(recent_rewards) = 4.0\n",
            "n_try = 4   action = 0  reward = 1  mean(recent_rewards) = 3.25\n",
            "n_try = 5   action = 1  reward = 10 mean(recent_rewards) = 4.6\n",
            "n_try = 6   action = 0  reward = 1  mean(recent_rewards) = 4.0\n",
            "n_try = 7   action = 0  reward = 1  mean(recent_rewards) = 3.5714285714285716\n",
            "n_try = 8   action = 0  reward = 1  mean(recent_rewards) = 3.25\n",
            "n_try = 9   action = 0  reward = 1  mean(recent_rewards) = 3.0\n",
            "n_try = 10  action = 0  reward = 1  mean(recent_rewards) = 2.8\n",
            "n_try = 11  action = 0  reward = 1  mean(recent_rewards) = 2.6363636363636362\n",
            "n_try = 12  action = 0  reward = 1  mean(recent_rewards) = 2.5\n",
            "n_try = 13  action = 0  reward = 1  mean(recent_rewards) = 2.3846153846153846\n",
            "n_try = 14  action = 0  reward = 1  mean(recent_rewards) = 2.2857142857142856\n",
            "n_try = 15  action = 1  reward = 10 mean(recent_rewards) = 2.8\n",
            "n_try = 16  action = 0  reward = 1  mean(recent_rewards) = 2.6875\n",
            "n_try = 17  action = 0  reward = 1  mean(recent_rewards) = 2.588235294117647\n",
            "n_try = 18  action = 1  reward = 10 mean(recent_rewards) = 3.0\n",
            "n_try = 19  action = 0  reward = 1  mean(recent_rewards) = 2.8947368421052633\n",
            "n_try = 20  action = 0  reward = 1  mean(recent_rewards) = 2.8\n",
            "n_try = 21  action = 1  reward = 10 mean(recent_rewards) = 3.25\n",
            "n_try = 22  action = 1  reward = 10 mean(recent_rewards) = 3.7\n",
            "n_try = 23  action = 1  reward = 10 mean(recent_rewards) = 3.7\n",
            "n_try = 24  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 25  action = 1  reward = 10 mean(recent_rewards) = 4.15\n",
            "n_try = 26  action = 1  reward = 10 mean(recent_rewards) = 4.6\n",
            "n_try = 27  action = 1  reward = 10 mean(recent_rewards) = 5.05\n",
            "n_try = 28  action = 1  reward = 10 mean(recent_rewards) = 5.5\n",
            "n_try = 29  action = 1  reward = 10 mean(recent_rewards) = 5.95\n",
            "n_try = 30  action = 1  reward = 10 mean(recent_rewards) = 6.4\n",
            "n_try = 31  action = 1  reward = 10 mean(recent_rewards) = 6.85\n",
            "n_try = 32  action = 1  reward = 10 mean(recent_rewards) = 7.3\n",
            "n_try = 33  action = 1  reward = 10 mean(recent_rewards) = 7.75\n",
            "n_try = 34  action = 1  reward = 10 mean(recent_rewards) = 8.2\n",
            "n_try = 35  action = 1  reward = 10 mean(recent_rewards) = 8.2\n",
            "n_try = 36  action = 1  reward = 10 mean(recent_rewards) = 8.65\n",
            "n_try = 37  action = 0  reward = 1  mean(recent_rewards) = 8.65\n",
            "n_try = 38  action = 1  reward = 10 mean(recent_rewards) = 8.65\n",
            "n_try = 39  action = 1  reward = 10 mean(recent_rewards) = 9.1\n",
            "n_try = 40  action = 1  reward = 10 mean(recent_rewards) = 9.55\n",
            "Game Clear"
          ]
        }
      ],
      "source": [
        "env = Bandit() \n",
        "agent = Agent()\n",
        "for t in range(50): \n",
        "    ## 1. main 코드 \n",
        "    # step1: agent >> env \n",
        "    agent.act() \n",
        "    env.agent_action = agent.action\n",
        "    # step2: agent << env \n",
        "    agent.reward = env.step(env.agent_action)\n",
        "    agent.save_experience() \n",
        "    # step3: learn \n",
        "    agent.learn()\n",
        "    #--# \n",
        "    print(\n",
        "        f\"n_try = {t+1}\\t\"\n",
        "        f\"action = {agent.action}\\t\"\n",
        "        f\"reward = {agent.reward}\\t\"\n",
        "        f\"mean(recent_rewards) = {np.mean(agent.rewards[-20:])}\"\n",
        "    )\n",
        "    #--#\n",
        "    if t < 20:\n",
        "        pass\n",
        "    else:\n",
        "        if np.mean(agent.rewards[-20:]) >= 9.5:\n",
        "            print(\"Game Clear\")\n",
        "            break  "
      ],
      "id": "23269b00-b8b6-4e57-89b2-963594874899"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  }
}