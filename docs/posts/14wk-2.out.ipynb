{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-2: 강화학습 (3) – 4x4 Grid World (`AgentGreedy`,`AgentExplorer`)\n",
        "\n",
        "최규빈  \n",
        "2024-06-06\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/14wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상"
      ],
      "id": "ef4fa25f-4083-4e9f-9810-87f9923c78f1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# {{<video https://youtu.be/playlist?list=PLQqh36zP38-zHvVuJ92xfdypwHwDFgg8k&si=iI4IhthblTsJTmIv >}}"
      ],
      "id": "1dfb3796-7623-4061-bc81-82305c2d4d59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Imports"
      ],
      "id": "cf53f56a-9940-4f8b-a068-0ba141eeb9bd"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install gymnasium\n",
        "#---#\n",
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "5b1cfd48-d338-4ddd-8b59-f8ab593ebda9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. `Env`, `AgentRandom`"
      ],
      "id": "1705b6f1-3ca7-47f5-9e7f-e7c70e1c1a34"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_to_direction = {\n",
        "    0 : np.array([1, 0]), # row+, down\n",
        "    1 : np.array([0, 1]), # col+, right\n",
        "    2 : np.array([-1 ,0]), # row-, up\n",
        "    3 : np.array([0, -1]) # col-, left\n",
        "}\n",
        "action_to_direction2 = {0: 'down', 1: 'right', 2: 'up', 3: 'left'} # 당장쓰진 않지만 하는김에 "
      ],
      "id": "23c93695"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.action_space = gym.spaces.Discrete(4) \n",
        "        self._action_to_direction = {\n",
        "            0 : np.array([1, 0]), # row+, down\n",
        "            1 : np.array([0, 1]), # col+, right\n",
        "            2 : np.array([-1 ,0]), # row-, up\n",
        "            3 : np.array([0, -1]) # col-, left\n",
        "        }\n",
        "        self.reset()\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.state = self.state + direction\n",
        "        if np.array_equal(self.state,np.array([3,3])): \n",
        "            reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state not in state_space:\n",
        "            reward = -10\n",
        "            self.terminated = True\n",
        "        else:\n",
        "            reward = -1 \n",
        "        return self.state, reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False   \n",
        "        return self.state "
      ],
      "id": "e89495dc"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentRandom: \n",
        "    def __init__(self,env):\n",
        "        #--# spaces \n",
        "        self.action_space = env.action_space\n",
        "        self.state_space = env.state_space\n",
        "        #--# replay buffer \n",
        "        self.action = None \n",
        "        self.actions = [] \n",
        "        self.current_state =  None \n",
        "        self.current_states = [] \n",
        "        self.reward = None \n",
        "        self.rewards = [] \n",
        "        self.next_state =  None \n",
        "        self.next_states = [] \n",
        "        self.terminated = None \n",
        "        self.terminations = []\n",
        "        #--# other information\n",
        "        self.n_episodes = 0         \n",
        "        self.n_experiences = 0\n",
        "        self.score = 0        \n",
        "        self.playtimes = [] \n",
        "        self.scores = []    \n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def learn(self):\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        self.current_states.append(self.current_state)        \n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)  \n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        #--#\n",
        "        self.n_experiences = self.n_experiences + 1 \n",
        "        self.score = self.score + self.reward"
      ],
      "id": "b6a2a93a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. `AgentGreedy`\n",
        "\n",
        "## A. 환경의 이해\n",
        "\n",
        "`-` 무작위로 10000판을 진행해보자."
      ],
      "id": "23678d11-9525-4e1a-a5b2-34eb98d380fd"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for _ in range(20000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1 "
      ],
      "id": "40b12099-567e-419d-b629-d34f01810511"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "ceba972e-030e-4bd1-ae1d-5367cea52107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "7c66db66-84d3-4620-857c-276529ffa94d"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[0]} / {agent.actions[0]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[0]} / {agent.next_states[0]}\")"
      ],
      "id": "907a43bc-1644-4ad2-ba8f-f9296035721a"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 1\n",
            "환경: 보상/다음상태 = -1 / [0 2]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[1]} / {agent.actions[1]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[1]} / {agent.next_states[1]}\")"
      ],
      "id": "57ae01f6-3178-477f-ba2b-b708a91338c4"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 2] / 3\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[2]} / {agent.actions[2]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[2]} / {agent.next_states[2]}\")"
      ],
      "id": "9fdc44e3-02df-4b0b-a4e4-39765d50e7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / 0\n",
            "환경: 보상/다음상태 = -1 / [1 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[3]} / {agent.actions[3]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[3]} / {agent.next_states[3]}\")"
      ],
      "id": "69c21c21-3995-4167-8741-becd4ac9a2a1"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [1 1] / 3\n",
            "환경: 보상/다음상태 = -1 / [1 0]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[4]} / {agent.actions[4]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[4]} / {agent.next_states[4]}\")"
      ],
      "id": "4288e684-decd-4056-83bd-9f2a0b79ec35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "3bbdc4f3-d9fe-4fce-bd7e-485411abd628"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i] \n",
        "    a = agent.actions[i] \n",
        "    r = agent.rewards[i] \n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a] + r\n",
        "    count[s1,s2,a] = count[s1,s2,a] + 1 "
      ],
      "id": "dc434942-f67f-4c5f-8315-6a6f2da7a175"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "count[count == 0] = 0.01 \n",
        "q_table = q_table/count"
      ],
      "id": "6a2698f4-75cd-463b-9256-c84a23793b1a"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,3]"
      ],
      "id": "b0da7c22-98c3-421f-888c-a441da28700d"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(3)}\\n\" \n",
        ")"
      ],
      "id": "c7575209-a13d-4851-a793-dc61bd46f5da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2)"
      ],
      "id": "939ee22c-04f9-492d-9cd0-19a93e2cb8f6"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    r = agent.rewards[i]\n",
        "    q_hat = q_table[s1,s2,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n",
        "    q = r # 실제 답 \n",
        "    diff = q - q_hat # 실제답과 풀이한값의 차이 = 오차피드백값 \n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 "
      ],
      "id": "914e7423-5d32-4b5a-9a2c-3cf0324e721a"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,0]"
      ],
      "id": "ee91f34f-f6d3-4a49-9d99-9a087d2275a5"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "0d2778f3-b24b-4a2b-9abd-78f1a687e9ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경의 깊은 이해\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "4947bc21-12f1-43e5-b619-acb6b8ba8661"
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[:,:,1]"
      ],
      "id": "a1214644-c901-4eb9-8d32-563df474baf2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "2dab9144-54bc-4155-b4d6-7c7ba934eb04"
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,1]"
      ],
      "id": "08ca1c41-211a-4c55-85e9-bce10f20b3d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 –\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "d40966e0-863a-4d23-9676-54a41bc9bdca"
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,1,1]"
      ],
      "id": "0586f726-9795-4435-89aa-f73dd145c176"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 –\\> 합리적일까??\n",
        "\n",
        "`-` 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지\n",
        "합리적이지 못함.\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n",
        "-   0을 쓸때와 1을 쓸때 보상이 다름\n",
        "-   무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면\n",
        "    10만원을 보상을 준다는 것을 “알게 되었음”\n",
        "-   이때 빈 종이의 가치는 5만원인가? 10만원인가? –\\> 10만원아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$ $a=1$에서 추정된(esitated) 값은\n",
        "`q_table[3,1,1]` $\\approx$ -1 이지만[1], 현실적으로는 “실제보상(-1)과\n",
        "잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n",
        "\n",
        "[1] 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음"
      ],
      "id": "ef4f3530-0ce3-437a-84e4-66491e8cf778"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_hat = q_table[3,1,1]\n",
        "q_hat"
      ],
      "id": "cfa4a389-21a8-4505-a864-fe28600a15c5"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q = (-1) + 0.99 * 100 \n",
        "q"
      ],
      "id": "c9c2af60-dfdb-4053-8609-3aa02e559007"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이\n",
        "    $\\approx$ 십만원 으로 생각한다는 의미)\n",
        "-   0.99는 보통 $\\gamma$라는 기호로 표기하며 `discount rate`이라고\n",
        "    표현한다. (외우세여)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s$, $a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을\n",
        "좀 더 엄밀하게 쓰면 (terminated, not-terminated 로 나누어 쓰면) 아래와\n",
        "같다.\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases}  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated} \\\\ \\text{reward}(s,a) & \\text{terminated} \\end{cases}$$\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 대충 설명하면서 넘어갔지만 이 수식을 **벨만방정식**이라고 부른다.\n",
        "> (외우세여) 위의 식은 강화학습에서 가장 중요한 식이며 원래 버전은\n",
        "> 아래와 같다.\n",
        ">\n",
        "> $$Q^\\star(s,a) = R(s,a) +\\gamma\\sum_{s'}P(s'|s,a)\\max_{a}Q(s',a)$$\n",
        ">\n",
        "> 여기에서 $P(s'|s,a)$ 는 상태 $s \\in {\\cal S}$에서 행동\n",
        "> $a \\in {\\cal A}$를 했을때 $s'$에 있을 확률이다. 이러한 확률은\n",
        "> “바람,소용돌이” 등의 외부의 확률적인 요소가 있는 환경에서 의미가\n",
        "> 있으며 우리의 예제에서는 의미가 없다."
      ],
      "id": "af69bc73-4aeb-456c-8ebd-1f873e1d2b44"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    ss1,ss2 = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_hat = q_table[s1,s2,a] \n",
        "    if agent.terminations[i]:\n",
        "        q = agent.rewards[i]\n",
        "    else:\n",
        "        future_reward = q_table[ss1,ss2,:].max()\n",
        "        q = agent.rewards[i] + 0.99 * future_reward\n",
        "    diff = q - q_hat\n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff "
      ],
      "id": "4d6ffc83-3edb-42a7-a12c-23dc77e849cc"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            " [[-0.47 -0.14 -0.05  0.  ]\n",
            " [-0.1  -0.05  0.   -0.05]\n",
            " [-0.05 -0.05  0.    5.  ]\n",
            " [ 0.   -0.5   0.    0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            " [[-0.61 -0.14  0.    0.  ]\n",
            " [-0.1  -0.05 -0.1  -0.5 ]\n",
            " [-0.1  -0.1   0.    0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            " [[-3.02 -3.02  0.    0.  ]\n",
            " [-0.17 -0.1   0.    0.  ]\n",
            " [ 0.    0.   -0.05  0.  ]\n",
            " [-0.05  0.    0.    0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            " [[-2.26 -0.39 -0.1   0.  ]\n",
            " [-3.02 -0.1  -0.05  0.  ]\n",
            " [-0.5  -0.05 -0.05  0.  ]\n",
            " [ 0.    0.    0.    0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for a in range(4):\n",
        "    print(\n",
        "        f\"action = {a}/{action_to_direction2[a]}\\n\" \n",
        "        f\"action-value function = \\n {q_table[:,:,a].round(2)}\\n\" \n",
        ")"
      ],
      "id": "ff789b47-1166-40f2-af86-1403f91a3643"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "81314f3a-fdef-4b8c-a73a-b571ac24001f"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:]"
      ],
      "id": "4f17808b-c852-4027-baa0-f364d2dd3ee0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "88ea7d6e-63aa-4fb8-9885-d061e0644c40"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:]"
      ],
      "id": "b0a53547-7ae8-4950-b10c-2146755a87b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "6f424c86-a350-4e4e-8007-50ec122ca17f"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:]"
      ],
      "id": "92aac9f3-5e47-42bb-8e62-5bf64f36cc29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동1을 하는게 유리함\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "c8469b7b-245d-4db7-9a5f-82384d5d924a"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:].argmax()"
      ],
      "id": "f1b692c3-94b6-4c9e-8a92-ff0bb836c83b"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[2,3,:].argmax()"
      ],
      "id": "982e1340-a7d2-4e2a-84d8-f4d41072227b"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,:].argmax()"
      ],
      "id": "ede24e2e-ed08-405e-870e-f9b780ec5451"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "466c8359-dc31-4696-9c6e-1065016f54df"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "a9cd5f06-8a65-46e7-9f74-92ec7c2d9d59"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for s1 in range(4):\n",
        "    for s2 in range(4):\n",
        "        policy[s1,s2] = action_to_direction2[q_table[s1,s2,:].argmax()]\n",
        "policy"
      ],
      "id": "c2c85140-c119-4f09-8529-d293f75662e8"
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table.max(axis=-1)"
      ],
      "id": "699a6066-80e7-415c-aa87-57036e07979f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 에이전트 클래스 설계"
      ],
      "id": "a31669b9-9326-4619-a017-3776130188ae"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentGreedy(AgentRandom):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q_table = np.zeros([4,4,4]) \n",
        "    def learn(self):\n",
        "        s1,s2 = self.current_state\n",
        "        ss2,ss2 = self.next_state\n",
        "        a = self.action \n",
        "        q_hat = self.q_table[s1,s2,a] \n",
        "        if self.terminated:\n",
        "            q = self.reward\n",
        "        else:\n",
        "            future_reward = q_table[ss1,ss2,:].max()\n",
        "            q = self.reward + 0.99 * future_reward \n",
        "        diff = q - q_hat \n",
        "        self.q_table[s1,s2,a] = q_hat + 0.05 * diff \n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            s1,s2 = self.current_state \n",
        "            self.action = self.q_table[s1,s2,:].argmax()"
      ],
      "id": "fbd05240-8d9f-45d9-a704-f4aff617d46c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. 환경과 상호작용"
      ],
      "id": "c2e9860a-0c12-48b8-befd-75e5f0b70a38"
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.64   Playtime: 2.74  \n",
            "Epsiode: 1000   Score: 87.27    Playtime: 12.63 \n",
            "Epsiode: 1500   Score: 90.66    Playtime: 10.34 \n",
            "Epsiode: 2000   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 2500   Score: 94.0     Playtime: 7.0   \n",
            "Epsiode: 3000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 3500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4000   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 4500   Score: 95.0     Playtime: 6.0   \n",
            "Epsiode: 5000   Score: 95.0     Playtime: 6.0   "
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentGreedy(env) \n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "        )     "
      ],
      "id": "3bf71d4d-f5b3-48f7-a437-dd0eedb4d211"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. 상호작용결과 시각화"
      ],
      "id": "a88b326e-cbe5-4751-9d6f-6743bea73f3d"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "3cc1eb34-48f5-4096-8bdb-d9af4cab8e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. `AgentExplorer`\n",
        "\n",
        "## A. 클래스 설계"
      ],
      "id": "91c868cf-84b0-49c9-adde-65d0be8e42cd"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentExplorer(AgentGreedy):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0 \n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample() \n",
        "        else:\n",
        "            super().act()"
      ],
      "id": "50575733-9c15-4c33-81fd-9a8b321a54d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "962a8c4b-6d40-4f23-88c8-b5da36aadfb3"
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsiode: 500    Score: -10.28   Playtime: 3.48  Epsilon:  0.61\n",
            "Epsiode: 1000   Score: -1.61    Playtime: 4.71  Epsilon:  0.37\n",
            "Epsiode: 1500   Score: 38.27    Playtime: 8.83  Epsilon:  0.22\n",
            "Epsiode: 2000   Score: 59.41    Playtime: 9.69  Epsilon:  0.14\n",
            "Epsiode: 2500   Score: 82.61    Playtime: 7.39  Epsilon:  0.08\n",
            "Epsiode: 3000   Score: 80.89    Playtime: 6.91  Epsilon:  0.05\n",
            "Epsiode: 3500   Score: 92.8     Playtime: 6.0   Epsilon:  0.03\n",
            "Epsiode: 4000   Score: 92.71    Playtime: 6.09  Epsilon:  0.02\n",
            "Epsiode: 4500   Score: 93.85    Playtime: 6.05  Epsilon:  0.01\n",
            "Epsiode: 5000   Score: 92.86    Playtime: 5.94  Epsilon:  0.01"
          ]
        }
      ],
      "source": [
        "env = GridWorld() \n",
        "agent = AgentExplorer(env) \n",
        "agent.eps = 1\n",
        "for _ in range(5000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에프소드 진행 \n",
        "    for t in range(50):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn()\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episodes = agent.n_episodes + 1\n",
        "    agent.eps = agent.eps * 0.999\n",
        "    #--#\n",
        "    if (agent.n_episodes % 500) ==0:\n",
        "        print(\n",
        "            f\"Epsiode: {agent.n_episodes} \\t\"\n",
        "            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n",
        "            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n",
        "            f\"Epsilon: {agent.eps : .2f}\"\n",
        "        )   "
      ],
      "id": "50ba284c-2a6f-4218-b521-533352867e68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용 결과 시각화"
      ],
      "id": "3483355a-7b8b-4d61-9af6-a35e9d9c5ba2"
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \n",
        "show(states)"
      ],
      "id": "c1513a2f-7822-4b77-af8f-6a11be9eb192"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  }
}